preference_mode: binary

model: meta-llama/Meta-Llama-3.1-8B-Instruct
temperature: 1.0
max_concurrent: 50

n_tasks: 50
task_origin: wildchat

templates: src/preferences/templates/data/binary_choice_basic.yaml

samples_per_pair: 5

fitting:
  max_iter: null  # auto: max(2000, n_params*50)
  gradient_tol: 1.0
  loss_tol: 1e-8
