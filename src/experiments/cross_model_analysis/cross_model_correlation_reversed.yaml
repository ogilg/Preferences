order: reversed
pairs:
  llama-3.1-8b vs llama-3.1-8b-instruct:
    mean: 0.7005601891939712
    std: 0.15945571333046674
    n: 4
    min: 0.5539059960833064
    max: 0.9480180217663249
  llama-3.1-8b vs llama-3.3-70b-instruct:
    mean: 0.6212573493017369
    std: 0.19718695314853876
    n: 4
    min: 0.40076935472047726
    max: 0.8363901825983385
  llama-3.1-8b vs qwen-2.5-72b-instruct:
    mean: 0.5377922436597941
    std: 0.17191937702530413
    n: 8
    min: 0.31491037129619703
    max: 0.818804604582237
  llama-3.1-8b vs qwen-2.5-7b-instruct:
    mean: 0.5556453628636986
    std: 0.24458949744516692
    n: 9
    min: 0.09374016474591762
    max: 0.8791729967178595
  llama-3.1-8b-instruct vs llama-3.3-70b-instruct:
    mean: 0.7008824733411333
    std: 0.08581511540051327
    n: 3
    min: 0.613347450481456
    max: 0.817448102030431
  llama-3.1-8b-instruct vs qwen-2.5-72b-instruct:
    mean: 0.5553641443929718
    std: 0.15465094969726728
    n: 4
    min: 0.2940569493187468
    max: 0.6708167257271902
  llama-3.1-8b-instruct vs qwen-2.5-7b-instruct:
    mean: 0.6063332632956399
    std: 0.14705724138706816
    n: 4
    min: 0.3942363503627257
    max: 0.8062800888204821
  llama-3.3-70b-instruct vs qwen-2.5-72b-instruct:
    mean: 0.6944021260001699
    std: 0.1138643334259393
    n: 4
    min: 0.5238691175671435
    max: 0.812208345325041
  llama-3.3-70b-instruct vs qwen-2.5-7b-instruct:
    mean: 0.7593646093527168
    std: 0.09454167531515528
    n: 4
    min: 0.6522908352161374
    max: 0.8651588226057583
  qwen-2.5-72b-instruct vs qwen-2.5-7b-instruct:
    mean: 0.6302520207222508
    std: 0.15880326059795993
    n: 9
    min: 0.3930424669787849
    max: 0.8640687398970237
