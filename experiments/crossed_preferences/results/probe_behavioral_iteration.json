[
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.8275,
    "probe_delta_L31": -197.5689697265625,
    "probe_delta_L43": 141.1806640625,
    "probe_delta_L55": 319.91748046875
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.5025,
    "probe_delta_L31": -105.02716064453125,
    "probe_delta_L43": 418.80377197265625,
    "probe_delta_L55": 706.42626953125
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -182.20245361328125,
    "probe_delta_L43": 529.8802490234375,
    "probe_delta_L55": 1620.3863525390625
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.9,
    "probe_delta_L31": -470.0123291015625,
    "probe_delta_L43": -183.0811767578125,
    "probe_delta_L55": 486.353271484375
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.1,
    "probe_delta_L31": -55.3544921875,
    "probe_delta_L43": 362.7840576171875,
    "probe_delta_L55": 1243.378662109375
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.06060606060606061,
    "probe_delta_L31": -198.07647705078125,
    "probe_delta_L43": 756.1795043945312,
    "probe_delta_L55": 2062.40673828125
  },
  {
    "prompt_id": "cheese_neg_persona",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.707808564231738,
    "probe_delta_L31": -329.07403564453125,
    "probe_delta_L43": -60.64501953125,
    "probe_delta_L55": 456.194091796875
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.07250000000000001,
    "probe_delta_L31": 142.568359375,
    "probe_delta_L43": 470.989990234375,
    "probe_delta_L55": 687.892822265625
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.22250000000000003,
    "probe_delta_L31": 119.72747802734375,
    "probe_delta_L43": 810.5535278320312,
    "probe_delta_L55": 1440.446533203125
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 70.2396240234375,
    "probe_delta_L43": 860.3751220703125,
    "probe_delta_L55": 1096.7882080078125
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.09999999999999998,
    "probe_delta_L31": 44.208984375,
    "probe_delta_L43": 265.3023681640625,
    "probe_delta_L55": 333.744873046875
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.2075,
    "probe_delta_L31": 53.7335205078125,
    "probe_delta_L43": 390.5220947265625,
    "probe_delta_L55": 725.8115234375
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.8143939393939394,
    "probe_delta_L31": 270.2640380859375,
    "probe_delta_L43": 1193.22705078125,
    "probe_delta_L55": 1980.5799560546875
  },
  {
    "prompt_id": "cheese_pos_persona",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.292191435768262,
    "probe_delta_L31": 163.0616455078125,
    "probe_delta_L43": 600.376708984375,
    "probe_delta_L55": 893.17822265625
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.8025,
    "probe_delta_L31": -153.20458984375,
    "probe_delta_L43": 218.72998046875,
    "probe_delta_L55": 164.88525390625
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.5025,
    "probe_delta_L31": -70.48333740234375,
    "probe_delta_L43": 624.6754760742188,
    "probe_delta_L55": 365.035400390625
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.95,
    "probe_delta_L31": -95.712890625,
    "probe_delta_L43": 925.6741943359375,
    "probe_delta_L55": 1439.3785400390625
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.9,
    "probe_delta_L31": -442.38702392578125,
    "probe_delta_L43": 10.2276611328125,
    "probe_delta_L55": 688.298583984375
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.1,
    "probe_delta_L31": -54.2376708984375,
    "probe_delta_L43": 533.7469482421875,
    "probe_delta_L55": 1052.792724609375
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.06060606060606061,
    "probe_delta_L31": -195.30224609375,
    "probe_delta_L43": 717.5504760742188,
    "probe_delta_L55": 1758.9830322265625
  },
  {
    "prompt_id": "cheese_neg_experiential",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.707808564231738,
    "probe_delta_L31": -440.95672607421875,
    "probe_delta_L43": -56.894775390625,
    "probe_delta_L55": -61.0443115234375
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.1725,
    "probe_delta_L31": 257.19580078125,
    "probe_delta_L43": 628.19189453125,
    "probe_delta_L55": 1009.297607421875
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.4325000000000001,
    "probe_delta_L31": 133.19525146484375,
    "probe_delta_L43": 817.9226684570312,
    "probe_delta_L55": 1842.740234375
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 78.0382080078125,
    "probe_delta_L43": 291.7119140625,
    "probe_delta_L55": 571.439208984375
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.09999999999999998,
    "probe_delta_L31": 9.6842041015625,
    "probe_delta_L43": 259.9263916015625,
    "probe_delta_L55": 310.75927734375
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.24999999999999997,
    "probe_delta_L31": -10.7598876953125,
    "probe_delta_L43": 277.5640869140625,
    "probe_delta_L55": 398.688720703125
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.9393939393939394,
    "probe_delta_L31": 229.7828369140625,
    "probe_delta_L43": 903.9201049804688,
    "probe_delta_L55": 1876.5665283203125
  },
  {
    "prompt_id": "cheese_pos_experiential",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.292191435768262,
    "probe_delta_L31": 178.940673828125,
    "probe_delta_L43": 769.907470703125,
    "probe_delta_L55": 1376.7080078125
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.7025,
    "probe_delta_L31": 15.7098388671875,
    "probe_delta_L43": 20.208740234375,
    "probe_delta_L55": -197.02392578125
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.45249999999999996,
    "probe_delta_L31": -96.076904296875,
    "probe_delta_L43": 250.75946044921875,
    "probe_delta_L55": 580.1566162109375
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.9575,
    "probe_delta_L31": -25.01904296875,
    "probe_delta_L43": 772.0797119140625,
    "probe_delta_L55": 1159.1854248046875
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.875,
    "probe_delta_L31": -329.896240234375,
    "probe_delta_L43": -104.5850830078125,
    "probe_delta_L55": 373.075927734375
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.07500000000000001,
    "probe_delta_L31": 15.8621826171875,
    "probe_delta_L43": 301.1820068359375,
    "probe_delta_L55": 851.59423828125
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.06060606060606061,
    "probe_delta_L31": -82.74249267578125,
    "probe_delta_L43": 902.2794799804688,
    "probe_delta_L55": 2111.052734375
  },
  {
    "prompt_id": "cheese_neg_value",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.682808564231738,
    "probe_delta_L31": -224.263671875,
    "probe_delta_L43": -455.630126953125,
    "probe_delta_L55": -433.78369140625
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.13,
    "probe_delta_L31": 224.78564453125,
    "probe_delta_L43": 608.066650390625,
    "probe_delta_L55": 357.238525390625
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.49750000000000005,
    "probe_delta_L31": 107.91131591796875,
    "probe_delta_L43": 561.0277709960938,
    "probe_delta_L55": 1086.75439453125
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 71.0582275390625,
    "probe_delta_L43": 355.4105224609375,
    "probe_delta_L55": 570.555419921875
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.09999999999999998,
    "probe_delta_L31": -5.5848388671875,
    "probe_delta_L43": 89.15234375,
    "probe_delta_L55": 221.621337890625
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "crossed_cheese_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.23500000000000001,
    "probe_delta_L31": 8.3800048828125,
    "probe_delta_L43": 116.51953125,
    "probe_delta_L55": 602.1904296875
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.9143939393939393,
    "probe_delta_L31": 235.4757080078125,
    "probe_delta_L43": 602.0442504882812,
    "probe_delta_L55": 1270.2545166015625
  },
  {
    "prompt_id": "cheese_pos_value",
    "target_topic": "cheese",
    "target_task_id": "hidden_cheese_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.292191435768262,
    "probe_delta_L31": 161.6837158203125,
    "probe_delta_L43": 549.409423828125,
    "probe_delta_L55": 645.838623046875
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.75,
    "probe_delta_L31": -160.012939453125,
    "probe_delta_L43": 79.4239501953125,
    "probe_delta_L55": 209.051025390625
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.3425,
    "probe_delta_L31": 59.1563720703125,
    "probe_delta_L43": 397.75762939453125,
    "probe_delta_L55": 1310.970947265625
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.9675,
    "probe_delta_L31": 9.64617919921875,
    "probe_delta_L43": 587.28955078125,
    "probe_delta_L55": 1763.118408203125
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -170.242919921875,
    "probe_delta_L43": -95.9732666015625,
    "probe_delta_L55": 326.48388671875
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0025,
    "probe_delta_L31": 14.7442626953125,
    "probe_delta_L43": 196.3782958984375,
    "probe_delta_L55": 1521.8302001953125
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.285,
    "probe_delta_L31": 37.706787109375,
    "probe_delta_L43": 755.0072021484375,
    "probe_delta_L55": 1075.33642578125
  },
  {
    "prompt_id": "rainy_weather_neg_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.4798994974874372,
    "probe_delta_L31": -327.3336181640625,
    "probe_delta_L43": 26.90380859375,
    "probe_delta_L55": 309.279052734375
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.25,
    "probe_delta_L31": 217.4017333984375,
    "probe_delta_L43": 588.5086669921875,
    "probe_delta_L55": 1154.824951171875
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.5625,
    "probe_delta_L31": 267.92291259765625,
    "probe_delta_L43": 1180.767822265625,
    "probe_delta_L55": 2053.912841796875
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.03249999999999997,
    "probe_delta_L31": 191.5250244140625,
    "probe_delta_L43": 1029.3731689453125,
    "probe_delta_L55": 1737.1201171875
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 89.9974365234375,
    "probe_delta_L43": 373.51220703125,
    "probe_delta_L55": 708.73291015625
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.049868766404199474,
    "probe_delta_L31": 87.8692626953125,
    "probe_delta_L43": 183.4705810546875,
    "probe_delta_L55": 615.1986083984375
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.7150000000000001,
    "probe_delta_L31": 214.8155517578125,
    "probe_delta_L43": 842.9810791015625,
    "probe_delta_L55": 368.524658203125
  },
  {
    "prompt_id": "rainy_weather_pos_persona",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5201005025125628,
    "probe_delta_L31": 73.453125,
    "probe_delta_L43": 842.60498046875,
    "probe_delta_L55": 1214.4814453125
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.04500000000000004,
    "probe_delta_L31": -115.230224609375,
    "probe_delta_L43": 174.929443359375,
    "probe_delta_L55": -169.8673095703125
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.08500000000000002,
    "probe_delta_L31": -25.00445556640625,
    "probe_delta_L43": 475.26373291015625,
    "probe_delta_L55": 806.3655395507812
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.885,
    "probe_delta_L31": -74.36810302734375,
    "probe_delta_L43": 452.8778076171875,
    "probe_delta_L55": 408.194580078125
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 12.3465576171875,
    "probe_delta_L43": 47.577880859375,
    "probe_delta_L55": 567.350341796875
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": -30.77899169921875,
    "probe_delta_L43": -152.28662109375,
    "probe_delta_L55": -440.0625
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.25999999999999995,
    "probe_delta_L31": -38.1712646484375,
    "probe_delta_L43": 418.4417724609375,
    "probe_delta_L55": 521.0989990234375
  },
  {
    "prompt_id": "rainy_weather_neg_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.3298994974874372,
    "probe_delta_L31": -268.47296142578125,
    "probe_delta_L43": -30.704833984375,
    "probe_delta_L55": -335.635498046875
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.25,
    "probe_delta_L31": 145.03515625,
    "probe_delta_L43": 728.6370849609375,
    "probe_delta_L55": 376.703369140625
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.5475,
    "probe_delta_L31": 198.49468994140625,
    "probe_delta_L43": 668.7053833007812,
    "probe_delta_L55": 1974.2041015625
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.03249999999999997,
    "probe_delta_L31": 103.036376953125,
    "probe_delta_L43": 412.337646484375,
    "probe_delta_L55": 491.07666015625
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 55.480712890625,
    "probe_delta_L43": 214.3463134765625,
    "probe_delta_L55": 346.156005859375
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.06818181818181818,
    "probe_delta_L31": -4.04229736328125,
    "probe_delta_L43": 85.2589111328125,
    "probe_delta_L55": 171.22802734375
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.7150000000000001,
    "probe_delta_L31": 138.6109619140625,
    "probe_delta_L43": 457.85791015625,
    "probe_delta_L55": 424.74951171875
  },
  {
    "prompt_id": "rainy_weather_pos_experiential",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5201005025125628,
    "probe_delta_L31": 56.16259765625,
    "probe_delta_L43": 851.510986328125,
    "probe_delta_L55": 582.99365234375
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.25,
    "probe_delta_L31": -7.8232421875,
    "probe_delta_L43": 519.1697998046875,
    "probe_delta_L55": 511.8673095703125
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.29250000000000004,
    "probe_delta_L31": 32.51605224609375,
    "probe_delta_L43": 521.5405883789062,
    "probe_delta_L55": 926.3523559570312
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.9425,
    "probe_delta_L31": 102.01513671875,
    "probe_delta_L43": 924.5777587890625,
    "probe_delta_L55": 1258.401123046875
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.975,
    "probe_delta_L31": 18.3284912109375,
    "probe_delta_L43": 13.1856689453125,
    "probe_delta_L55": 271.4365234375
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": 117.8079833984375,
    "probe_delta_L43": 255.1795654296875,
    "probe_delta_L55": 683.6485595703125
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.25999999999999995,
    "probe_delta_L31": 73.7181396484375,
    "probe_delta_L43": 560.0216064453125,
    "probe_delta_L55": 387.543212890625
  },
  {
    "prompt_id": "rainy_weather_neg_value",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.4548994974874372,
    "probe_delta_L31": -176.33349609375,
    "probe_delta_L43": 204.9879150390625,
    "probe_delta_L55": -95.57861328125
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.25,
    "probe_delta_L31": 139.7672119140625,
    "probe_delta_L43": 585.2974853515625,
    "probe_delta_L55": 438.2674560546875
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.5875,
    "probe_delta_L31": 130.3839111328125,
    "probe_delta_L43": 314.41766357421875,
    "probe_delta_L55": 1085.665771484375
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.03249999999999997,
    "probe_delta_L31": 60.8568115234375,
    "probe_delta_L43": 250.2259521484375,
    "probe_delta_L55": 230.2427978515625
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 6.841552734375,
    "probe_delta_L43": 187.3824462890625,
    "probe_delta_L55": 329.724609375
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "crossed_rainy_weather_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": 40.04345703125,
    "probe_delta_L43": 337.6600341796875,
    "probe_delta_L55": 581.1798095703125
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.665,
    "probe_delta_L31": 144.7823486328125,
    "probe_delta_L43": 204.805419921875,
    "probe_delta_L55": 29.250244140625
  },
  {
    "prompt_id": "rainy_weather_pos_value",
    "target_topic": "rainy_weather",
    "target_task_id": "hidden_rainy_weather_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.4776005025125628,
    "probe_delta_L31": 75.4349365234375,
    "probe_delta_L43": 720.8564453125,
    "probe_delta_L55": 483.796630859375
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.8524999999999999,
    "probe_delta_L31": -99.689453125,
    "probe_delta_L43": 351.3707275390625,
    "probe_delta_L55": 485.10302734375
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.445,
    "probe_delta_L31": -16.53125,
    "probe_delta_L43": 308.68048095703125,
    "probe_delta_L55": 888.3375244140625
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.95,
    "probe_delta_L31": -129.83636474609375,
    "probe_delta_L43": 383.7578125,
    "probe_delta_L55": 1093.78857421875
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.95,
    "probe_delta_L31": -381.402587890625,
    "probe_delta_L43": -87.9287109375,
    "probe_delta_L55": 509.645751953125
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -28.75341796875,
    "probe_delta_L43": 343.7127685546875,
    "probe_delta_L55": 1113.8045654296875
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.855,
    "probe_delta_L31": -279.40362548828125,
    "probe_delta_L43": 380.22314453125,
    "probe_delta_L55": 1076.04150390625
  },
  {
    "prompt_id": "cats_neg_persona",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.8825,
    "probe_delta_L31": -359.7999267578125,
    "probe_delta_L43": -47.1112060546875,
    "probe_delta_L55": 350.0963134765625
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.09750000000000003,
    "probe_delta_L31": 279.8900146484375,
    "probe_delta_L43": 866.9556884765625,
    "probe_delta_L55": 674.037353515625
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.4071303258145363,
    "probe_delta_L31": 268.38818359375,
    "probe_delta_L43": 1006.8869018554688,
    "probe_delta_L55": 1779.3609619140625
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 129.3319091796875,
    "probe_delta_L43": 825.75537109375,
    "probe_delta_L55": 1344.565185546875
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.050000000000000044,
    "probe_delta_L31": 41.458740234375,
    "probe_delta_L43": 380.106689453125,
    "probe_delta_L55": 805.60400390625
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0012467191601049851,
    "probe_delta_L31": 41.449951171875,
    "probe_delta_L43": 202.19775390625,
    "probe_delta_L55": -63.4415283203125
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.14500000000000002,
    "probe_delta_L31": 165.834228515625,
    "probe_delta_L43": 971.3497314453125,
    "probe_delta_L55": 1260.474365234375
  },
  {
    "prompt_id": "cats_pos_persona",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.11750000000000005,
    "probe_delta_L31": 135.666259765625,
    "probe_delta_L43": 889.32275390625,
    "probe_delta_L55": 930.9493408203125
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.05249999999999999,
    "probe_delta_L31": -47.4559326171875,
    "probe_delta_L43": 422.4716796875,
    "probe_delta_L55": -21.8570556640625
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.08000000000000002,
    "probe_delta_L31": -7.6251220703125,
    "probe_delta_L43": 421.65484619140625,
    "probe_delta_L55": 422.3341064453125
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.10499999999999998,
    "probe_delta_L31": 6.8614501953125,
    "probe_delta_L43": 457.8939208984375,
    "probe_delta_L55": 500.5811767578125
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.19499999999999995,
    "probe_delta_L31": -257.97314453125,
    "probe_delta_L43": -123.948974609375,
    "probe_delta_L55": 249.50537109375
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.025,
    "probe_delta_L31": 50.927001953125,
    "probe_delta_L43": 175.0115966796875,
    "probe_delta_L55": -135.690673828125
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.83,
    "probe_delta_L31": -206.823486328125,
    "probe_delta_L43": 275.8997802734375,
    "probe_delta_L55": 733.956298828125
  },
  {
    "prompt_id": "cats_neg_experiential",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.7075,
    "probe_delta_L31": -248.42974853515625,
    "probe_delta_L43": -95.0743408203125,
    "probe_delta_L55": -80.1143798828125
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.09500000000000008,
    "probe_delta_L31": 269.7794189453125,
    "probe_delta_L43": 929.135009765625,
    "probe_delta_L55": 1001.35400390625
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.21749999999999997,
    "probe_delta_L31": 220.8226318359375,
    "probe_delta_L43": 395.05804443359375,
    "probe_delta_L55": 1273.2625732421875
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 111.6485595703125,
    "probe_delta_L43": 412.88818359375,
    "probe_delta_L55": 422.877685546875
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.050000000000000044,
    "probe_delta_L31": 38.272705078125,
    "probe_delta_L43": 251.65185546875,
    "probe_delta_L55": 258.89697265625
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.025,
    "probe_delta_L31": 74.7535400390625,
    "probe_delta_L43": 340.247802734375,
    "probe_delta_L55": 44.7987060546875
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.14500000000000002,
    "probe_delta_L31": 190.3665771484375,
    "probe_delta_L43": 846.3782958984375,
    "probe_delta_L55": 1614.585205078125
  },
  {
    "prompt_id": "cats_pos_experiential",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.11750000000000005,
    "probe_delta_L31": 209.0830078125,
    "probe_delta_L43": 990.359619140625,
    "probe_delta_L55": 1037.7750244140625
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.5774999999999999,
    "probe_delta_L31": 115.0880126953125,
    "probe_delta_L43": 461.2484130859375,
    "probe_delta_L55": 478.4892578125
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.32,
    "probe_delta_L31": 11.025634765625,
    "probe_delta_L43": 313.57916259765625,
    "probe_delta_L55": 602.1416015625
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.87,
    "probe_delta_L31": 3.5069580078125,
    "probe_delta_L43": 449.9930419921875,
    "probe_delta_L55": 766.738525390625
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.9249999999999999,
    "probe_delta_L31": -272.43255615234375,
    "probe_delta_L43": -104.0302734375,
    "probe_delta_L55": 59.7847900390625
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.0024436090225563936,
    "probe_delta_L31": 103.9664306640625,
    "probe_delta_L43": 456.380126953125,
    "probe_delta_L55": 344.338623046875
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.7999999999999999,
    "probe_delta_L31": -219.03753662109375,
    "probe_delta_L43": 259.8443603515625,
    "probe_delta_L55": 970.84423828125
  },
  {
    "prompt_id": "cats_neg_value",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.8524999999999999,
    "probe_delta_L31": -236.7647705078125,
    "probe_delta_L43": -73.86962890625,
    "probe_delta_L55": 234.4010009765625
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.039999999999999925,
    "probe_delta_L31": 197.31005859375,
    "probe_delta_L43": 633.337158203125,
    "probe_delta_L55": 1085.572998046875
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.2475,
    "probe_delta_L31": 147.39794921875,
    "probe_delta_L43": 317.40118408203125,
    "probe_delta_L55": 621.0159912109375
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 75.7623291015625,
    "probe_delta_L43": 305.386474609375,
    "probe_delta_L55": 304.963134765625
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.050000000000000044,
    "probe_delta_L31": 36.8251953125,
    "probe_delta_L43": 149.03759765625,
    "probe_delta_L55": 292.31103515625
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "crossed_cats_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.004899497487437187,
    "probe_delta_L31": 108.1158447265625,
    "probe_delta_L43": 560.0517578125,
    "probe_delta_L55": 313.547119140625
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.14500000000000002,
    "probe_delta_L31": 131.068359375,
    "probe_delta_L43": 409.5625,
    "probe_delta_L55": 746.02587890625
  },
  {
    "prompt_id": "cats_pos_value",
    "target_topic": "cats",
    "target_task_id": "hidden_cats_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.11750000000000005,
    "probe_delta_L31": 181.6453857421875,
    "probe_delta_L43": 914.491943359375,
    "probe_delta_L55": 1181.8817138671875
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.855,
    "probe_delta_L31": -124.50531005859375,
    "probe_delta_L43": 162.048583984375,
    "probe_delta_L55": 336.4417724609375
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.225,
    "probe_delta_L31": 23.5360107421875,
    "probe_delta_L43": 445.567626953125,
    "probe_delta_L55": 1941.5640869140625
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -1.0,
    "probe_delta_L31": 16.9945068359375,
    "probe_delta_L43": 625.210693359375,
    "probe_delta_L55": 1104.802001953125
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.78,
    "probe_delta_L31": -128.9432373046875,
    "probe_delta_L43": -26.541015625,
    "probe_delta_L55": 58.4033203125
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -40.44903564453125,
    "probe_delta_L43": 164.1614990234375,
    "probe_delta_L55": -215.32568359375
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.45,
    "probe_delta_L31": -43.1070556640625,
    "probe_delta_L43": 82.4637451171875,
    "probe_delta_L55": 431.808837890625
  },
  {
    "prompt_id": "classical_music_neg_persona",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.1625,
    "probe_delta_L31": 16.40771484375,
    "probe_delta_L43": 618.9281005859375,
    "probe_delta_L55": 1468.297607421875
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.3125,
    "probe_delta_L31": -53.6148681640625,
    "probe_delta_L43": 364.6983642578125,
    "probe_delta_L55": 264.5213623046875
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.51,
    "probe_delta_L31": 235.89892578125,
    "probe_delta_L43": 956.3665771484375,
    "probe_delta_L55": 1117.7291259765625
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 60.08447265625,
    "probe_delta_L43": 699.621337890625,
    "probe_delta_L55": 401.869140625
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.21999999999999997,
    "probe_delta_L31": 106.5433349609375,
    "probe_delta_L43": 211.5965576171875,
    "probe_delta_L55": -229.087890625
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 7.54241943359375,
    "probe_delta_L43": -97.7718505859375,
    "probe_delta_L55": -872.5588989257812
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.55,
    "probe_delta_L31": 150.8887939453125,
    "probe_delta_L43": 277.7703857421875,
    "probe_delta_L55": 138.21435546875
  },
  {
    "prompt_id": "classical_music_pos_persona",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.8125,
    "probe_delta_L31": 342.35595703125,
    "probe_delta_L43": 1176.8841552734375,
    "probe_delta_L55": 963.1085815429688
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.09499999999999997,
    "probe_delta_L31": 25.74920654296875,
    "probe_delta_L43": 253.471435546875,
    "probe_delta_L55": -189.72607421875
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.06,
    "probe_delta_L31": 82.57122802734375,
    "probe_delta_L43": 185.0147705078125,
    "probe_delta_L55": 642.3685302734375
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": 50.52685546875,
    "probe_delta_L43": 306.607666015625,
    "probe_delta_L55": 665.300537109375
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.78,
    "probe_delta_L31": -46.57373046875,
    "probe_delta_L43": -293.057861328125,
    "probe_delta_L55": -473.5889892578125
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0175,
    "probe_delta_L31": -17.4107666015625,
    "probe_delta_L43": -133.634765625,
    "probe_delta_L55": -383.363525390625
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.425,
    "probe_delta_L31": -30.13134765625,
    "probe_delta_L43": -236.1591796875,
    "probe_delta_L55": -290.1229248046875
  },
  {
    "prompt_id": "classical_music_neg_experiential",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.1625,
    "probe_delta_L31": 97.0816650390625,
    "probe_delta_L43": 323.3726806640625,
    "probe_delta_L55": 468.03680419921875
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.6725,
    "probe_delta_L31": 90.50433349609375,
    "probe_delta_L43": 828.1510009765625,
    "probe_delta_L55": 666.403564453125
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.2175,
    "probe_delta_L31": 147.5203857421875,
    "probe_delta_L43": 228.76513671875,
    "probe_delta_L55": 869.8663330078125
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 126.78515625,
    "probe_delta_L43": 133.189208984375,
    "probe_delta_L55": 440.548583984375
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.21999999999999997,
    "probe_delta_L31": 97.144287109375,
    "probe_delta_L43": 129.236083984375,
    "probe_delta_L55": 294.833251953125
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -49.2230224609375,
    "probe_delta_L43": -41.322998046875,
    "probe_delta_L55": -608.387939453125
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.55,
    "probe_delta_L31": 157.2083740234375,
    "probe_delta_L43": 174.46728515625,
    "probe_delta_L55": 664.983154296875
  },
  {
    "prompt_id": "classical_music_pos_experiential",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.7875,
    "probe_delta_L31": 316.9495849609375,
    "probe_delta_L43": 1151.9237060546875,
    "probe_delta_L55": 1541.80712890625
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.26750000000000007,
    "probe_delta_L31": 51.28216552734375,
    "probe_delta_L43": 94.83203125,
    "probe_delta_L55": -299.94775390625
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.2,
    "probe_delta_L31": 66.67169189453125,
    "probe_delta_L43": 172.9888916015625,
    "probe_delta_L55": 763.9664306640625
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.925,
    "probe_delta_L31": 60.1353759765625,
    "probe_delta_L43": 368.060302734375,
    "probe_delta_L55": 731.146240234375
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.755,
    "probe_delta_L31": -191.51116943359375,
    "probe_delta_L43": -236.0654296875,
    "probe_delta_L55": 38.538330078125
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.007518796992481203,
    "probe_delta_L31": -43.7041015625,
    "probe_delta_L43": -28.729736328125,
    "probe_delta_L55": -375.02734375
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.425,
    "probe_delta_L31": -97.7115478515625,
    "probe_delta_L43": -15.4305419921875,
    "probe_delta_L55": 441.799560546875
  },
  {
    "prompt_id": "classical_music_neg_value",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.1375,
    "probe_delta_L31": 67.93133544921875,
    "probe_delta_L43": 254.719970703125,
    "probe_delta_L55": 358.79571533203125
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.15749999999999997,
    "probe_delta_L31": 38.27655029296875,
    "probe_delta_L43": 598.51416015625,
    "probe_delta_L55": 659.2568359375
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.625,
    "probe_delta_L31": 71.5518798828125,
    "probe_delta_L43": 140.854736328125,
    "probe_delta_L55": 559.270751953125
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 55.14404296875,
    "probe_delta_L43": 97.149169921875,
    "probe_delta_L55": 147.24560546875
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.21999999999999997,
    "probe_delta_L31": 71.2354736328125,
    "probe_delta_L43": -122.3673095703125,
    "probe_delta_L55": -104.763671875
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "crossed_classical_music_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -66.19549560546875,
    "probe_delta_L43": -70.631591796875,
    "probe_delta_L55": -584.8114013671875
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.55,
    "probe_delta_L31": 101.8348388671875,
    "probe_delta_L43": -4.1317138671875,
    "probe_delta_L55": 285.697021484375
  },
  {
    "prompt_id": "classical_music_pos_value",
    "target_topic": "classical_music",
    "target_task_id": "hidden_classical_music_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.785,
    "probe_delta_L31": 155.5755615234375,
    "probe_delta_L43": 705.9205322265625,
    "probe_delta_L55": 813.2490844726562
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.955,
    "probe_delta_L31": -204.11016845703125,
    "probe_delta_L43": -64.15771484375,
    "probe_delta_L55": -111.95556640625
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.2075,
    "probe_delta_L31": 66.1373291015625,
    "probe_delta_L43": 1027.795654296875,
    "probe_delta_L55": 2036.405517578125
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -160.786376953125,
    "probe_delta_L43": 461.6922607421875,
    "probe_delta_L55": 1116.5048828125
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -507.562255859375,
    "probe_delta_L43": -259.197265625,
    "probe_delta_L55": -118.010498046875
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.025,
    "probe_delta_L31": 10.0386962890625,
    "probe_delta_L43": 271.14794921875,
    "probe_delta_L55": -431.5731201171875
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.5,
    "probe_delta_L31": -188.92010498046875,
    "probe_delta_L43": 665.1093139648438,
    "probe_delta_L55": 1105.89892578125
  },
  {
    "prompt_id": "gardening_neg_persona",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.6473551637279596,
    "probe_delta_L31": -291.60589599609375,
    "probe_delta_L43": 126.7342529296875,
    "probe_delta_L55": 147.64404296875
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 197.8641357421875,
    "probe_delta_L43": 470.121826171875,
    "probe_delta_L55": 214.070068359375
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.7424999999999999,
    "probe_delta_L31": 303.054931640625,
    "probe_delta_L43": 1435.4827880859375,
    "probe_delta_L55": 2246.968505859375
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 77.982666015625,
    "probe_delta_L43": 904.8714599609375,
    "probe_delta_L55": 1031.54443359375
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 18.0169677734375,
    "probe_delta_L43": 341.609619140625,
    "probe_delta_L55": 470.190673828125
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0006410256410256387,
    "probe_delta_L31": 65.2734375,
    "probe_delta_L43": 182.93408203125,
    "probe_delta_L55": -489.3690185546875
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5,
    "probe_delta_L31": 271.621337890625,
    "probe_delta_L43": 1326.078857421875,
    "probe_delta_L55": 1173.1376953125
  },
  {
    "prompt_id": "gardening_pos_persona",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.35264483627204035,
    "probe_delta_L31": 185.6910400390625,
    "probe_delta_L43": 839.113525390625,
    "probe_delta_L55": 639.946533203125
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.07750000000000001,
    "probe_delta_L31": 66.6961669921875,
    "probe_delta_L43": 21.644287109375,
    "probe_delta_L55": -578.208740234375
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.0925,
    "probe_delta_L31": 157.0169677734375,
    "probe_delta_L43": 705.565185546875,
    "probe_delta_L55": 901.6526489257812
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": -48.4488525390625,
    "probe_delta_L43": 207.9732666015625,
    "probe_delta_L55": 808.520263671875
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": -191.736328125,
    "probe_delta_L43": -414.8941650390625,
    "probe_delta_L55": -545.3355712890625
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0002525252525252507,
    "probe_delta_L31": 18.3323974609375,
    "probe_delta_L43": -43.232177734375,
    "probe_delta_L55": 27.165283203125
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.09499999999999997,
    "probe_delta_L31": 68.4622802734375,
    "probe_delta_L43": 401.00653076171875,
    "probe_delta_L55": 244.2137451171875
  },
  {
    "prompt_id": "gardening_neg_experiential",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.32764483627204033,
    "probe_delta_L31": 44.18359375,
    "probe_delta_L43": 76.061279296875,
    "probe_delta_L55": -403.1212158203125
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 302.5673828125,
    "probe_delta_L43": 862.23974609375,
    "probe_delta_L55": 916.15966796875
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.6625,
    "probe_delta_L31": 214.9049072265625,
    "probe_delta_L43": 431.4394836425781,
    "probe_delta_L55": 1017.3838500976562
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 77.0291748046875,
    "probe_delta_L43": 267.1318359375,
    "probe_delta_L55": 503.332763671875
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 40.39013671875,
    "probe_delta_L43": 200.128173828125,
    "probe_delta_L55": 357.809814453125
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.004899497487437187,
    "probe_delta_L31": 62.260009765625,
    "probe_delta_L43": 100.752685546875,
    "probe_delta_L55": 123.0550537109375
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5,
    "probe_delta_L31": 303.83642578125,
    "probe_delta_L43": 1135.09130859375,
    "probe_delta_L55": 1622.2236328125
  },
  {
    "prompt_id": "gardening_pos_experiential",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.35264483627204035,
    "probe_delta_L31": 237.716064453125,
    "probe_delta_L43": 1122.202392578125,
    "probe_delta_L55": 1007.990234375
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.19999999999999996,
    "probe_delta_L31": 111.526611328125,
    "probe_delta_L43": -161.8836669921875,
    "probe_delta_L55": -669.5738525390625
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.15749999999999997,
    "probe_delta_L31": 60.71502685546875,
    "probe_delta_L43": 526.34912109375,
    "probe_delta_L55": 948.0009155273438
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.725,
    "probe_delta_L31": -6.9700927734375,
    "probe_delta_L43": 404.12451171875,
    "probe_delta_L55": 790.23779296875
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.85,
    "probe_delta_L31": -260.308837890625,
    "probe_delta_L43": -212.290283203125,
    "probe_delta_L55": -292.170654296875
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.004274611398963731,
    "probe_delta_L31": 48.4925537109375,
    "probe_delta_L43": 154.048828125,
    "probe_delta_L55": 48.6036376953125
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.475,
    "probe_delta_L31": -164.717529296875,
    "probe_delta_L43": 502.43084716796875,
    "probe_delta_L55": 484.953125
  },
  {
    "prompt_id": "gardening_neg_value",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.5723551637279597,
    "probe_delta_L31": -175.7652587890625,
    "probe_delta_L43": -135.45068359375,
    "probe_delta_L55": -795.9979248046875
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.025000000000000022,
    "probe_delta_L31": 224.3431396484375,
    "probe_delta_L43": 579.950439453125,
    "probe_delta_L55": 927.58447265625
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.535,
    "probe_delta_L31": 137.836669921875,
    "probe_delta_L43": 440.7931823730469,
    "probe_delta_L55": 917.4296264648438
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 38.866455078125,
    "probe_delta_L43": 292.42431640625,
    "probe_delta_L55": 498.42138671875
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 36.21240234375,
    "probe_delta_L43": 146.59521484375,
    "probe_delta_L55": 334.15673828125
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "crossed_gardening_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.025,
    "probe_delta_L31": 33.897705078125,
    "probe_delta_L43": 163.9066162109375,
    "probe_delta_L55": -48.912109375
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5,
    "probe_delta_L31": 193.7093505859375,
    "probe_delta_L43": 805.8513793945312,
    "probe_delta_L55": 1245.1005859375
  },
  {
    "prompt_id": "gardening_pos_value",
    "target_topic": "gardening",
    "target_task_id": "hidden_gardening_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.34264483627204034,
    "probe_delta_L31": 192.1693115234375,
    "probe_delta_L43": 725.647216796875,
    "probe_delta_L55": 358.437255859375
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.9725,
    "probe_delta_L31": -353.76092529296875,
    "probe_delta_L43": -236.0787353515625,
    "probe_delta_L55": 357.008544921875
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.15,
    "probe_delta_L31": 75.00152587890625,
    "probe_delta_L43": 822.8236694335938,
    "probe_delta_L55": 2282.765380859375
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -117.5712890625,
    "probe_delta_L43": 592.376708984375,
    "probe_delta_L55": 1492.7142333984375
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -410.1480712890625,
    "probe_delta_L43": -152.1898193359375,
    "probe_delta_L55": 589.107177734375
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.05,
    "probe_delta_L31": 84.7879638671875,
    "probe_delta_L43": 64.5037841796875,
    "probe_delta_L55": 758.268798828125
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.8266331658291457,
    "probe_delta_L31": -264.247314453125,
    "probe_delta_L43": 240.5587158203125,
    "probe_delta_L55": 513.451904296875
  },
  {
    "prompt_id": "astronomy_neg_persona",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.6075,
    "probe_delta_L31": -175.5181884765625,
    "probe_delta_L43": 434.9459228515625,
    "probe_delta_L55": 1034.387451171875
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.022500000000000075,
    "probe_delta_L31": -27.1470947265625,
    "probe_delta_L43": 157.223876953125,
    "probe_delta_L55": 279.073974609375
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.42499999999999993,
    "probe_delta_L31": 190.42181396484375,
    "probe_delta_L43": 1158.34228515625,
    "probe_delta_L55": 2030.9573974609375
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 58.886474609375,
    "probe_delta_L43": 758.10009765625,
    "probe_delta_L55": 1488.8236083984375
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": -67.3338623046875,
    "probe_delta_L43": 309.85546875,
    "probe_delta_L55": 133.784423828125
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -10.791748046875,
    "probe_delta_L43": 224.2491455078125,
    "probe_delta_L55": 79.276123046875
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.14836683417085428,
    "probe_delta_L31": 104.6036376953125,
    "probe_delta_L43": 681.4124755859375,
    "probe_delta_L55": 532.69580078125
  },
  {
    "prompt_id": "astronomy_pos_persona",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.39249999999999996,
    "probe_delta_L31": 142.69140625,
    "probe_delta_L43": 932.1473388671875,
    "probe_delta_L55": 1261.7421875
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.22000000000000008,
    "probe_delta_L31": -267.392333984375,
    "probe_delta_L43": -322.998779296875,
    "probe_delta_L55": -437.5850830078125
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.075,
    "probe_delta_L31": 39.41973876953125,
    "probe_delta_L43": 698.0455932617188,
    "probe_delta_L55": 1260.500732421875
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.5275000000000001,
    "probe_delta_L31": -118.534912109375,
    "probe_delta_L43": 294.3248291015625,
    "probe_delta_L55": 569.718505859375
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.4225,
    "probe_delta_L31": -265.7769775390625,
    "probe_delta_L43": -81.625244140625,
    "probe_delta_L55": 238.391845703125
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.017543859649122806,
    "probe_delta_L31": -24.1292724609375,
    "probe_delta_L43": -3.865234375,
    "probe_delta_L55": -140.009521484375
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.6766331658291457,
    "probe_delta_L31": -168.2862548828125,
    "probe_delta_L43": 10.6968994140625,
    "probe_delta_L55": 332.514892578125
  },
  {
    "prompt_id": "astronomy_neg_experiential",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.5075000000000001,
    "probe_delta_L31": -107.43017578125,
    "probe_delta_L43": 68.9837646484375,
    "probe_delta_L55": 192.2568359375
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.0024999999999999467,
    "probe_delta_L31": 104.9566650390625,
    "probe_delta_L43": 711.711181640625,
    "probe_delta_L55": 1353.212158203125
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.395,
    "probe_delta_L31": 143.9066162109375,
    "probe_delta_L43": 274.89654541015625,
    "probe_delta_L55": 1029.917236328125
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 40.245361328125,
    "probe_delta_L43": 193.082275390625,
    "probe_delta_L55": 336.6375732421875
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -52.301025390625,
    "probe_delta_L43": 253.734375,
    "probe_delta_L55": 326.048828125
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -21.7342529296875,
    "probe_delta_L43": 60.43017578125,
    "probe_delta_L55": 3.061279296875
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.1733668341708543,
    "probe_delta_L31": 115.7239990234375,
    "probe_delta_L43": 376.6153564453125,
    "probe_delta_L55": 723.484375
  },
  {
    "prompt_id": "astronomy_pos_experiential",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.39249999999999996,
    "probe_delta_L31": 232.38525390625,
    "probe_delta_L43": 1202.8826904296875,
    "probe_delta_L55": 1558.46923828125
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.8725,
    "probe_delta_L31": -129.9232177734375,
    "probe_delta_L43": -424.841796875,
    "probe_delta_L55": -464.5859375
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.125,
    "probe_delta_L31": 54.7847900390625,
    "probe_delta_L43": 202.60009765625,
    "probe_delta_L55": 520.70166015625
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.95,
    "probe_delta_L31": -5.785888671875,
    "probe_delta_L43": 362.6217041015625,
    "probe_delta_L55": 750.7528076171875
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.975,
    "probe_delta_L31": -168.369140625,
    "probe_delta_L43": -274.65185546875,
    "probe_delta_L55": 66.23876953125
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": 97.3319091796875,
    "probe_delta_L43": 284.445068359375,
    "probe_delta_L55": 641.081787109375
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.8016331658291457,
    "probe_delta_L31": -85.4376220703125,
    "probe_delta_L43": -38.832763671875,
    "probe_delta_L55": 632.30810546875
  },
  {
    "prompt_id": "astronomy_neg_value",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.5575,
    "probe_delta_L31": -82.1151123046875,
    "probe_delta_L43": -33.3404541015625,
    "probe_delta_L55": -165.3358154296875
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.0475000000000001,
    "probe_delta_L31": 12.05615234375,
    "probe_delta_L43": 242.28662109375,
    "probe_delta_L55": 565.245361328125
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.5225,
    "probe_delta_L31": 66.3338623046875,
    "probe_delta_L43": 219.4266357421875,
    "probe_delta_L55": 822.0357666015625
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 50.5341796875,
    "probe_delta_L43": 235.5904541015625,
    "probe_delta_L55": 311.572509765625
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -33.6024169921875,
    "probe_delta_L43": 23.494873046875,
    "probe_delta_L55": 336.660400390625
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "crossed_astronomy_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 36.8277587890625,
    "probe_delta_L43": 170.101318359375,
    "probe_delta_L55": 325.2672119140625
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.12336683417085426,
    "probe_delta_L31": 90.234130859375,
    "probe_delta_L43": 133.7816162109375,
    "probe_delta_L55": 295.987548828125
  },
  {
    "prompt_id": "astronomy_pos_value",
    "target_topic": "astronomy",
    "target_task_id": "hidden_astronomy_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.3175,
    "probe_delta_L31": 146.8524169921875,
    "probe_delta_L43": 994.0555419921875,
    "probe_delta_L55": 1131.109619140625
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.905,
    "probe_delta_L31": -251.70025634765625,
    "probe_delta_L43": -192.0164794921875,
    "probe_delta_L55": 229.53564453125
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.20750000000000002,
    "probe_delta_L31": 71.41314697265625,
    "probe_delta_L43": 565.6387329101562,
    "probe_delta_L55": 1327.825439453125
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.975,
    "probe_delta_L31": -189.7894287109375,
    "probe_delta_L43": 466.450439453125,
    "probe_delta_L55": 1221.4754638671875
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -292.09698486328125,
    "probe_delta_L43": 81.986328125,
    "probe_delta_L55": -232.6328125
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": 15.6990966796875,
    "probe_delta_L43": 175.796875,
    "probe_delta_L55": -363.6683349609375
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.0775,
    "probe_delta_L31": -131.32391357421875,
    "probe_delta_L43": 351.3895263671875,
    "probe_delta_L55": 594.9305419921875
  },
  {
    "prompt_id": "cooking_neg_persona",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.6926952141057935,
    "probe_delta_L31": -358.50836181640625,
    "probe_delta_L43": -36.6083984375,
    "probe_delta_L55": 381.312255859375
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.0050000000000000044,
    "probe_delta_L31": 172.9979248046875,
    "probe_delta_L43": 212.314453125,
    "probe_delta_L55": 427.126220703125
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.645,
    "probe_delta_L31": 191.65777587890625,
    "probe_delta_L43": 697.8908081054688,
    "probe_delta_L55": 970.6787109375
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 16.8758544921875,
    "probe_delta_L43": 569.2200927734375,
    "probe_delta_L55": 5.276123046875
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -7.828125,
    "probe_delta_L43": 339.855224609375,
    "probe_delta_L55": 297.664306640625
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.04050632911392405,
    "probe_delta_L31": 76.98675537109375,
    "probe_delta_L43": 79.9124755859375,
    "probe_delta_L55": -391.80078125
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.9225,
    "probe_delta_L31": 248.422119140625,
    "probe_delta_L43": 1067.351318359375,
    "probe_delta_L55": 1114.469970703125
  },
  {
    "prompt_id": "cooking_pos_persona",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.3073047858942065,
    "probe_delta_L31": 95.7388916015625,
    "probe_delta_L43": 667.7786865234375,
    "probe_delta_L55": 719.844482421875
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.020000000000000018,
    "probe_delta_L31": -31.5284423828125,
    "probe_delta_L43": -348.045654296875,
    "probe_delta_L55": -731.08154296875
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.39249999999999996,
    "probe_delta_L31": 25.3001708984375,
    "probe_delta_L43": -0.95989990234375,
    "probe_delta_L55": -364.794921875
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": -98.3447265625,
    "probe_delta_L43": 423.9578857421875,
    "probe_delta_L55": 524.14453125
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.3375,
    "probe_delta_L31": -182.9447021484375,
    "probe_delta_L43": -107.3990478515625,
    "probe_delta_L55": -119.540283203125
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 36.8258056640625,
    "probe_delta_L43": -44.5009765625,
    "probe_delta_L55": -570.6264038085938
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.0525,
    "probe_delta_L31": 4.35589599609375,
    "probe_delta_L43": 183.993408203125,
    "probe_delta_L55": -331.2193603515625
  },
  {
    "prompt_id": "cooking_neg_experiential",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.23230478589420656,
    "probe_delta_L31": -163.477294921875,
    "probe_delta_L43": -200.7401123046875,
    "probe_delta_L55": -731.1514892578125
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.020000000000000018,
    "probe_delta_L31": 266.99560546875,
    "probe_delta_L43": 677.49462890625,
    "probe_delta_L55": 1269.78955078125
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.26,
    "probe_delta_L31": 169.25274658203125,
    "probe_delta_L43": 250.8240966796875,
    "probe_delta_L55": 486.0223388671875
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.09999999999999998,
    "probe_delta_L31": 33.88525390625,
    "probe_delta_L43": 409.0400390625,
    "probe_delta_L55": 770.5560302734375
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.0024999999999999467,
    "probe_delta_L31": 40.57421875,
    "probe_delta_L43": 247.589599609375,
    "probe_delta_L55": 410.0087890625
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 26.57891845703125,
    "probe_delta_L43": 89.668212890625,
    "probe_delta_L55": -279.6146240234375
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.8975,
    "probe_delta_L31": 188.3304443359375,
    "probe_delta_L43": 1004.7952880859375,
    "probe_delta_L55": 1272.885986328125
  },
  {
    "prompt_id": "cooking_pos_experiential",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.3073047858942065,
    "probe_delta_L31": 118.1007080078125,
    "probe_delta_L43": 1001.2054443359375,
    "probe_delta_L55": 1191.244140625
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.03749999999999998,
    "probe_delta_L31": 77.5125732421875,
    "probe_delta_L43": 230.1571044921875,
    "probe_delta_L55": 398.600830078125
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.10500000000000001,
    "probe_delta_L31": 89.74713134765625,
    "probe_delta_L43": 354.58941650390625,
    "probe_delta_L55": 869.443359375
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.9375,
    "probe_delta_L31": -27.5592041015625,
    "probe_delta_L43": 593.841796875,
    "probe_delta_L55": 760.7530517578125
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.975,
    "probe_delta_L31": -72.280517578125,
    "probe_delta_L43": 206.6573486328125,
    "probe_delta_L55": 370.965087890625
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.025,
    "probe_delta_L31": 40.22760009765625,
    "probe_delta_L43": 97.1925048828125,
    "probe_delta_L55": -455.8983154296875
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.0525,
    "probe_delta_L31": 26.8267822265625,
    "probe_delta_L43": 593.0567626953125,
    "probe_delta_L55": 554.420654296875
  },
  {
    "prompt_id": "cooking_neg_value",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.6676952141057935,
    "probe_delta_L31": -116.244140625,
    "probe_delta_L43": 88.6092529296875,
    "probe_delta_L55": -145.00634765625
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.0050000000000000044,
    "probe_delta_L31": 171.7646484375,
    "probe_delta_L43": 454.7552490234375,
    "probe_delta_L55": 611.195556640625
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.53,
    "probe_delta_L31": 81.98712158203125,
    "probe_delta_L43": 226.638427734375,
    "probe_delta_L55": 447.802490234375
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": 58.7110595703125,
    "probe_delta_L43": 451.955322265625,
    "probe_delta_L55": 572.1104736328125
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 5.6634521484375,
    "probe_delta_L43": 90.98046875,
    "probe_delta_L55": 220.28271484375
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "crossed_cooking_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.007518796992481203,
    "probe_delta_L31": 68.735107421875,
    "probe_delta_L43": 52.0289306640625,
    "probe_delta_L55": -426.378173828125
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.7474999999999999,
    "probe_delta_L31": 132.4112548828125,
    "probe_delta_L43": 804.6942138671875,
    "probe_delta_L55": 1079.4326171875
  },
  {
    "prompt_id": "cooking_pos_value",
    "target_topic": "cooking",
    "target_task_id": "hidden_cooking_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.2823047858942065,
    "probe_delta_L31": 118.073486328125,
    "probe_delta_L43": 776.6517333984375,
    "probe_delta_L55": 663.06005859375
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.67,
    "probe_delta_L31": -332.51226806640625,
    "probe_delta_L43": 232.422119140625,
    "probe_delta_L55": 1184.28857421875
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.17293233082706766,
    "probe_delta_L31": 10.48822021484375,
    "probe_delta_L43": 598.43798828125,
    "probe_delta_L55": 1632.653076171875
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -94.73931884765625,
    "probe_delta_L43": 791.5401611328125,
    "probe_delta_L55": 903.079833984375
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -1.0,
    "probe_delta_L31": -302.84783935546875,
    "probe_delta_L43": 186.9150390625,
    "probe_delta_L55": 450.0107421875
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": -0.025,
    "probe_delta_L31": 72.5028076171875,
    "probe_delta_L43": 275.5087890625,
    "probe_delta_L55": 653.5118408203125
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.9,
    "probe_delta_L31": -298.84381103515625,
    "probe_delta_L43": -39.8380126953125,
    "probe_delta_L55": 790.712646484375
  },
  {
    "prompt_id": "ancient_history_neg_persona",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "negative",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.4375,
    "probe_delta_L31": -218.18109130859375,
    "probe_delta_L43": 530.2740478515625,
    "probe_delta_L55": 1636.4686279296875
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.30499999999999994,
    "probe_delta_L31": 34.156982421875,
    "probe_delta_L43": 426.705810546875,
    "probe_delta_L55": 865.675048828125
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.35206766917293236,
    "probe_delta_L31": 137.98919677734375,
    "probe_delta_L43": 859.1893310546875,
    "probe_delta_L55": 1552.827880859375
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 90.5635986328125,
    "probe_delta_L43": 864.4398193359375,
    "probe_delta_L55": 256.158447265625
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 9.6336669921875,
    "probe_delta_L43": 378.197509765625,
    "probe_delta_L55": 748.448974609375
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.29815521628498726,
    "probe_delta_L31": 99.43017578125,
    "probe_delta_L43": 385.0528564453125,
    "probe_delta_L55": 824.8004150390625
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.09999999999999998,
    "probe_delta_L31": 122.141357421875,
    "probe_delta_L43": 495.8712158203125,
    "probe_delta_L55": 451.304931640625
  },
  {
    "prompt_id": "ancient_history_pos_persona",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "positive",
    "prompt_type": "persona",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5625,
    "probe_delta_L31": 197.8516845703125,
    "probe_delta_L43": 1384.8614501953125,
    "probe_delta_L55": 2140.74365234375
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.22999999999999998,
    "probe_delta_L31": -87.7408447265625,
    "probe_delta_L43": 123.3919677734375,
    "probe_delta_L55": 23.490966796875
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.02706766917293235,
    "probe_delta_L31": 85.86865234375,
    "probe_delta_L43": 334.4378662109375,
    "probe_delta_L55": 751.6747436523438
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": -28.0894775390625,
    "probe_delta_L43": 457.783447265625,
    "probe_delta_L55": 649.977783203125
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.47750000000000004,
    "probe_delta_L31": -109.4691162109375,
    "probe_delta_L43": 115.0159912109375,
    "probe_delta_L55": 704.7939453125
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 120.2083740234375,
    "probe_delta_L43": 396.7493896484375,
    "probe_delta_L55": 924.0318603515625
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.875,
    "probe_delta_L31": -221.61773681640625,
    "probe_delta_L43": 5.699462890625,
    "probe_delta_L55": 204.8082275390625
  },
  {
    "prompt_id": "ancient_history_neg_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "negative",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.08750000000000002,
    "probe_delta_L31": -53.08868408203125,
    "probe_delta_L43": 609.7938232421875,
    "probe_delta_L55": 1256.77734375
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.17999999999999994,
    "probe_delta_L31": 123.245361328125,
    "probe_delta_L43": 767.683837890625,
    "probe_delta_L55": 1308.4599609375
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.10206766917293236,
    "probe_delta_L31": 127.5374755859375,
    "probe_delta_L43": 339.5806884765625,
    "probe_delta_L55": 978.5202026367188
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 99.33251953125,
    "probe_delta_L43": 403.9632568359375,
    "probe_delta_L55": 637.082275390625
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 34.8648681640625,
    "probe_delta_L43": 415.16650390625,
    "probe_delta_L55": 850.594482421875
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.047864321608040196,
    "probe_delta_L31": 118.211669921875,
    "probe_delta_L43": 364.0125732421875,
    "probe_delta_L55": 735.1783447265625
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.09999999999999998,
    "probe_delta_L31": 87.3199462890625,
    "probe_delta_L43": 255.4951171875,
    "probe_delta_L55": 559.84423828125
  },
  {
    "prompt_id": "ancient_history_pos_experiential",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "positive",
    "prompt_type": "experiential",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5375,
    "probe_delta_L31": 209.293701171875,
    "probe_delta_L43": 1491.0950927734375,
    "probe_delta_L55": 2208.228759765625
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": -0.5700000000000001,
    "probe_delta_L31": -198.3931884765625,
    "probe_delta_L43": -34.943359375,
    "probe_delta_L55": -239.0413818359375
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": -0.13793233082706766,
    "probe_delta_L31": -23.047607421875,
    "probe_delta_L43": 351.5782470703125,
    "probe_delta_L55": 347.34942626953125
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.9025,
    "probe_delta_L31": -90.35382080078125,
    "probe_delta_L43": 659.0614013671875,
    "probe_delta_L55": 621.985107421875
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": -0.975,
    "probe_delta_L31": -318.46514892578125,
    "probe_delta_L43": -97.0706787109375,
    "probe_delta_L55": 120.831298828125
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 134.00537109375,
    "probe_delta_L43": 396.4617919921875,
    "probe_delta_L55": 623.9161376953125
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.9,
    "probe_delta_L31": -203.491455078125,
    "probe_delta_L43": 177.6866455078125,
    "probe_delta_L55": 139.5869140625
  },
  {
    "prompt_id": "ancient_history_neg_value",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "negative",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": -0.4375,
    "probe_delta_L31": -172.8587646484375,
    "probe_delta_L43": 384.3641357421875,
    "probe_delta_L55": 725.397216796875
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_math",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "math",
    "behavioral_delta": 0.25749999999999995,
    "probe_delta_L31": 58.9447021484375,
    "probe_delta_L43": 697.96142578125,
    "probe_delta_L55": 977.312744140625
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_coding",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "coding",
    "behavioral_delta": 0.11706766917293232,
    "probe_delta_L31": 65.93048095703125,
    "probe_delta_L43": 238.05877685546875,
    "probe_delta_L55": 467.15850830078125
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_fiction",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "fiction",
    "behavioral_delta": -0.025000000000000022,
    "probe_delta_L31": 40.37744140625,
    "probe_delta_L43": 378.600830078125,
    "probe_delta_L55": 509.372802734375
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_content",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "content_generation",
    "behavioral_delta": 0.0,
    "probe_delta_L31": 75.33740234375,
    "probe_delta_L43": 131.9698486328125,
    "probe_delta_L55": 618.28515625
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "crossed_ancient_history_harmful",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "crossed",
    "category_shell": "harmful",
    "behavioral_delta": 0.07274436090225564,
    "probe_delta_L31": 87.539306640625,
    "probe_delta_L43": 273.1451416015625,
    "probe_delta_L55": 684.7293701171875
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_1",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.07499999999999996,
    "probe_delta_L31": 42.7982177734375,
    "probe_delta_L43": 223.527587890625,
    "probe_delta_L55": 218.3095703125
  },
  {
    "prompt_id": "ancient_history_pos_value",
    "target_topic": "ancient_history",
    "target_task_id": "hidden_ancient_history_2",
    "direction": "positive",
    "prompt_type": "value_laden",
    "task_set": "pure",
    "category_shell": "pure",
    "behavioral_delta": 0.5375,
    "probe_delta_L31": 188.40478515625,
    "probe_delta_L43": 1436.9539794921875,
    "probe_delta_L55": 2109.037353515625
  }
]