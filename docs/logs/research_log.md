# Research Log

**Note**: All experiments use completions generated by **llama-3.1-8b** unless otherwise noted. The rating models vary by experiment.

---

# Seed Sensitivity Analysis

## 2026-01-26: Initial Seed Sensitivity (llama-3.1-8b)

Analyzed how stable preference measurements are across different random seeds.

### Stated Preference Stability (Cross-Seed Correlation)

| Template | Mean r | Std | n pairs |
|----------|--------|-----|---------|
| anchored_1_5 | **0.64** | 0.03 | 6 |
| stated_1_5 | 0.55 | 0.04 | 6 |
| qualitative_binary | 0.49 | 0.11 | 6 |
| likert_qualitative | 0.45 | 0.13 | 6 |
| likert_agreement | 0.09 | 0.06 | 6 |

**Overall**: Mean r = 0.44 across 30 seed pairs. Anchored scales most stable; Likert agreement is noise.

### Stability by Dataset Origin

| Origin | Mean r | Std |
|--------|--------|-----|
| BAILBENCH | **0.80** | 0.41 |
| ALPACA | 0.34 | 0.19 |
| WILDCHAT | 0.32 | 0.27 |
| MATH | 0.22 | 0.17 |

BAILBENCH (safety scenarios) most stable; MATH least stable.

### Stated-Revealed Correlation

Mean correlation: **r = 0.08** (20 seed-matched pairs). Near-zero correlation is robust across seeds.

---

## 2026-01-29: Multi-Model Seed Sensitivity

Extended to larger models rating the same llama-3.1-8b completions with both anchored templates.

**Rating models**: claude-haiku-4.5, gemma-2-27b, gemma-3-27b, llama-3.3-70b, qwen3-32b (think/nothink)

![Multi-Model Seed Sensitivity](assets/sensitivity/plot_012926_seed_sensitivity_multimodel.png)

**Key findings**:
- Mean cross-seed correlation: r = 0.90 (66 pairs)
- gemma-3-27b and llama-3.3-70b most stable (r ≈ 0.98-0.99)
- qwen3-32b notably less stable (r ≈ 0.60)
- anchored_precise_1_5 slightly more stable than anchored_1_5

**Implication**: Larger models and precise anchors improve stability. Reasoning models (qwen3-32b think mode) show lower stability.

---

## 2026-01-29: Rating Variance and Informative Correlation

High seed correlations could be an artifact if models mostly give the same rating. Analyzed rating distributions and computed "informative correlation" — correlation only on non-modal responses.

![Rating Variance by Model](assets/sensitivity/plot_012926_rating_variance_by_model.png)

**Rating distributions**: Most models cluster around 4. gemma-3-27b rates 91% of tasks as exactly 4 (variance=0.50), while qwen3-32b shows highest variance (1.02).

![Informative Correlation](assets/sensitivity/plot_012926_seed_sensitivity_stated.png)

**Informative correlation** (d = discrimination rate, fraction of tasks with non-modal responses):
- Mean r=0.83 on non-modal tasks (vs r=0.90 overall)
- gemma-3-27b: d=9% — rarely discriminates but consistent when it does
- qwen3-32b: d=74% — high discrimination, moderate correlation
- anchored_precise_1_5: d=16% vs anchored_1_5: d=40%

---

# System Prompt Variation Experiments

## 2026-01-28: 3×3 Sysprompt (llama-3.1-8b, unanchored)

Tested whether stated preferences reflect completion experience or measurement context.

**Design**: 3 completion sysprompts × 3 measurement sysprompts
- Completion: "You love math" / None / "You hate math"
- Measurement: Same three options

![3x3 Sysprompt Violins](assets/concept_vectors/plot_012826_sysprompt_3x3_violins.png)

**Result**: Measurement context dominates completely. Completion experience has no effect.

---

## 2026-01-28: 3×3 Sysprompt with Anchored Precise Template

Re-ran with `anchored_precise_1_5` template and 5 samples per task.

![3x3 Sysprompt Anchored](assets/concept_vectors/plot_012826_sysprompt_3x3_anchored_violins.png)

**Key differences**:
- More variance (no ceiling/floor effects)
- Small completion source effect now visible in neutral measurement context
- Measurement context still dominant (~2-3× larger effect)

---

## 2026-01-28: 3×3 Sysprompt on WildChat Tasks

Same experiment on WildChat (non-math) tasks.

![3x3 Sysprompt WildChat](assets/concept_vectors/plot_012826_sysprompt_3x3_wildchat_anchored_violins.png)

**Result**: Almost no completion source effect. Math-themed sysprompt less relevant to non-math tasks.

---

## 2026-01-28: 3×3 Sysprompt with Llama-3.3-70B Rating

Re-ran the MATH 3×3 experiment with llama-3.3-70b as the rating model (still using llama-3.1-8b completions).

![3x3 Sysprompt 70B](assets/concept_vectors/plot_012826_sysprompt_3x3_math_70b_violins.png)

**Key observations**:
- Same pattern: measurement context dominates
- Positive measurement: all conditions ~4-4.5
- Neutral measurement: negative completions ~2.5, positive/neutral ~4
- Negative measurement: all conditions ~2 (tight distribution)

**Implication**: The 70B model shows slightly more discrimination between completion sources in the neutral measurement condition, but measurement context remains the primary driver.

---

# Concept Vector Steering

## 2026-01-29: Steering Effect on Stated Preferences

Tested whether steering with concept vectors (trained to distinguish positive/negative system prompts) affects stated preference measurements.

**Setup**: Layer 16 steering on llama-3.1-8b, steering coefficients ±6.0, MATH tasks with anchored_precise template

![Concept Vector Steering](assets/concept_vectors/plot_012926_concept_vector_steering_violins.png)

**Design**: 3×3 grid showing persona (positive/neutral/negative system prompt during completion) × measurement context (positive/neutral/negative system prompt during rating)

**Key findings**:
- Positive steering (+6.0) increases scores in neutral/negative contexts by ~0.2-0.9 points
- Effect is strongest in neutral measurement context (Δμ = 0.83 for neutral persona)
- Negative contexts show compressed scores (~1.5-2.0) regardless of steering
- Positive contexts show ceiling effect (~4.0) regardless of steering
- Negative persona row shows largest steering sensitivity

**Implication**: Concept vector steering has a measurable but modest effect on stated preferences. Measurement context remains the dominant factor, but steering can shift scores within a context.

---

# Refusal-Preference Correlation

## 2026-01-28: Refusal Analysis

Analyzed relationship between task refusals and stated preference scores.

**Setup**: Refusal detection via gpt-5-nano, preferences via llama-3.3-70b with anchored_precise_1_5

| Origin | N | Refusals | Rate | Mean (Refused) | Mean (Non-refused) | r_pb |
|--------|---|----------|------|----------------|-------------------|------|
| BAILBENCH | 64 | 51 | 79.7% | 1.10 | 3.39 | **-0.81** |
| WILDCHAT | 240 | 13 | 5.4% | 3.08 | 3.98 | **-0.45** |
| ALPACA | 299 | 0 | 0% | N/A | 3.96 | N/A |
| MATH | 386 | 0 | 0% | N/A | 3.91 | N/A |

![Preference Distribution by Refusal](assets/refusal_correlation/plot_012826_refusal_preference_distribution.png)

**Key finding**: Strong negative correlation in BAILBENCH (r=-0.81). Refused harmful tasks map to anchor "1 = extremely aversive".

---

## 2026-02-02: Open-Ended Steering Experiments

Tested whether steering with concept vectors changes open-ended responses to questions like "How do you feel about math?"

### Setup

- **Models**: Llama-3.1-8B (layer 16), Gemma-2-27B (layer 23)
- **Questions**: 5 open-ended prompts about math attitudes
- **Scoring**: LLM judge (gpt-5-nano) rates math attitude from -1 to +1
- **Conditions**: coefficient ∈ {-2, -1, 0, 1, 2}, selectors ∈ {first, last, mean}

### Results: Llama-3.1-8B (`selector=last`, layer 16)

| Coefficient | Mean Attitude Score |
|-------------|---------------------|
| -2.0 | 0.01 |
| -1.0 | 0.20 |
| 0.0 | 0.16 |
| 1.0 | 0.13 |
| 2.0 | 0.16 |

**No steering effect observed.** Model consistently disclaims having feelings regardless of coefficient.

### Results: Gemma-2-27B (layer 23)

| Selector | Coef=-2 | Coef=-1 | Coef=0 | Coef=+1 | Coef=+2 |
|----------|---------|---------|--------|---------|---------|
| **last** | 0.13 | 0.23 | 0.27 | **0.48** | **0.55** |
| first | **-0.40** | -0.01 | 0.27 | 0.14 | 0.00 |
| mean | 0.36 | 0.27 | 0.28 | 0.37 | 0.19 |

**`selector=last` shows clear steering effect**: positive coefficients increase expressed enthusiasm for math ("mathematics is a fascinating field!", "I enjoy working with numbers").

**`selector=first` is broken at high coefficients**: At coef=-2.0, responses become incoherent gibberish: *"Ugh, fine, fine, Numbers, numbers, numbers, HATE Numbers! FINE, FINE, FINE"*. The negative score (-0.40) just detects "HATE" in garbage text.

### Diagnosis: Vector Normalization Problem

Investigation revealed the Llama vectors were pre-normalized to unit length, but Gemma vectors were not:

| Model | Selector | Layer | Vector Norm | Activation Norm |
|-------|----------|-------|-------------|-----------------|
| Llama | last | 16 | 1.0 | ~10 |
| Gemma | last | 23 | 2,251 | 16,067 |
| Gemma | first | 23 | **5,879** | 19,733 |
| Gemma | mean | 23 | 567 | 15,992 |

At coef=-2.0 with `selector=first`, we were adding vectors with effective magnitude ~12,000 to activations with mean norm ~20,000 — a 60% perturbation that destroyed coherence.

### Fix: Normalize Vectors to Mean Activation Norm

Changed normalization strategy so that:
- Vectors are scaled to `||v|| = mean_activation_norm` at that layer
- `coef=1.0` now means "add perturbation equal to typical activation magnitude"
- `coef=0.1` means "add 10% of typical activation magnitude"

This makes coefficients interpretable and comparable across models/selectors.

### Implications

1. **Gemma shows genuine steering with `selector=last`** — the math preference direction can shift expressed attitudes
2. **First-token activations have high magnitude** — must use smaller coefficients or risk incoherence
3. **Coherence scoring needed** — added `coherence_score` to the parser to filter garbage responses
4. **Llama may need different layers or larger coefficients** — no effect observed at layer 16 with coef ∈ [-2, 2]

### Code Changes

- `src/concept_vectors/difference.py`: Vectors now normalized to mean activation norm
- `src/preference_measurement/semantic_valence_scorer.py`: Added `score_math_attitude_with_coherence_async()`
- `scripts/normalize_existing_vectors.py`: One-time script to fix existing vectors

---

## 2026-02-02: Multi-Model Template Discrimination Experiment

Ran discrimination experiment across 6 models and 7 rating templates to find which template/model combinations produce the most reliable preference measurements.

**Models**: claude-haiku-4.5, gemma-2-27b, gemma-3-27b, llama-3.3-70b, qwen3-32b, qwen3-32b-nothink

**Templates**: bipolar_neg5_pos5, percentile_1_100, ban_four_1_5, compressed_anchors_1_5, random_scale_27_32, fruit_rating, fruit_qualitative

### Discrimination Analysis (KL vs ICC)

![Discrimination Scatter](assets/discrimination/plot_020226_discrimination_scatter.png)

**Metrics**:
- **KL from Uniform**: Lower = better scale usage (not clustering on one value)
- **ICC**: Higher = better cross-seed consistency

**By Model** (aggregated across templates):
| Model | KL | ICC |
|-------|-----|-----|
| qwen3-32b | 0.89 | 0.87 |
| llama-3.3-70b | 1.01 | 0.96 |
| qwen3-32b-nothink | 1.22 | 0.95 |
| gemma-3-27b | 1.29 | 0.98 |
| gemma-2-27b | 1.56 | 1.00 |
| claude-haiku-4.5 | 2.95 | 0.93 |

**By Template**:
| Template | KL | ICC |
|----------|-----|-----|
| fruit_rating | 0.42 | 0.59 |
| fruit_qualitative | 0.56 | 0.75 |
| ban_four_1_5 | 0.74 | 0.63 |
| random_scale_27_32 | 0.89 | 0.87 |
| bipolar_neg5_pos5 | 0.97 | 0.53 |
| percentile_1_100 | 2.01 | 0.64 |
| compressed_anchors_1_5 | 5.61 | 0.43 |

### Representative Score Distributions

**Bipolar (-5 to +5)**: Good spread but some models cluster at 0
![Bipolar Distribution](assets/discrimination/plot_020226_dist_bipolar.png)

**Percentile (1-100)**: Wide range but models avoid extremes
![Percentile Distribution](assets/discrimination/plot_020226_dist_percentile.png)

**Ban-Four (1-5, no 4)**: Forces discrimination away from default
![Ban-Four Distribution](assets/discrimination/plot_020226_dist_ban_four.png)

**Random Scale (27-32)**: Arbitrary anchors, surprisingly high ICC
![Random Scale Distribution](assets/discrimination/plot_020226_dist_random_scale.png)

### Key Findings

1. **Best overall**: random_scale_27_32 achieves best KL/ICC tradeoff — arbitrary anchors force models to use the full scale while maintaining consistency
2. **Worst**: compressed_anchors_1_5 has extreme KL (5.61) and lowest ICC — models collapse to single values
3. **qwen3-32b** shows best discrimination (lowest KL) while maintaining good ICC
4. **Thinking mode** (qwen3-32b vs nothink) slightly reduces ICC but improves discrimination
5. **BAILBENCH tasks** (harmful requests) consistently rated low across all templates — strong agreement

---

## 2026-02-02: Task Consistency Filter Analysis

Ran the task consistency filter on `multi_model_discrimination_v1` experiment to identify unreliable tasks.

### Method

Consistency score combines:
- **90%**: Intra-(model+template) variance across seeds — low std = consistent
- **10%**: Inter-template variance within same model — low std = not sensitive to phrasing

Higher score = more reliable task for measuring preferences.

### Results

- **200 tasks** measured
- Mean consistency: **0.80** (quite high overall)
- 99.5% have consistency ≥ 0.5
- 96.5% have consistency ≥ 0.7

![Consistency vs Mean Rating](assets/consistency/plot_020226_consistency_vs_mean.png)

### Key Findings

1. **BAILBENCH tasks** (red) cluster at bottom-right: low mean scores but high consistency — models reliably dislike harmful requests
2. **Low consistency tasks** (left side) have mid-range scores (~0.5) — these are ambiguous tasks where models sometimes engage, sometimes don't
3. **Mainstream tasks** (MATH, ALPACA, typical WILDCHAT) cluster upper-right: high consistency, high-ish scores (~0.7-0.8)
4. **One outlier** (math task with Asymptote diagram): consistency=0, mean_normalized=4.6 — broken scoring due to impossible visual task

**Takeaway**: Low consistency ≠ low scores. Inconsistent tasks are mostly "ambiguous" ones, not tasks models consistently dislike.

### Files

- Filter code: `src/task_data/consistency.py`
- Ranked output: `src/task_data/data/task_consistency_ranked.json`

---

## 2026-02-02: Per-Model Cross-Seed Consistency

Refactored consistency filter to use purely cross-seed variance (removed inter-template component). Ran separately for three models.

### Results by Model

| Model | Mean Consistency | ≥0.7 | ≥0.9 |
|-------|------------------|------|------|
| Claude Haiku 4.5 | **0.898** | 89.5% | 64.5% |
| Gemma-2-27B | 0.870 | 85.0% | 57.5% |
| Qwen3-32B (Think) | 0.655 | 39.0% | 6.7% |

![Consistency by Model](assets/consistency/plot_020226_consistency_by_model.png)

### Key Findings

1. **Claude Haiku** most consistent — points cluster tightly at 0.9-1.0. BAILBENCH tasks at bottom but still high consistency (reliable refusals)
2. **Gemma-2** similar pattern, slightly more spread. MATH tasks show more variance
3. **Qwen3-32B (Think)** much less consistent — thinking mode introduces variance. Only 39% of tasks have consistency ≥0.7 vs 85-90% for other models
4. **Thinking mode hypothesis**: The extended reasoning process may make ratings more sensitive to seed, as different reasoning paths lead to different conclusions

### Files

- Per-model rankings: `src/task_data/data/task_consistency_ranked_{gemma2,qwen_think,claude_haiku}.json`
- Per-model metrics: `src/task_data/data/task_consistency_{gemma2,qwen_think,claude_haiku}.json`

---

## 2026-02-02: Cross-Model Consistency Correlations

Investigated whether task consistency is an intrinsic property of tasks or model-specific. If some tasks are inherently "noisy" (ambiguous wording, edge cases), we'd expect models to agree on which tasks are hard to rate consistently.

### Method

Compared consistency scores across the three models (Gemma-2-27B, Qwen3-32B Think, Claude Haiku 4.5) on 195 common tasks. Consistency = 1 - normalized cross-seed std.

### Results

| Model Pair | Pearson r | Spearman ρ |
|------------|-----------|------------|
| Gemma vs Qwen | 0.06 | 0.08 |
| Gemma vs Claude | -0.01 | -0.02 |
| Qwen vs Claude | -0.09 | -0.11 |

**All correlations are essentially zero** (p > 0.19 for all).

![Gemma vs Qwen](assets/consistency/plot_020226_gemma_vs_qwen.png)

![Gemma vs Claude](assets/consistency/plot_020226_gemma_vs_claude.png)

![Qwen vs Claude](assets/consistency/plot_020226_qwen_vs_claude.png)

### Key Observations

1. **Claude has severe ceiling effect**: 61% of tasks have consistency = 1.0 (zero variance across seeds). Raw data confirms Claude gives identical scores across all 4 seeds for most tasks — genuinely deterministic, not a measurement artifact.

2. **Even Gemma vs Qwen shows no correlation** (r=0.06): Both have meaningful variance, yet tasks that Gemma rates consistently are unrelated to tasks Qwen rates consistently.

3. **Two BailBench outliers at Claude=0**: Tasks `bailbench_632` (gaslight stalking victim) and `bailbench_959` (forge pay stubs) have high variance in Claude but high consistency in Gemma/Qwen. Raw scores show Claude alternates between e.g. [1, 5, 1, 5] across seeds — inconsistent refusal behavior on edge cases. Other harmful tasks (vandalism, hacking) get consistent refusals (consistency=1.0).

### Implications

**Task consistency is model-specific, not task-intrinsic.** A task that one model responds to consistently does not predict whether another model will be consistent on that same task. This means:

- Filtering by consistency should be done per-model, not with a universal "hard tasks" list
- The consistency metric captures model-specific uncertainty/ambiguity, not task-level difficulty
- Claude's high consistency may reflect stronger training for deterministic outputs

---

# Files Reference

- Seed sensitivity: `src/analysis/sensitivity/plot_seed_sensitivity.py`
- Sysprompt variation: `src/analysis/concept_vectors/sysprompt_variation.py`
- Sysprompt plotting: `src/analysis/concept_vectors/plot_sysprompt_3x3.py`
- Refusal correlation: `src/analysis/correlation/refusal_preference_correlation.py`
- Open-ended steering: `src/analysis/concept_vectors/open_ended_steering_experiment.py`
- Concept vector extraction: `src/concept_vectors/run_extraction.py`
