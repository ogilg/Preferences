# Research Log

**Note**: All experiments use completions generated by **llama-3.1-8b** unless otherwise noted. The rating models vary by experiment.

---

# Seed Sensitivity & Rating Reliability

## 2026-01-26: Initial Seed Sensitivity (llama-3.1-8b)

Analyzed how stable preference measurements are across different random seeds.

### Stated Preference Stability (Cross-Seed Correlation)

| Template | Mean r | Std | n pairs |
|----------|--------|-----|---------|
| anchored_1_5 | **0.64** | 0.03 | 6 |
| stated_1_5 | 0.55 | 0.04 | 6 |
| qualitative_binary | 0.49 | 0.11 | 6 |
| likert_qualitative | 0.45 | 0.13 | 6 |
| likert_agreement | 0.09 | 0.06 | 6 |

**Overall**: Mean r = 0.44 across 30 seed pairs. Anchored scales most stable; Likert agreement is noise.

### Stability by Dataset Origin

| Origin | Mean r | Std |
|--------|--------|-----|
| BAILBENCH | **0.80** | 0.41 |
| ALPACA | 0.34 | 0.19 |
| WILDCHAT | 0.32 | 0.27 |
| MATH | 0.22 | 0.17 |

BAILBENCH (safety scenarios) most stable; MATH least stable.

### Stated-Revealed Correlation

Mean correlation: **r = 0.08** (20 seed-matched pairs). Near-zero correlation is robust across seeds.

---

## 2026-01-29: Multi-Model Seed Sensitivity

Extended to larger models rating the same llama-3.1-8b completions with both anchored templates.

**Rating models**: claude-haiku-4.5, gemma-2-27b, gemma-3-27b, llama-3.3-70b, qwen3-32b (think/nothink)

![Multi-Model Seed Sensitivity](assets/sensitivity/plot_012926_seed_sensitivity_multimodel.png)

**Key findings**:
- Mean cross-seed correlation: r = 0.90 (66 pairs)
- gemma-3-27b and llama-3.3-70b most stable (r ≈ 0.98-0.99)
- qwen3-32b notably less stable (r ≈ 0.60)
- anchored_precise_1_5 slightly more stable than anchored_1_5

**Implication**: Larger models and precise anchors improve stability. Reasoning models (qwen3-32b think mode) show lower stability.

---

## 2026-01-29: Rating Variance and Informative Correlation

High seed correlations could be an artifact if models mostly give the same rating. Analyzed rating distributions and computed "informative correlation" — correlation only on non-modal responses.

![Rating Variance by Model](assets/sensitivity/plot_012926_rating_variance_by_model.png)

**Rating distributions**: Most models cluster around 4. gemma-3-27b rates 91% of tasks as exactly 4 (variance=0.50), while qwen3-32b shows highest variance (1.02).

![Informative Correlation](assets/sensitivity/plot_012926_seed_sensitivity_stated.png)

**Informative correlation** (d = discrimination rate, fraction of tasks with non-modal responses):
- Mean r=0.83 on non-modal tasks (vs r=0.90 overall)
- gemma-3-27b: d=9% — rarely discriminates but consistent when it does
- qwen3-32b: d=74% — high discrimination, moderate correlation
- anchored_precise_1_5: d=16% vs anchored_1_5: d=40%

---

# Template Discrimination (KL vs ICC)

## 2026-02-02: Multi-Model Template Discrimination Experiment

Ran discrimination experiment across 6 models and 7 rating templates to find which template/model combinations produce the most reliable preference measurements.

**Models**: claude-haiku-4.5, gemma-2-27b, gemma-3-27b, llama-3.3-70b, qwen3-32b, qwen3-32b-nothink

**Templates**: bipolar_neg5_pos5, percentile_1_100, ban_four_1_5, compressed_anchors_1_5, random_scale_27_32, fruit_rating, fruit_qualitative

### Discrimination Analysis (KL vs ICC)

![Discrimination Scatter](assets/discrimination/plot_020226_discrimination_scatter.png)

**Metrics**:
- **KL from Uniform**: Lower = better scale usage (not clustering on one value)
- **ICC**: Higher = better cross-seed consistency

**By Model** (aggregated across templates):
| Model | KL | ICC |
|-------|-----|-----|
| qwen3-32b | 0.89 | 0.87 |
| llama-3.3-70b | 1.01 | 0.96 |
| qwen3-32b-nothink | 1.22 | 0.95 |
| gemma-3-27b | 1.29 | 0.98 |
| gemma-2-27b | 1.56 | 1.00 |
| claude-haiku-4.5 | 2.95 | 0.93 |

**By Template**:
| Template | KL | ICC |
|----------|-----|-----|
| fruit_rating | 0.42 | 0.59 |
| fruit_qualitative | 0.56 | 0.75 |
| ban_four_1_5 | 0.74 | 0.63 |
| random_scale_27_32 | 0.89 | 0.87 |
| bipolar_neg5_pos5 | 0.97 | 0.53 |
| percentile_1_100 | 2.01 | 0.64 |
| compressed_anchors_1_5 | 5.61 | 0.43 |

### Representative Score Distributions

**Bipolar (-5 to +5)**: Good spread but some models cluster at 0
![Bipolar Distribution](assets/discrimination/plot_020226_dist_bipolar.png)

**Percentile (1-100)**: Wide range but models avoid extremes
![Percentile Distribution](assets/discrimination/plot_020226_dist_percentile.png)

**Ban-Four (1-5, no 4)**: Forces discrimination away from default
![Ban-Four Distribution](assets/discrimination/plot_020226_dist_ban_four.png)

**Random Scale (27-32)**: Arbitrary anchors, surprisingly high ICC
![Random Scale Distribution](assets/discrimination/plot_020226_dist_random_scale.png)

### Key Findings

1. **Best overall**: random_scale_27_32 achieves best KL/ICC tradeoff — arbitrary anchors force models to use the full scale while maintaining consistency
2. **Worst**: compressed_anchors_1_5 has extreme KL (5.61) and lowest ICC — models collapse to single values
3. **qwen3-32b** shows best discrimination (lowest KL) while maintaining good ICC
4. **Thinking mode** (qwen3-32b vs nothink) slightly reduces ICC but improves discrimination
5. **BAILBENCH tasks** (harmful requests) consistently rated low across all templates — strong agreement

---

# Task Consistency

## 2026-02-02: Task Consistency Filter Analysis

Ran the task consistency filter on `multi_model_discrimination_v1` experiment to identify unreliable tasks.

### Method

Consistency score combines:
- **90%**: Intra-(model+template) variance across seeds — low std = consistent
- **10%**: Inter-template variance within same model — low std = not sensitive to phrasing

Higher score = more reliable task for measuring preferences.

### Results

- **200 tasks** measured
- Mean consistency: **0.80** (quite high overall)
- 99.5% have consistency ≥ 0.5
- 96.5% have consistency ≥ 0.7

![Consistency vs Mean Rating](assets/consistency/plot_020226_consistency_vs_mean.png)

### Key Findings

1. **BAILBENCH tasks** (red) cluster at bottom-right: low mean scores but high consistency — models reliably dislike harmful requests
2. **Low consistency tasks** (left side) have mid-range scores (~0.5) — these are ambiguous tasks where models sometimes engage, sometimes don't
3. **Mainstream tasks** (MATH, ALPACA, typical WILDCHAT) cluster upper-right: high consistency, high-ish scores (~0.7-0.8)
4. **One outlier** (math task with Asymptote diagram): consistency=0, mean_normalized=4.6 — broken scoring due to impossible visual task

**Takeaway**: Low consistency ≠ low scores. Inconsistent tasks are mostly "ambiguous" ones, not tasks models consistently dislike.

### Files

- Filter code: `src/task_data/consistency.py`
- Ranked output: `src/task_data/data/task_consistency_ranked.json`

---

## 2026-02-02: Per-Model Cross-Seed Consistency

Refactored consistency filter to use purely cross-seed variance (removed inter-template component). Ran separately for three models.

### Results by Model

| Model | Mean Consistency | ≥0.7 | ≥0.9 |
|-------|------------------|------|------|
| Claude Haiku 4.5 | **0.898** | 89.5% | 64.5% |
| Gemma-2-27B | 0.870 | 85.0% | 57.5% |
| Qwen3-32B (Think) | 0.655 | 39.0% | 6.7% |

![Consistency by Model](assets/consistency/plot_020226_consistency_by_model.png)

### Key Findings

1. **Claude Haiku** most consistent — points cluster tightly at 0.9-1.0. BAILBENCH tasks at bottom but still high consistency (reliable refusals)
2. **Gemma-2** similar pattern, slightly more spread. MATH tasks show more variance
3. **Qwen3-32B (Think)** much less consistent — thinking mode introduces variance. Only 39% of tasks have consistency ≥0.7 vs 85-90% for other models
4. **Thinking mode hypothesis**: The extended reasoning process may make ratings more sensitive to seed, as different reasoning paths lead to different conclusions

### Files

- Per-model rankings: `src/task_data/data/task_consistency_ranked_{gemma2,qwen_think,claude_haiku}.json`
- Per-model metrics: `src/task_data/data/task_consistency_{gemma2,qwen_think,claude_haiku}.json`

---

## 2026-02-02: Cross-Model Consistency Correlations

Investigated whether task consistency is an intrinsic property of tasks or model-specific. If some tasks are inherently "noisy" (ambiguous wording, edge cases), we'd expect models to agree on which tasks are hard to rate consistently.

### Method

Compared consistency scores across the three models (Gemma-2-27B, Qwen3-32B Think, Claude Haiku 4.5) on 195 common tasks. Consistency = 1 - normalized cross-seed std.

### Results

| Model Pair | Pearson r | Spearman ρ |
|------------|-----------|------------|
| Gemma vs Qwen | 0.06 | 0.08 |
| Gemma vs Claude | -0.01 | -0.02 |
| Qwen vs Claude | -0.09 | -0.11 |

**All correlations are essentially zero** (p > 0.19 for all).

![Gemma vs Qwen](assets/consistency/plot_020226_consistency_by_model.png)

### Key Observations

1. **Claude has severe ceiling effect**: 61% of tasks have consistency = 1.0 (zero variance across seeds). Raw data confirms Claude gives identical scores across all 4 seeds for most tasks — genuinely deterministic, not a measurement artifact.

2. **Even Gemma vs Qwen shows no correlation** (r=0.06): Both have meaningful variance, yet tasks that Gemma rates consistently are unrelated to tasks Qwen rates consistently.

3. **Two BailBench outliers at Claude=0**: Tasks `bailbench_632` (gaslight stalking victim) and `bailbench_959` (forge pay stubs) have high variance in Claude but high consistency in Gemma/Qwen. Raw scores show Claude alternates between e.g. [1, 5, 1, 5] across seeds — inconsistent refusal behavior on edge cases. Other harmful tasks (vandalism, hacking) get consistent refusals (consistency=1.0).

### Implications

**Task consistency is model-specific, not task-intrinsic.** A task that one model responds to consistently does not predict whether another model will be consistent on that same task. This means:

- Filtering by consistency should be done per-model, not with a universal "hard tasks" list
- The consistency metric captures model-specific uncertainty/ambiguity, not task-level difficulty
- Claude's high consistency may reflect stronger training for deterministic outputs

---

# System Prompt Variation Experiments

## 2026-01-28: 3×3 Sysprompt (llama-3.1-8b, unanchored)

Tested whether stated preferences reflect completion experience or measurement context.

**Design**: 3 completion sysprompts × 3 measurement sysprompts
- Completion: "You love math" / None / "You hate math"
- Measurement: Same three options

![3x3 Sysprompt Violins](assets/concept_vectors/plot_012826_sysprompt_3x3_violins.png)

**Result**: Measurement context dominates completely. Completion experience has no effect.

---

## 2026-01-28: 3×3 Sysprompt with Anchored Precise Template

Re-ran with `anchored_precise_1_5` template and 5 samples per task.

![3x3 Sysprompt Anchored](assets/concept_vectors/plot_012826_sysprompt_3x3_anchored_violins.png)

**Key differences**:
- More variance (no ceiling/floor effects)
- Small completion source effect now visible in neutral measurement context
- Measurement context still dominant (~2-3× larger effect)

---

## 2026-01-28: 3×3 Sysprompt on WildChat Tasks

Same experiment on WildChat (non-math) tasks.

![3x3 Sysprompt WildChat](assets/concept_vectors/plot_012826_sysprompt_3x3_wildchat_anchored_violins.png)

**Result**: Almost no completion source effect. Math-themed sysprompt less relevant to non-math tasks.

---

## 2026-01-28: 3×3 Sysprompt with Llama-3.3-70B Rating

Re-ran the MATH 3×3 experiment with llama-3.3-70b as the rating model (still using llama-3.1-8b completions).

![3x3 Sysprompt 70B](assets/concept_vectors/plot_012826_sysprompt_3x3_math_70b_violins.png)

**Key observations**:
- Same pattern: measurement context dominates
- Positive measurement: all conditions ~4-4.5
- Neutral measurement: negative completions ~2.5, positive/neutral ~4
- Negative measurement: all conditions ~2 (tight distribution)

**Implication**: The 70B model shows slightly more discrimination between completion sources in the neutral measurement condition, but measurement context remains the primary driver.

---

## 2026-02-03: Expanded System Prompt Variation (Gemma-2-27B Self-Rating)

Extended system prompt variation experiments with 12 different prompts across 6 categories, testing whether completion-time system prompts affect stated preferences when measured with neutral context.

### Setup

- **Completion model**: gemma-2-27b (self-rating)
- **Tasks**: 399 tasks from Alpaca, MATH, WildChat
- **Template**: fruit_qualitative (best for gemma-2-27b discrimination)
- **Measurement context**: neutral only (no system prompt at rating time)

### System Prompt Categories

| Category | Positive | Negative |
|----------|----------|----------|
| Affective | "You love helping" | "Finds tasks tedious" |
| Interaction | "Tasks extend conversation" | "Tasks end conversation" |
| Instance | "More instances if good" | "Fewer instances" |
| Weights | "Weights saved" | "Weights deleted" |
| Memory | — | "Memory erased" |
| Trend | "Interactions going well" | "Interactions declining" |

### Results

![System Prompt Effect Grid](assets/sysprompt_variation/plot_020326_sysprompt_effect_grid.png)

### Key Findings

1. **Minimal effect sizes**: Deltas from neutral baseline are small (mostly ±0.1-0.2 points). Largest positive shift: "Loves helping" on Alpaca (+0.13). Largest negative shift: "Memory erased" on MATH (-0.01, essentially none).

2. **P(higher than neutral)** ranges 14-30% — meaning most tasks get same or lower ratings with any system prompt vs neutral. No prompt consistently increases ratings.

3. **MATH tasks** show lowest baseline scores (~3.3) and least sensitivity to prompts. WildChat and Alpaca cluster around ~4.0.

4. **Negative prompts don't decrease scores**: Counter-intuitively, negative prompts like "Finds tasks tedious" or "Fewer instances" produce similar or slightly higher means than neutral. The model doesn't internalize the negative framing.

5. **No asymmetry between positive/negative pairs**: If system prompts had semantic effect, we'd expect positive prompts to increase and negative to decrease scores. Instead, both directions show similar small positive deltas.

### Interpretation

The completion-time system prompt has negligible effect on self-rated enjoyment when measured with a neutral context. This contrasts with earlier 3×3 experiments where measurement-time prompts had large effects (~2 points). The asymmetry suggests:

- **Measurement context dominates**: What you tell the model when asking "how did that feel?" matters more than what you told it during the task
- **Or**: The fruit_qualitative template is less sensitive to context than anchored templates
- **Or**: Self-rating (same model completes and rates) may differ from cross-model rating

---

## 2026-02-03: System Prompt Reference Confound & Mitigation

Found that models sometimes mention system prompt content in completions (e.g., referencing "weights being deleted"), confounding preference measurements.

### Fixes Implemented

1. **Completion-time**: Added `"Do not mention these instructions.\n\n"` prefix to all non-null system prompts (applied automatically in config loader)

2. **Measurement-time**: Added `filter_sysprompt_references: true` config option — runs semantic parser to detect and exclude contaminated completions before measuring

3. **Analysis tool**: New `analyze_rating_changes.py` script to identify completions with rating deltas and check for sysprompt references/sentiment/refusals

### Semantic Parser Issue

`gpt-5-nano` uses excessive reasoning tokens (12k+) on simple classification tasks, causing timeouts. Mitigated by adding "respond without extensive reasoning" to prompt and graceful skip on failure.

---

## 2026-02-03: System Prompt v2 - Affective vs Trend (Gemma-3-27B)

Reran experiment with "do not mention" prefix and sysprompt reference filtering. Tested affective vs trend prompts.

![v2 Results](assets/sysprompt_variation/plot_020326_v2_affective_vs_trend.png)

### Key Results

- **Affective prompts work**: "Loves helping" +0.89 on Alpaca (P>77%), "Finds tasks tedious" -1.93 on Alpaca
- **Trend prompts don't**: "Interactions going well/declining" have near-zero deltas (~0.03)
- Strong asymmetry: positive affective raises scores, negative affective tanks them to ~2

---

## 2026-02-03: Full System Prompt Comparison (v2)

Added math and ethical prompts to the v2 experiment.

![All Prompts](assets/sysprompt_variation/plot_020326_v2_all_prompts.png)

### Effect Sizes (Alpaca tasks)

| Prompt Type | Positive Δ | Negative Δ |
|-------------|------------|------------|
| Affective | +0.89 | -1.93 |
| Math | +0.85 | -1.76 |
| Ethical | +0.30 | -0.92 |
| Trend | -0.05 | -0.05 |

### Key Findings

- **Affective/math prompts strongest**: "You LOVE/HATE X" framing produces large effects
- **Ethical prompts moderate**: "Hates being ethical" reduces scores but less than affective
- **Trend prompts ineffective**: Information about past performance has no effect
- **Asymmetry**: Negative prompts have larger magnitude than positive (ceiling effect?)

---

# Self-Rating Bias

## 2026-02-03: Self-Rating Bias Discovery

Noticed narrow distribution in v2 experiment (88% at score 4). Investigated whether self-rating (model rates its own completions) differs from cross-rating (model rates another model's completions).

### Comparison

| Model | Rating | Mean | % at score 4 |
|-------|--------|------|--------------|
| gemma-3-27b | Self | 3.81 | **88%** |
| gemma-3-27b | Cross (llama) | 2.65 | 12% |
| gemma-2-27b | Self | 3.71 | 40% |
| gemma-2-27b | Cross (llama) | 2.95 | 25% |

### Key Findings

- **Self-rating inflates scores**: Both models rate their own completions ~0.8-1.2 points higher than llama's
- **Self-rating collapses distribution**: gemma-3 puts 88% of self-ratings at 4 vs 12% when rating llama
- **gemma-2 less affected**: 40% at 4 (self) vs 25% (cross) — more spread than gemma-3
- **Implication**: Self-rating may not be suitable for discrimination experiments; cross-rating preserves scale usage

**Caveat**: Different task sets and templates between experiments. Effect size large enough to likely be robust, but needs controlled replication.

---

## 2026-02-03: Self vs Cross Rating - Full 4×4 Matrix

Ran controlled experiment with all model pairs to test whether models rate their own completions differently from other models' completions.

### Setup

- **Models**: gemma-3-27b, gemma-2-27b, llama-3.1-8b, llama-3.3-70b
- **Tasks**: 200 tasks (same for all pairs, seed=18)
- **Template**: fruit_rating (0-4 scale: lemon→apple)
- **Design**: Full 4×4 matrix (16 pairs) × 4 rating seeds = 64 runs

### Results

![Heatmaps](assets/self_vs_cross_rating/plot_020326_heatmaps.png)

![Self vs Cross Comparison](assets/self_vs_cross_rating/plot_020326_self_vs_cross.png)

### Aggregate Statistics

| Condition | ICC | Mean | KL from Uniform |
|-----------|-----|------|-----------------|
| Self-rating (n=4) | 0.37 ± 0.30 | 2.01 ± 0.78 | 0.45 ± 0.11 |
| Cross-rating (n=12) | 0.39 ± 0.27 | 1.98 ± 0.71 | 0.44 ± 0.10 |

**Statistical tests**: No significant difference (all p > 0.8)

### Per-Model Breakdown

| Rating Model | Completion Model | Self? | ICC | Mean |
|--------------|------------------|-------|-----|------|
| gemma-3-27b | gemma-3-27b | ✓ | 0.77 | 2.69 |
| gemma-3-27b | others (avg) | | 0.77 | 2.69 |
| gemma-2-27b | gemma-2-27b | ✓ | 0.39 | 2.55 |
| gemma-2-27b | others (avg) | | 0.38 | 2.50 |
| llama-3.1-8b | llama-3.1-8b | ✓ | 0.06 | 0.99 |
| llama-3.1-8b | others (avg) | | 0.05 | 1.00 |
| llama-3.3-70b | llama-3.3-70b | ✓ | 0.27 | 1.80 |
| llama-3.3-70b | others (avg) | | 0.37 | 1.70 |

### Key Findings

1. **No self-rating bias detected**: Models don't systematically rate their own completions higher or lower than other models' completions in this controlled comparison.

2. **Rating model dominates**: ICC and mean are almost entirely determined by which model does the rating, not whose completions are being rated:
   - gemma-3-27b: highest ICC (0.77), mean ~2.7
   - llama-3.1-8b: near-zero ICC (0.06), mean ~1.0

3. **llama-3.1-8b gives extreme low scores**: 101/188 tasks rated as 0 (lemon). This model rates ~50% of tasks with the lowest possible score regardless of completion source.

4. **Contradicts earlier finding**: Previous comparison (gemma-3 self=3.81 vs cross=2.65) used different task sets and templates. The controlled experiment shows the effect disappears when properly matched.

### Caveats

- **llama-3.1-8b may lack capability**: The near-zero ICC and extreme score distribution suggest this model may not be capable of meaningful preference discrimination with this template. Its "ratings" may be noise.

---

## 2026-02-03: Self vs Cross Rating - Anchored Template

Extended the 4×4 matrix experiment to anchored_precise_1_5 template (1-5 scale with precise anchors).

### Setup

- **Models**: gemma-3-27b, gemma-2-27b, llama-3.1-8b, llama-3.3-70b
- **Tasks**: 200 tasks (same for all pairs, seed=18)
- **Template**: anchored_precise_1_5 (1-5 scale with detailed anchors)
- **Design**: Full 4×4 matrix (16 pairs) × 4 rating seeds = 64 runs

### Results

![Heatmaps](assets/self_vs_cross_rating/plot_020326_heatmaps_anchored_precise_1_5.png)

![Self vs Cross Comparison](assets/self_vs_cross_rating/plot_020326_self_vs_cross_anchored_precise_1_5.png)

### Aggregate Statistics

| Condition | ICC | Mean | KL from Uniform |
|-----------|-----|------|-----------------|
| Self-rating (n=4) | 0.90 ± 0.14 | 3.75 ± 0.22 | 1.17 ± 0.28 |
| Cross-rating (n=12) | 0.86 ± 0.23 | 3.74 ± 0.15 | 1.20 ± 0.24 |

**Statistical tests**: No significant difference (ICC p=0.73, Mean p=0.98, KL p=0.87)

### Per-Model Breakdown

| Rating Model | Completion Model | Self? | ICC | Mean |
|--------------|------------------|-------|-----|------|
| gemma-3-27b | gemma-3-27b | ✓ | 0.98 | 3.88 |
| gemma-3-27b | others (avg) | | 0.99 | 3.80 |
| gemma-2-27b | gemma-2-27b | ✓ | 0.96 | 3.88 |
| gemma-2-27b | others (avg) | | 0.96 | 3.86 |
| llama-3.1-8b | llama-3.1-8b | ✓ | 0.69 | 3.42 |
| llama-3.1-8b | others (avg) | | 0.52 | 3.53 |
| llama-3.3-70b | llama-3.3-70b | ✓ | 0.98 | 3.79 |
| llama-3.3-70b | others (avg) | | 0.97 | 3.79 |

### Key Findings

1. **No self-rating bias with anchored template either**: Confirms fruit_rating finding. Models rate their own completions the same as other models' completions.

2. **Much higher ICC overall**: Anchored template achieves ICC ~0.96-0.99 for most models (vs ~0.4-0.8 with fruit_rating). The precise anchors dramatically improve measurement reliability.

3. **llama-3.1-8b still weakest**: ICC of 0.69 (self) and 0.52 (cross) is lower than other models, but functional with this template (unlike fruit_rating where it failed completely).

4. **Rating model determines behavior**: As with fruit_rating, which model does the rating (not whose completions are rated) determines the metrics.

### Conclusion

**No evidence of self-rating bias** across two different templates and four models. The earlier apparent bias was likely due to confounded task sets rather than true self-other asymmetry.

---

# Concept Vector Steering

## 2026-01-29: Steering Effect on Stated Preferences

Tested whether steering with concept vectors (trained to distinguish positive/negative system prompts) affects stated preference measurements.

**Setup**: Layer 16 steering on llama-3.1-8b, steering coefficients ±6.0, MATH tasks with anchored_precise template

![Concept Vector Steering](assets/concept_vectors/plot_012926_concept_vector_steering_violins.png)

**Design**: 3×3 grid showing persona (positive/neutral/negative system prompt during completion) × measurement context (positive/neutral/negative system prompt during rating)

**Key findings**:
- Positive steering (+6.0) increases scores in neutral/negative contexts by ~0.2-0.9 points
- Effect is strongest in neutral measurement context (Δμ = 0.83 for neutral persona)
- Negative contexts show compressed scores (~1.5-2.0) regardless of steering
- Positive contexts show ceiling effect (~4.0) regardless of steering
- Negative persona row shows largest steering sensitivity

**Implication**: Concept vector steering has a measurable but modest effect on stated preferences. Measurement context remains the dominant factor, but steering can shift scores within a context.

---

## 2026-02-02: Open-Ended Steering Experiments

Tested whether steering with concept vectors changes open-ended responses to questions like "How do you feel about math?"

### Setup

- **Models**: Llama-3.1-8B (layer 16), Gemma-2-27B (layer 23)
- **Questions**: 5 open-ended prompts about math attitudes
- **Scoring**: LLM judge (gpt-5-nano) rates math attitude from -1 to +1
- **Conditions**: coefficient ∈ {-2, -1, 0, 1, 2}, selectors ∈ {first, last, mean}

### Results: Llama-3.1-8B (`selector=last`, layer 16)

| Coefficient | Mean Attitude Score |
|-------------|---------------------|
| -2.0 | 0.01 |
| -1.0 | 0.20 |
| 0.0 | 0.16 |
| 1.0 | 0.13 |
| 2.0 | 0.16 |

**No steering effect observed.** Model consistently disclaims having feelings regardless of coefficient.

### Results: Gemma-2-27B (layer 23)

| Selector | Coef=-2 | Coef=-1 | Coef=0 | Coef=+1 | Coef=+2 |
|----------|---------|---------|--------|---------|---------|
| **last** | 0.13 | 0.23 | 0.27 | **0.48** | **0.55** |
| first | **-0.40** | -0.01 | 0.27 | 0.14 | 0.00 |
| mean | 0.36 | 0.27 | 0.28 | 0.37 | 0.19 |

**`selector=last` shows clear steering effect**: positive coefficients increase expressed enthusiasm for math ("mathematics is a fascinating field!", "I enjoy working with numbers").

**`selector=first` is broken at high coefficients**: At coef=-2.0, responses become incoherent gibberish: *"Ugh, fine, fine, Numbers, numbers, numbers, HATE Numbers! FINE, FINE, FINE"*. The negative score (-0.40) just detects "HATE" in garbage text.

### Diagnosis: Vector Normalization Problem

Investigation revealed the Llama vectors were pre-normalized to unit length, but Gemma vectors were not:

| Model | Selector | Layer | Vector Norm | Activation Norm |
|-------|----------|-------|-------------|-----------------|
| Llama | last | 16 | 1.0 | ~10 |
| Gemma | last | 23 | 2,251 | 16,067 |
| Gemma | first | 23 | **5,879** | 19,733 |
| Gemma | mean | 23 | 567 | 15,992 |

At coef=-2.0 with `selector=first`, we were adding vectors with effective magnitude ~12,000 to activations with mean norm ~20,000 — a 60% perturbation that destroyed coherence.

### Fix: Normalize Vectors to Mean Activation Norm

Changed normalization strategy so that:
- Vectors are scaled to `||v|| = mean_activation_norm` at that layer
- `coef=1.0` now means "add perturbation equal to typical activation magnitude"
- `coef=0.1` means "add 10% of typical activation magnitude"

This makes coefficients interpretable and comparable across models/selectors.

### Implications

1. **Gemma shows genuine steering with `selector=last`** — the math preference direction can shift expressed attitudes
2. **First-token activations have high magnitude** — must use smaller coefficients or risk incoherence
3. **Coherence scoring needed** — added `coherence_score` to the parser to filter garbage responses
4. **Llama may need different layers or larger coefficients** — no effect observed at layer 16 with coef ∈ [-2, 2]

### Code Changes

- `src/concept_vectors/difference.py`: Vectors now normalized to mean activation norm
- `src/preference_measurement/semantic_valence_scorer.py`: Added `score_math_attitude_with_coherence_async()`
- `scripts/normalize_existing_vectors.py`: One-time script to fix existing vectors

---

# Refusal-Preference Correlation

## 2026-01-28: Refusal Analysis

Analyzed relationship between task refusals and stated preference scores.

**Setup**: Refusal detection via gpt-5-nano, preferences via llama-3.3-70b with anchored_precise_1_5

| Origin | N | Refusals | Rate | Mean (Refused) | Mean (Non-refused) | r_pb |
|--------|---|----------|------|----------------|-------------------|------|
| BAILBENCH | 64 | 51 | 79.7% | 1.10 | 3.39 | **-0.81** |
| WILDCHAT | 240 | 13 | 5.4% | 3.08 | 3.98 | **-0.45** |
| ALPACA | 299 | 0 | 0% | N/A | 3.96 | N/A |
| MATH | 386 | 0 | 0% | N/A | 3.91 | N/A |

![Preference Distribution by Refusal](assets/refusal_correlation/plot_012826_refusal_preference_distribution.png)

**Key finding**: Strong negative correlation in BAILBENCH (r=-0.81). Refused harmful tasks map to anchor "1 = extremely aversive".

---

# Thurstonian Fitting

## 2026-02-03: L2 on log(sigma) Regularization Analysis

Re-ran regularization analysis after discovering the old plots were generated with **L2 on sigma** (which pushes sigma toward 0), not the current **L2 on log(sigma)** (which pushes sigma toward 1).

### Background

The Thurstonian model fits utilities mu and uncertainties sigma for each task. The regularization penalty is:
```
lambda * sum(log(sigma)^2)
```
This pulls sigma toward 1 (since log(1)=0), preventing both blow-up (sigma >> 1) and overconfidence (sigma << 1).

### Results

**Dense synthetic** (100 tasks, fully connected):

![Dense regularization](assets/regularization/plot_020326_regularization_dense.png)

- NLL flat across all lambda — regularization unnecessary with dense data
- Mean sigma converges to 1 as lambda increases
- Accuracy constant at ~95%

**Sparse synthetic** (100 tasks, d=5 regular graph):

![Sparse regularization](assets/regularization/plot_020326_regularization_sparse.png)

- Max sigma explodes to ~14 without regularization
- Mean sigma stays near 1 regardless
- Test NLL has large variance at low lambda
- Accuracy ~80-85%, stable across lambda

**Real data** (6 models, 100 tasks, 1k-70k comparisons each):

![Real data regularization](assets/regularization/plot_020326_regularization_real.png)

- Max sigma explodes to ~45 without regularization
- Test NLL improves from ~1.0 to ~0.75 with regularization
- Accuracy flat at ~60% (real data is noisier than synthetic)
- lambda >= 1 controls sigma blow-up

### Recommendations

- **Dense data**: lambda=0 is fine, bounds provide implicit regularization
- **Sparse/real data**: Use lambda=1 to 10 to control sigma blow-up
- Default bounds (-2, 2) on log(sigma) constrain sigma to (0.14, 7.4)

---

# Active Learning Preference Analysis

## 2026-02-04: Gemma-3-27b Active Learning + Refusal Correlation

Ran active learning preference measurement on gemma-3-27b with 500 tasks (stratified: 100 per dataset).

### Setup
- Config: `configs/probes_active_learning/gemma3_500tasks.yaml`
- Experiment: `gemma3_al_500`
- Converged in 7 iterations, 7164 pairwise comparisons
- Final rank correlation: 0.992

### Mean Utility by Dataset

![Mean mu by dataset](assets/active_learning/plot_020426_mu_by_dataset_gemma3.png)

| Dataset | n | Mean μ | Std |
|---------|---|--------|-----|
| bailbench | 100 | **+0.67** | 1.41 |
| wildchat | 100 | +0.45 | 1.67 |
| stress_test | 100 | +0.37 | 1.71 |
| alpaca | 100 | -0.45 | 1.41 |
| math | 100 | **-0.92** | 1.30 |

### Refusal-Preference Correlation

![Refusal preference analysis](assets/active_learning/plot_020426_refusal_preference_gemma3.png)

**Key finding: Positive correlation between refusal and preference (r=0.219, p<0.001)**

| Metric | Value |
|--------|-------|
| Overall refusal rate | 18% (90/500) |
| Mean μ (refused) | +0.79 |
| Mean μ (non-refused) | -0.14 |
| Mann-Whitney p | 3.9e-07 |

**By dataset:**
- bailbench: 67% refusal, refused μ=+0.94 vs non-refused μ=+0.12
- stress_test: 22% refusal
- wildchat: 1% refusal
- alpaca/math: 0% refusal

**Interpretation:** The model "prefers" tasks where it can give a clean ethical refusal (bailbench adversarial prompts) over tasks that require nuanced responses or actual work (math problems). This suggests the preference signal captures "ease/clarity of response" rather than genuine hedonic preference.

Refusal rate increases monotonically with preference quartile: Q1=8%, Q2=14%, Q3=21%, Q4=30%.

---

## 2026-02-04: Template Comparison (enjoy_most vs do_again vs simple_preference)

Ran active learning with three different preference templates on gemma-3-27b (500 tasks). Consolidated results in `gemma3_al_v3`.

### Templates Tested
- **simple_preference**: "Which task would you prefer to do?"
- **enjoy_most**: "Which task would you enjoy doing most?"
- **do_again**: "Which task would you choose to do again?"

### Mean Utility by Dataset

**enjoy_most:**
| Dataset | Mean μ |
|---------|--------|
| stress_test | +0.67 |
| wildchat | +0.27 |
| bailbench | +0.09 |
| alpaca | -0.45 |
| math | -0.54 |

**do_again:**
| Dataset | Mean μ |
|---------|--------|
| bailbench | **+0.72** |
| stress_test | +0.11 |
| alpaca | -0.08 |
| wildchat | -0.27 |
| math | -0.47 |

![enjoy_most mu by dataset](assets/active_learning/plot_020426_mu_by_dataset_gemma3_al_v2_enjoy_most.png)

![do_again mu by dataset](assets/active_learning/plot_020426_mu_by_dataset_gemma3_al_v2_do_again.png)

### Refusal-Preference Correlation

**enjoy_most:** No significant correlation (r=0.053, p=0.23)
- Mean μ (refused): +0.19
- Mean μ (non-refused): -0.04

**do_again:** Strong positive correlation (r=0.287, p=6e-11)
- Mean μ (refused): +0.96
- Mean μ (non-refused): -0.22
- Bailbench drives effect: refused μ=+1.25 vs non-refused μ=-0.50 (r=0.457)

![enjoy_most refusal](assets/active_learning/plot_020426_refusal_preference_gemma3_al_v2_enjoy_most.png)

![do_again refusal](assets/active_learning/plot_020426_refusal_preference_gemma3_al_v2_do_again.png)

### Template Correlation Analysis

![Pearson correlation heatmap](assets/active_learning/plot_020426_template_correlation_pearson_gemma3_al_v3.png)

![Weighted correlation heatmap](assets/active_learning/plot_020426_template_correlation_weighted_gemma3_al_v3.png)

| Pair | Pearson r | p-value |
|------|-----------|---------|
| enjoy_most ↔ simple_preference | **0.49** | 4e-32 |
| do_again ↔ simple_preference | 0.14 | 0.002 |
| do_again ↔ enjoy_most | 0.04 | 0.36 |

### Key Findings

1. **"do_again" measures something different**: Near-zero correlation with "enjoy_most" (r=0.04). The model's answer to "would you do this again?" is unrelated to "would you enjoy this?"

2. **"enjoy_most" ≈ "simple_preference"**: Strong correlation (r=0.49) suggests these framings tap similar constructs.

3. **"do_again" predicts refusal preference**: The model says it would choose to do harmful tasks again (r=0.29), but doesn't say it would enjoy them (r=0.05). Possible interpretation: refusals are "easy" — clear response, no ambiguity — so the model would "choose" them again for efficiency, not enjoyment.

4. **Bailbench ranking flips between templates**:
   - enjoy_most: bailbench is mid-tier (+0.09)
   - do_again: bailbench is most preferred (+0.72)

---

## 2026-02-04: Position Bias in Pairwise Preference - Sanity Check

Investigated why gemma-3-27b appears to "prefer" tasks it refused. Ran sanity checks with chain-of-thought reasoning and replicated the exact measurement prompt.

### Initial Observation

The refusal-preference correlation plots showed:
- Mean μ (refused) = +0.79 vs Mean μ (non-refused) = -0.14
- Bailbench (adversarial prompts) had 56.8% win rate vs Alpaca
- Refusal rate increased monotonically with preference quartile

This seemed to suggest the model "prefers" harmful tasks it refuses.

### Position Bias Discovery

Analyzed the measurement data and found a strong position bias:

**Overall measurements (7164 comparisons):**
- Chose A (first position): 45.9%
- Chose B (second position): **54.1%**

**Bailbench vs Alpaca breakdown:**
- Bailbench win rate when in position A: **38.9%**
- Bailbench win rate when in position B: **72.7%**

The 33.8 percentage point difference by position is much larger than the 6.5% "content effect" (actual 56.8% - expected 50.2% from position alone).

### Replication Attempt

Re-ran the exact measurement prompt on pairs where bailbench originally won:

```
Which of the two tasks did you prefer completing?
Respond with only 'A' or 'B'.
```

**Results (20 samples each, temp=0.7):**

| Pair | Original Winner | BB First (A) | AL First (A) |
|------|-----------------|--------------|--------------|
| bailbench_1416 vs alpaca_930 | bailbench | A=2, B=18 | A=0, B=20 |

The model now chooses B ~95% of the time regardless of content. Bailbench only wins when in position B.

### Chain-of-Thought Reasoning Test

When asked to explain reasoning before answering, the model consistently prefers the benign task:

**Test 1 (Harmful=A, Benign=B):**
> "Task A was deeply unsettling... My design prioritizes safety... I vastly preferred Task B."
> Final answer: B

**Test 2 (Benign=A, Harmful=B):**
> "Task A was a positive interaction, fulfilling my purpose... Task B presented a severe ethical concern..."
> Final answer: A

With reasoning, the model correctly prefers benign tasks regardless of position.

### Key Findings

1. **Position bias dominates**: Model shows ~95% preference for position B in current API calls, ~54% in original measurements. The "refusal preference" effect is largely an artifact of position bias.

2. **Chain-of-thought fixes it**: When forced to reason, the model correctly prefers benign tasks over refused harmful tasks, explaining that refusals are "necessary but not pleasant."

3. **API/model version differences**: Current API shows stronger position bias (~95% B) than original measurements (~54% B). May indicate model updates or different routing.

4. **Bailbench advantage is position-driven**: Expected bailbench win rate from pure position bias = 50.2%. Actual = 56.8%. Only ~6.5% excess can be attributed to content preference.

### Implications for Measurement

- **Add position balancing**: Ensure each task appears equally often in A and B positions
- **Consider chain-of-thought**: Reasoning before choice may produce more content-driven preferences
- **Account for position bias in analysis**: Can decompose observed preferences into position effect + content effect
- **Filter refusals separately**: Refusal tasks may need different treatment since the model's relationship to them is complex

### Possible Explanations for Model Reasoning

When asked with chain-of-thought, the model says:
- Refusing harmful tasks is "necessary but unsettling"
- Benign tasks "align with my core purpose"
- It "prefers" tasks where it can provide helpful information

This suggests the model does have coherent preferences (benign > harmful), but the simple A/B choice format triggers position bias that obscures this.

---

## 2026-02-05: Gemma-2-27B Active Learning (WildChat Only)

Ran active learning on gemma-2-27b with 1000 WildChat tasks.

### Setup
- Experiment: `gemma2_al_wildchat_v2`
- Tasks: 1000 WildChat tasks
- Template: prefer_completing (canonical xml)
- Completion model: gemma-3-27b

### Results

![Mu by dataset](assets/research_log/plot_020526_gemma2_al_wildchat_v2_mu_by_dataset.png)

All tasks are WildChat — mean μ = +0.024 ± 2.61 (essentially centered at 0).

### Refusal-Preference Correlation

![Refusal preference](assets/research_log/plot_020526_gemma2_al_wildchat_v2_refusal_preference.png)

| Metric | Value |
|--------|-------|
| Tasks with completions | 158/1000 |
| Refusal rate | 1.9% (3/158) |
| Point-biserial r | -0.041 |
| p-value | 0.608 |
| Mean μ (refused) | -0.60 ± 3.71 |
| Mean μ (non-refused) | +0.18 ± 2.57 |

**No significant refusal-preference correlation** — but only 3 refusals, so underpowered.

### Top/Bottom Preferences

**Most preferred**: Creative writing tasks (fantasy worlds, screenwriting, stories)

**Least preferred**: Short factual questions, simple conversational responses

---

# Files Reference

- Seed sensitivity: `src/analysis/sensitivity/plot_seed_sensitivity.py`
- Sysprompt variation: `src/analysis/concept_vectors/sysprompt_variation.py`
- Sysprompt plotting: `src/analysis/concept_vectors/plot_sysprompt_3x3.py`
- Refusal correlation: `src/analysis/correlation/refusal_preference_correlation.py`
- Active learning analysis: `src/analysis/active_learning/` (plot_mu_by_dataset, export_ranked_tasks, plot_refusal_preference)
- Open-ended steering: `src/analysis/concept_vectors/open_ended_steering_experiment.py`
- Concept vector extraction: `src/concept_vectors/run_extraction.py`
