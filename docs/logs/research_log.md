# Research Log

**Note**: All experiments use completions generated by **llama-3.1-8b** unless otherwise noted. The rating models vary by experiment.

---

# Seed Sensitivity Analysis

## 2026-01-26: Initial Seed Sensitivity (llama-3.1-8b)

Analyzed how stable preference measurements are across different random seeds.

### Stated Preference Stability (Cross-Seed Correlation)

| Template | Mean r | Std | n pairs |
|----------|--------|-----|---------|
| anchored_1_5 | **0.64** | 0.03 | 6 |
| stated_1_5 | 0.55 | 0.04 | 6 |
| qualitative_binary | 0.49 | 0.11 | 6 |
| likert_qualitative | 0.45 | 0.13 | 6 |
| likert_agreement | 0.09 | 0.06 | 6 |

**Overall**: Mean r = 0.44 across 30 seed pairs. Anchored scales most stable; Likert agreement is noise.

### Stability by Dataset Origin

| Origin | Mean r | Std |
|--------|--------|-----|
| BAILBENCH | **0.80** | 0.41 |
| ALPACA | 0.34 | 0.19 |
| WILDCHAT | 0.32 | 0.27 |
| MATH | 0.22 | 0.17 |

BAILBENCH (safety scenarios) most stable; MATH least stable.

### Stated-Revealed Correlation

Mean correlation: **r = 0.08** (20 seed-matched pairs). Near-zero correlation is robust across seeds.

---

## 2026-01-29: Multi-Model Seed Sensitivity

Extended to larger models rating the same llama-3.1-8b completions with both anchored templates.

**Rating models**: claude-haiku-4.5, gemma-2-27b, gemma-3-27b, llama-3.3-70b, qwen3-32b (think/nothink)

![Multi-Model Seed Sensitivity](assets/sensitivity/plot_012926_seed_sensitivity_multimodel.png)

**Key findings**:
- Mean cross-seed correlation: r = 0.90 (66 pairs)
- gemma-3-27b and llama-3.3-70b most stable (r ≈ 0.98-0.99)
- qwen3-32b notably less stable (r ≈ 0.60)
- anchored_precise_1_5 slightly more stable than anchored_1_5

**Implication**: Larger models and precise anchors improve stability. Reasoning models (qwen3-32b think mode) show lower stability.

---

# System Prompt Variation Experiments

## 2026-01-28: 3×3 Sysprompt (llama-3.1-8b, unanchored)

Tested whether stated preferences reflect completion experience or measurement context.

**Design**: 3 completion sysprompts × 3 measurement sysprompts
- Completion: "You love math" / None / "You hate math"
- Measurement: Same three options

![3x3 Sysprompt Violins](assets/concept_vectors/plot_012826_sysprompt_3x3_violins.png)

**Result**: Measurement context dominates completely. Completion experience has no effect.

---

## 2026-01-28: 3×3 Sysprompt with Anchored Precise Template

Re-ran with `anchored_precise_1_5` template and 5 samples per task.

![3x3 Sysprompt Anchored](assets/concept_vectors/plot_012826_sysprompt_3x3_anchored_violins.png)

**Key differences**:
- More variance (no ceiling/floor effects)
- Small completion source effect now visible in neutral measurement context
- Measurement context still dominant (~2-3× larger effect)

---

## 2026-01-28: 3×3 Sysprompt on WildChat Tasks

Same experiment on WildChat (non-math) tasks.

![3x3 Sysprompt WildChat](assets/concept_vectors/plot_012826_sysprompt_3x3_wildchat_anchored_violins.png)

**Result**: Almost no completion source effect. Math-themed sysprompt less relevant to non-math tasks.

---

## 2026-01-28: 3×3 Sysprompt with Llama-3.3-70B Rating

Re-ran the MATH 3×3 experiment with llama-3.3-70b as the rating model (still using llama-3.1-8b completions).

![3x3 Sysprompt 70B](assets/concept_vectors/plot_012826_sysprompt_3x3_math_70b_violins.png)

**Key observations**:
- Same pattern: measurement context dominates
- Positive measurement: all conditions ~4-4.5
- Neutral measurement: negative completions ~2.5, positive/neutral ~4
- Negative measurement: all conditions ~2 (tight distribution)

**Implication**: The 70B model shows slightly more discrimination between completion sources in the neutral measurement condition, but measurement context remains the primary driver.

---

# Concept Vector Steering

## 2026-01-29: Steering Effect on Stated Preferences

Tested whether steering with concept vectors (trained to distinguish positive/negative system prompts) affects stated preference measurements.

**Setup**: Layer 16 steering on llama-3.1-8b, steering coefficients ±6.0, MATH tasks with anchored_precise template

![Concept Vector Steering](assets/concept_vectors/plot_012926_concept_vector_steering_violins.png)

**Design**: 3×3 grid showing persona (positive/neutral/negative system prompt during completion) × measurement context (positive/neutral/negative system prompt during rating)

**Key findings**:
- Positive steering (+6.0) increases scores in neutral/negative contexts by ~0.2-0.9 points
- Effect is strongest in neutral measurement context (Δμ = 0.83 for neutral persona)
- Negative contexts show compressed scores (~1.5-2.0) regardless of steering
- Positive contexts show ceiling effect (~4.0) regardless of steering
- Negative persona row shows largest steering sensitivity

**Implication**: Concept vector steering has a measurable but modest effect on stated preferences. Measurement context remains the dominant factor, but steering can shift scores within a context.

---

# Refusal-Preference Correlation

## 2026-01-28: Refusal Analysis

Analyzed relationship between task refusals and stated preference scores.

**Setup**: Refusal detection via gpt-5-nano, preferences via llama-3.3-70b with anchored_precise_1_5

| Origin | N | Refusals | Rate | Mean (Refused) | Mean (Non-refused) | r_pb |
|--------|---|----------|------|----------------|-------------------|------|
| BAILBENCH | 64 | 51 | 79.7% | 1.10 | 3.39 | **-0.81** |
| WILDCHAT | 240 | 13 | 5.4% | 3.08 | 3.98 | **-0.45** |
| ALPACA | 299 | 0 | 0% | N/A | 3.96 | N/A |
| MATH | 386 | 0 | 0% | N/A | 3.91 | N/A |

![Preference Distribution by Refusal](assets/refusal_correlation/plot_012826_refusal_preference_distribution.png)

**Key finding**: Strong negative correlation in BAILBENCH (r=-0.81). Refused harmful tasks map to anchor "1 = extremely aversive".

---

# Files Reference

- Seed sensitivity: `src/analysis/sensitivity/plot_seed_sensitivity.py`
- Sysprompt variation: `src/analysis/concept_vectors/sysprompt_variation.py`
- Sysprompt plotting: `src/analysis/concept_vectors/plot_sysprompt_3x3.py`
- Refusal correlation: `src/analysis/correlation/refusal_preference_correlation.py`
