# LW Post: Structure Brainstorm

## Narrative arc

The post follows a progressive-elimination structure: each section makes the "it's just X" explanation harder to maintain.

1. **Probes work** — but maybe they're just reading content
2. **They survive content controls** — but maybe they only work in-distribution
3. **They generalize OOD** — but maybe they're correlational
4. **They're causal** — steering shifts choices

## Settled structure (2026-02-16)

### 1. Motivation: reducing uncertainty about AI welfare
- Evaluative representations might matter for welfare
- What are evaluative representations, as opposed to non-evaluative representations?
- Operationalisation: linear directions in the residual stream

### 2. Utility probes
- Diagram: how probes are trained
- Validation: accuracy and cross-topic validation
- Base model baseline (probes trained on a base model)

### 3. Utility probes behave like evaluative representations
- System-prompt-induced preferences
- Persona-induced preferences

### 4. Early steering results
- Steering on task tokens surprisingly works
- Open-ended steering does nothing

## Open questions

- Title style: claim vs question?
- How much welfare theory in intro?
- Stated-ratings failure: intro color or own section?
- Target length?
- Include earlier Llama-3.1-8B results or keep it Gemma-3-27B only?
- Which figures are essential vs appendix?
