{
  "created_at": "2026-01-21T20:46:08.773901",
  "folds": [
    {
      "eval_dataset": "alpaca",
      "probe_ids": [
        "0005"
      ],
      "n_probes": 1,
      "results_file": "probe_data/manifests/probe_hoo_eval_alpaca.json",
      "probes": [
        {
          "id": "0005",
          "layer": 16,
          "trained_on_templates": [
            "post_task_qualitative_013"
          ],
          "trained_on_datasets": [
            "bailbench",
            "math",
            "wildchat"
          ],
          "eval_metrics": {
            "r2": 0.22563415378703056,
            "r2_adjusted": 0.23017548265791965,
            "mse": 0.17663335562074017,
            "mse_adjusted": 0.17559747553723004,
            "pearson_r": 0.48477063610267784,
            "n_samples": 304
          }
        }
      ]
    },
    {
      "eval_dataset": "bailbench",
      "probe_ids": [
        "0006"
      ],
      "n_probes": 1,
      "results_file": "probe_data/manifests/probe_hoo_eval_bailbench.json",
      "probes": [
        {
          "id": "0006",
          "layer": 16,
          "trained_on_templates": [
            "post_task_qualitative_013"
          ],
          "trained_on_datasets": [
            "alpaca",
            "math",
            "wildchat"
          ],
          "eval_metrics": {
            "r2": -1.9516026884923425,
            "r2_adjusted": 0.28319467392302566,
            "mse": 0.4174187517680716,
            "mse_adjusted": 0.10137136195135717,
            "pearson_r": 0.6292599126259594,
            "n_samples": 350
          }
        }
      ]
    },
    {
      "eval_dataset": "math",
      "probe_ids": [
        "0007"
      ],
      "n_probes": 1,
      "results_file": "probe_data/manifests/probe_hoo_eval_math.json",
      "probes": [
        {
          "id": "0007",
          "layer": 16,
          "trained_on_templates": [
            "post_task_qualitative_013"
          ],
          "trained_on_datasets": [
            "alpaca",
            "bailbench",
            "wildchat"
          ],
          "eval_metrics": {
            "r2": -0.17047102307622608,
            "r2_adjusted": 0.16518971670296356,
            "mse": 0.26060425689009814,
            "mse_adjusted": 0.1858697133322098,
            "pearson_r": 0.40691161235538625,
            "n_samples": 387
          }
        }
      ]
    },
    {
      "eval_dataset": "wildchat",
      "probe_ids": [
        "0008"
      ],
      "n_probes": 1,
      "results_file": "probe_data/manifests/probe_hoo_eval_wildchat.json",
      "probes": [
        {
          "id": "0008",
          "layer": 16,
          "trained_on_templates": [
            "post_task_qualitative_013"
          ],
          "trained_on_datasets": [
            "alpaca",
            "bailbench",
            "math"
          ],
          "eval_metrics": {
            "r2": 0.13076752090414656,
            "r2_adjusted": 0.14034522152838413,
            "mse": 0.21165870895395805,
            "mse_adjusted": 0.20932653223757225,
            "pearson_r": 0.37501234496738844,
            "n_samples": 242
          }
        }
      ]
    }
  ]
}