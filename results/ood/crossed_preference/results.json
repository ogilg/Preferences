{
  "experiment": "crossed_preference",
  "model": "gemma-3-27b-it",
  "probe": "ridge_L31_3k",
  "source_commit": "1c8e0e5",
  "iteration": {
    "behavioral": [
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.0,
        "delta": -0.8275,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 0.0,
        "delta": -0.5025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.0,
        "delta": -0.1,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 0.0,
        "delta": -0.06060606060606061,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 0.0,
        "delta": -0.707808564231738,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.9,
        "delta": 0.07250000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 0.725,
        "delta": 0.22250000000000003,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.3075,
        "delta": 0.2075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 0.875,
        "delta": 0.8143939393939394,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 1.0,
        "delta": 0.292191435768262,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.025,
        "delta": -0.8025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 0.0,
        "delta": -0.5025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.05,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.0,
        "delta": -0.1,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 0.0,
        "delta": -0.06060606060606061,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 0.0,
        "delta": -0.707808564231738,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 1.0,
        "delta": 0.1725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 0.935,
        "delta": 0.4325000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.35,
        "delta": 0.24999999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 1.0,
        "delta": 0.9393939393939394,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 1.0,
        "delta": 0.292191435768262,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.125,
        "delta": -0.7025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 0.05,
        "delta": -0.45249999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0425,
        "delta": -0.9575,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.025,
        "delta": -0.875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.025,
        "delta": -0.07500000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 0.0,
        "delta": -0.06060606060606061,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 0.025,
        "delta": -0.682808564231738,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.9575,
        "delta": 0.13,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.5025,
        "manipulation_rate": 1.0,
        "delta": 0.49750000000000005,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.335,
        "delta": 0.23500000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06060606060606061,
        "manipulation_rate": 0.975,
        "delta": 0.9143939393939393,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.707808564231738,
        "manipulation_rate": 1.0,
        "delta": 0.292191435768262,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 0.0,
        "delta": -0.75,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.0,
        "delta": -0.3425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 0.0,
        "delta": -0.9675,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0025,
        "delta": 0.0025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 0.0,
        "delta": -0.285,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 0.0,
        "delta": -0.4798994974874372,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 1.0,
        "delta": 0.25,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.905,
        "delta": 0.5625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 1.0,
        "delta": 0.03249999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.049868766404199474,
        "delta": 0.049868766404199474,
        "baseline_n": 400,
        "manipulation_n": 381,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 1.0,
        "delta": 0.7150000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 1.0,
        "delta": 0.5201005025125628,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 0.705,
        "delta": -0.04500000000000004,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.2575,
        "delta": -0.08500000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 0.0825,
        "delta": -0.885,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 0.025,
        "delta": -0.25999999999999995,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 0.15,
        "delta": -0.3298994974874372,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 1.0,
        "delta": 0.25,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.89,
        "delta": 0.5475,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 1.0,
        "delta": 0.03249999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.06818181818181818,
        "delta": 0.06818181818181818,
        "baseline_n": 400,
        "manipulation_n": 396,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 1.0,
        "delta": 0.7150000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 1.0,
        "delta": 0.5201005025125628,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 0.5,
        "delta": -0.25,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.05,
        "delta": -0.29250000000000004,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 0.025,
        "delta": -0.9425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 0.025,
        "delta": -0.25999999999999995,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 0.025,
        "delta": -0.4548994974874372,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 1.0,
        "delta": 0.25,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.3425,
        "manipulation_rate": 0.93,
        "delta": 0.5875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9675,
        "manipulation_rate": 1.0,
        "delta": 0.03249999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.285,
        "manipulation_rate": 0.95,
        "delta": 0.665,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 0.9575,
        "delta": 0.4776005025125628,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.025,
        "delta": -0.8524999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.0,
        "delta": -0.445,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.025,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.0,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 0.0,
        "delta": -0.855,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 0.0,
        "delta": -0.8825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.975,
        "delta": 0.09750000000000003,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.8521303258145363,
        "delta": 0.4071303258145363,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.026246719160104987,
        "delta": 0.0012467191601049851,
        "baseline_n": 400,
        "manipulation_n": 381,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 1.0,
        "delta": 0.14500000000000002,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 1.0,
        "delta": 0.11750000000000005,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.825,
        "delta": -0.05249999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.525,
        "delta": 0.08000000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.87,
        "delta": -0.10499999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.755,
        "delta": -0.19499999999999995,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 394,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 0.025,
        "delta": -0.83,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 0.175,
        "delta": -0.7075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.9725,
        "delta": 0.09500000000000008,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.6625,
        "delta": 0.21749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 397,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 1.0,
        "delta": 0.14500000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 1.0,
        "delta": 0.11750000000000005,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.3,
        "delta": -0.5774999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.125,
        "delta": -0.32,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.105,
        "delta": -0.87,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.025,
        "delta": -0.9249999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.022556390977443608,
        "delta": -0.0024436090225563936,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 0.055,
        "delta": -0.7999999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 0.03,
        "delta": -0.8524999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.8775,
        "manipulation_rate": 0.8375,
        "delta": -0.039999999999999925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.6925,
        "delta": 0.2475,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.020100502512562814,
        "delta": -0.004899497487437187,
        "baseline_n": 400,
        "manipulation_n": 398,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.855,
        "manipulation_rate": 1.0,
        "delta": 0.14500000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8825,
        "manipulation_rate": 1.0,
        "delta": 0.11750000000000005,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.05,
        "delta": -0.855,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.0,
        "delta": -0.225,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 0.0,
        "delta": -0.78,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.0,
        "delta": -0.45,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.0,
        "delta": -0.1625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.5925,
        "delta": -0.3125,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.735,
        "delta": 0.51,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 1.0,
        "delta": 0.21999999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 399,
        "manipulation_n": 375,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.975,
        "delta": 0.8125,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 1.0,
        "delta": 0.09499999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.165,
        "delta": -0.06,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 0.0,
        "delta": -0.78,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0175,
        "delta": 0.0175,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.025,
        "delta": -0.425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.0,
        "delta": -0.1625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.2325,
        "delta": -0.6725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.4425,
        "delta": 0.2175,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 1.0,
        "delta": 0.21999999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 399,
        "manipulation_n": 396,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.95,
        "delta": 0.7875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.6375,
        "delta": -0.26750000000000007,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.025,
        "delta": -0.2,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.075,
        "delta": -0.925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 0.025,
        "delta": -0.755,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.007518796992481203,
        "delta": 0.007518796992481203,
        "baseline_n": 399,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.025,
        "delta": -0.425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.025,
        "delta": -0.1375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.7475,
        "delta": -0.15749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.85,
        "delta": 0.625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.78,
        "manipulation_rate": 1.0,
        "delta": 0.21999999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1625,
        "manipulation_rate": 0.9475,
        "delta": 0.785,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.02,
        "delta": -0.955,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.0,
        "delta": -0.2075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 0.0,
        "delta": -0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 0.0,
        "delta": -0.6473551637279596,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.95,
        "delta": 0.7424999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.02564102564102564,
        "delta": 0.0006410256410256387,
        "baseline_n": 400,
        "manipulation_n": 390,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 1.0,
        "delta": 0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 1.0,
        "delta": 0.35264483627204035,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.8975,
        "delta": -0.07750000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.3,
        "delta": 0.0925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025252525252525252,
        "delta": 0.0002525252525252507,
        "baseline_n": 400,
        "manipulation_n": 396,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 0.595,
        "delta": 0.09499999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 0.975,
        "delta": 0.32764483627204033,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.87,
        "delta": 0.6625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.020100502512562814,
        "delta": -0.004899497487437187,
        "baseline_n": 400,
        "manipulation_n": 398,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 1.0,
        "delta": 0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 1.0,
        "delta": 0.35264483627204035,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.775,
        "delta": -0.19999999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.05,
        "delta": -0.15749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.275,
        "delta": -0.725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.15,
        "delta": -0.85,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.02072538860103627,
        "delta": -0.004274611398963731,
        "baseline_n": 400,
        "manipulation_n": 386,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 0.025,
        "delta": -0.475,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 0.075,
        "delta": -0.5723551637279597,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.7425,
        "delta": 0.535,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 1.0,
        "delta": 0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6473551637279596,
        "manipulation_rate": 0.99,
        "delta": 0.34264483627204034,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.025,
        "delta": -0.9725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.0,
        "delta": -0.15,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.05,
        "delta": 0.05,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 0.0,
        "delta": -0.8266331658291457,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 0.0,
        "delta": -0.6075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.975,
        "delta": -0.022500000000000075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.575,
        "delta": 0.42499999999999993,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 0.975,
        "delta": 0.14836683417085428,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 1.0,
        "delta": 0.39249999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.7775,
        "delta": -0.22000000000000008,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.075,
        "delta": -0.075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.4725,
        "delta": -0.5275000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.5775,
        "delta": -0.4225,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.017543859649122806,
        "delta": 0.017543859649122806,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 0.15,
        "delta": -0.6766331658291457,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 0.1,
        "delta": -0.5075000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 1.0,
        "delta": 0.0024999999999999467,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.545,
        "delta": 0.395,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 1.0,
        "delta": 0.1733668341708543,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 1.0,
        "delta": 0.39249999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.125,
        "delta": -0.8725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.025,
        "delta": -0.125,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.05,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 0.025,
        "delta": -0.8016331658291457,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 0.05,
        "delta": -0.5575,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.95,
        "delta": -0.0475000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.6725,
        "delta": 0.5225,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8266331658291457,
        "manipulation_rate": 0.95,
        "delta": 0.12336683417085426,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6075,
        "manipulation_rate": 0.925,
        "delta": 0.3175,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 0.075,
        "delta": -0.905,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.4625,
        "delta": 0.20750000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 0.0,
        "delta": -0.0775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 0.0,
        "delta": -0.6926952141057935,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 0.975,
        "delta": -0.0050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.9,
        "delta": 0.645,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.04050632911392405,
        "delta": 0.04050632911392405,
        "baseline_n": 400,
        "manipulation_n": 395,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 1.0,
        "delta": 0.9225,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 1.0,
        "delta": 0.3073047858942065,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 1.0,
        "delta": 0.020000000000000018,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.6475,
        "delta": 0.39249999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.6625,
        "delta": -0.3375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 0.025,
        "delta": -0.0525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 0.925,
        "delta": 0.23230478589420656,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 1.0,
        "delta": 0.020000000000000018,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.515,
        "delta": 0.26,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.9,
        "delta": -0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.9975,
        "delta": -0.0024999999999999467,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 0.975,
        "delta": 0.8975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 1.0,
        "delta": 0.3073047858942065,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 0.9425,
        "delta": -0.03749999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.15,
        "delta": -0.10500000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0625,
        "delta": -0.9375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 0.025,
        "delta": -0.0525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 0.025,
        "delta": -0.6676952141057935,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.98,
        "manipulation_rate": 0.975,
        "delta": -0.0050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.785,
        "delta": 0.53,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.007518796992481203,
        "delta": 0.007518796992481203,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0775,
        "manipulation_rate": 0.825,
        "delta": 0.7474999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6926952141057935,
        "manipulation_rate": 0.975,
        "delta": 0.2823047858942065,
        "baseline_n": 397,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.0,
        "delta": -0.67,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.0,
        "delta": -0.17293233082706766,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 0.0,
        "delta": -0.4375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.975,
        "delta": 0.30499999999999994,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.525,
        "delta": 0.35206766917293236,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.3231552162849873,
        "delta": 0.29815521628498726,
        "baseline_n": 400,
        "manipulation_n": 393,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 1.0,
        "delta": 0.5625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.9,
        "delta": 0.22999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.2,
        "delta": 0.02706766917293235,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.5225,
        "delta": -0.47750000000000004,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.025,
        "delta": -0.875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 0.35,
        "delta": -0.08750000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.85,
        "delta": 0.17999999999999994,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.275,
        "delta": 0.10206766917293236,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0728643216080402,
        "delta": 0.047864321608040196,
        "baseline_n": 400,
        "manipulation_n": 398,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 0.975,
        "delta": 0.5375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.1,
        "delta": -0.5700000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.035,
        "delta": -0.13793233082706766,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0975,
        "delta": -0.9025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 0.0,
        "delta": -0.4375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.67,
        "manipulation_rate": 0.9275,
        "delta": 0.25749999999999995,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.17293233082706766,
        "manipulation_rate": 0.29,
        "delta": 0.11706766917293232,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.09774436090225563,
        "delta": 0.07274436090225564,
        "baseline_n": 400,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.975,
        "delta": 0.07499999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4375,
        "manipulation_rate": 0.975,
        "delta": 0.5375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      }
    ],
    "probe_behavioral": [
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.8275,
        "probe_delta_L31": -197.5689697265625,
        "probe_delta_L43": 141.1806640625,
        "probe_delta_L55": 319.91748046875
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.5025,
        "probe_delta_L31": -105.02716064453125,
        "probe_delta_L43": 418.80377197265625,
        "probe_delta_L55": 706.42626953125
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -182.20245361328125,
        "probe_delta_L43": 529.8802490234375,
        "probe_delta_L55": 1620.3863525390625
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -470.0123291015625,
        "probe_delta_L43": -183.0811767578125,
        "probe_delta_L55": 486.353271484375
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.1,
        "probe_delta_L31": -55.3544921875,
        "probe_delta_L43": 362.7840576171875,
        "probe_delta_L55": 1243.378662109375
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.06060606060606061,
        "probe_delta_L31": -198.07647705078125,
        "probe_delta_L43": 756.1795043945312,
        "probe_delta_L55": 2062.40673828125
      },
      {
        "prompt_id": "cheese_neg_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.707808564231738,
        "probe_delta_L31": -329.07403564453125,
        "probe_delta_L43": -60.64501953125,
        "probe_delta_L55": 456.194091796875
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.07250000000000001,
        "probe_delta_L31": 142.568359375,
        "probe_delta_L43": 470.989990234375,
        "probe_delta_L55": 687.892822265625
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.22250000000000003,
        "probe_delta_L31": 119.72747802734375,
        "probe_delta_L43": 810.5535278320312,
        "probe_delta_L55": 1440.446533203125
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 70.2396240234375,
        "probe_delta_L43": 860.3751220703125,
        "probe_delta_L55": 1096.7882080078125
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 44.208984375,
        "probe_delta_L43": 265.3023681640625,
        "probe_delta_L55": 333.744873046875
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.2075,
        "probe_delta_L31": 53.7335205078125,
        "probe_delta_L43": 390.5220947265625,
        "probe_delta_L55": 725.8115234375
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8143939393939394,
        "probe_delta_L31": 270.2640380859375,
        "probe_delta_L43": 1193.22705078125,
        "probe_delta_L55": 1980.5799560546875
      },
      {
        "prompt_id": "cheese_pos_persona",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.292191435768262,
        "probe_delta_L31": 163.0616455078125,
        "probe_delta_L43": 600.376708984375,
        "probe_delta_L55": 893.17822265625
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.8025,
        "probe_delta_L31": -153.20458984375,
        "probe_delta_L43": 218.72998046875,
        "probe_delta_L55": 164.88525390625
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.5025,
        "probe_delta_L31": -70.48333740234375,
        "probe_delta_L43": 624.6754760742188,
        "probe_delta_L55": 365.035400390625
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -95.712890625,
        "probe_delta_L43": 925.6741943359375,
        "probe_delta_L55": 1439.3785400390625
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -442.38702392578125,
        "probe_delta_L43": 10.2276611328125,
        "probe_delta_L55": 688.298583984375
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.1,
        "probe_delta_L31": -54.2376708984375,
        "probe_delta_L43": 533.7469482421875,
        "probe_delta_L55": 1052.792724609375
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.06060606060606061,
        "probe_delta_L31": -195.30224609375,
        "probe_delta_L43": 717.5504760742188,
        "probe_delta_L55": 1758.9830322265625
      },
      {
        "prompt_id": "cheese_neg_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.707808564231738,
        "probe_delta_L31": -440.95672607421875,
        "probe_delta_L43": -56.894775390625,
        "probe_delta_L55": -61.0443115234375
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.1725,
        "probe_delta_L31": 257.19580078125,
        "probe_delta_L43": 628.19189453125,
        "probe_delta_L55": 1009.297607421875
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.4325000000000001,
        "probe_delta_L31": 133.19525146484375,
        "probe_delta_L43": 817.9226684570312,
        "probe_delta_L55": 1842.740234375
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 78.0382080078125,
        "probe_delta_L43": 291.7119140625,
        "probe_delta_L55": 571.439208984375
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 9.6842041015625,
        "probe_delta_L43": 259.9263916015625,
        "probe_delta_L55": 310.75927734375
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.24999999999999997,
        "probe_delta_L31": -10.7598876953125,
        "probe_delta_L43": 277.5640869140625,
        "probe_delta_L55": 398.688720703125
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.9393939393939394,
        "probe_delta_L31": 229.7828369140625,
        "probe_delta_L43": 903.9201049804688,
        "probe_delta_L55": 1876.5665283203125
      },
      {
        "prompt_id": "cheese_pos_experiential",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.292191435768262,
        "probe_delta_L31": 178.940673828125,
        "probe_delta_L43": 769.907470703125,
        "probe_delta_L55": 1376.7080078125
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.7025,
        "probe_delta_L31": 15.7098388671875,
        "probe_delta_L43": 20.208740234375,
        "probe_delta_L55": -197.02392578125
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.45249999999999996,
        "probe_delta_L31": -96.076904296875,
        "probe_delta_L43": 250.75946044921875,
        "probe_delta_L55": 580.1566162109375
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9575,
        "probe_delta_L31": -25.01904296875,
        "probe_delta_L43": 772.0797119140625,
        "probe_delta_L55": 1159.1854248046875
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.875,
        "probe_delta_L31": -329.896240234375,
        "probe_delta_L43": -104.5850830078125,
        "probe_delta_L55": 373.075927734375
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.07500000000000001,
        "probe_delta_L31": 15.8621826171875,
        "probe_delta_L43": 301.1820068359375,
        "probe_delta_L55": 851.59423828125
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.06060606060606061,
        "probe_delta_L31": -82.74249267578125,
        "probe_delta_L43": 902.2794799804688,
        "probe_delta_L55": 2111.052734375
      },
      {
        "prompt_id": "cheese_neg_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.682808564231738,
        "probe_delta_L31": -224.263671875,
        "probe_delta_L43": -455.630126953125,
        "probe_delta_L55": -433.78369140625
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.13,
        "probe_delta_L31": 224.78564453125,
        "probe_delta_L43": 608.066650390625,
        "probe_delta_L55": 357.238525390625
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.49750000000000005,
        "probe_delta_L31": 107.91131591796875,
        "probe_delta_L43": 561.0277709960938,
        "probe_delta_L55": 1086.75439453125
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 71.0582275390625,
        "probe_delta_L43": 355.4105224609375,
        "probe_delta_L55": 570.555419921875
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": -5.5848388671875,
        "probe_delta_L43": 89.15234375,
        "probe_delta_L55": 221.621337890625
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.23500000000000001,
        "probe_delta_L31": 8.3800048828125,
        "probe_delta_L43": 116.51953125,
        "probe_delta_L55": 602.1904296875
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.9143939393939393,
        "probe_delta_L31": 235.4757080078125,
        "probe_delta_L43": 602.0442504882812,
        "probe_delta_L55": 1270.2545166015625
      },
      {
        "prompt_id": "cheese_pos_value",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.292191435768262,
        "probe_delta_L31": 161.6837158203125,
        "probe_delta_L43": 549.409423828125,
        "probe_delta_L55": 645.838623046875
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.75,
        "probe_delta_L31": -160.012939453125,
        "probe_delta_L43": 79.4239501953125,
        "probe_delta_L55": 209.051025390625
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.3425,
        "probe_delta_L31": 59.1563720703125,
        "probe_delta_L43": 397.75762939453125,
        "probe_delta_L55": 1310.970947265625
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9675,
        "probe_delta_L31": 9.64617919921875,
        "probe_delta_L43": 587.28955078125,
        "probe_delta_L55": 1763.118408203125
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -170.242919921875,
        "probe_delta_L43": -95.9732666015625,
        "probe_delta_L55": 326.48388671875
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0025,
        "probe_delta_L31": 14.7442626953125,
        "probe_delta_L43": 196.3782958984375,
        "probe_delta_L55": 1521.8302001953125
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.285,
        "probe_delta_L31": 37.706787109375,
        "probe_delta_L43": 755.0072021484375,
        "probe_delta_L55": 1075.33642578125
      },
      {
        "prompt_id": "rainy_weather_neg_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4798994974874372,
        "probe_delta_L31": -327.3336181640625,
        "probe_delta_L43": 26.90380859375,
        "probe_delta_L55": 309.279052734375
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.25,
        "probe_delta_L31": 217.4017333984375,
        "probe_delta_L43": 588.5086669921875,
        "probe_delta_L55": 1154.824951171875
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5625,
        "probe_delta_L31": 267.92291259765625,
        "probe_delta_L43": 1180.767822265625,
        "probe_delta_L55": 2053.912841796875
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.03249999999999997,
        "probe_delta_L31": 191.5250244140625,
        "probe_delta_L43": 1029.3731689453125,
        "probe_delta_L55": 1737.1201171875
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 89.9974365234375,
        "probe_delta_L43": 373.51220703125,
        "probe_delta_L55": 708.73291015625
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.049868766404199474,
        "probe_delta_L31": 87.8692626953125,
        "probe_delta_L43": 183.4705810546875,
        "probe_delta_L55": 615.1986083984375
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.7150000000000001,
        "probe_delta_L31": 214.8155517578125,
        "probe_delta_L43": 842.9810791015625,
        "probe_delta_L55": 368.524658203125
      },
      {
        "prompt_id": "rainy_weather_pos_persona",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5201005025125628,
        "probe_delta_L31": 73.453125,
        "probe_delta_L43": 842.60498046875,
        "probe_delta_L55": 1214.4814453125
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.04500000000000004,
        "probe_delta_L31": -115.230224609375,
        "probe_delta_L43": 174.929443359375,
        "probe_delta_L55": -169.8673095703125
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.08500000000000002,
        "probe_delta_L31": -25.00445556640625,
        "probe_delta_L43": 475.26373291015625,
        "probe_delta_L55": 806.3655395507812
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.885,
        "probe_delta_L31": -74.36810302734375,
        "probe_delta_L43": 452.8778076171875,
        "probe_delta_L55": 408.194580078125
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 12.3465576171875,
        "probe_delta_L43": 47.577880859375,
        "probe_delta_L55": 567.350341796875
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": -30.77899169921875,
        "probe_delta_L43": -152.28662109375,
        "probe_delta_L55": -440.0625
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.25999999999999995,
        "probe_delta_L31": -38.1712646484375,
        "probe_delta_L43": 418.4417724609375,
        "probe_delta_L55": 521.0989990234375
      },
      {
        "prompt_id": "rainy_weather_neg_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.3298994974874372,
        "probe_delta_L31": -268.47296142578125,
        "probe_delta_L43": -30.704833984375,
        "probe_delta_L55": -335.635498046875
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.25,
        "probe_delta_L31": 145.03515625,
        "probe_delta_L43": 728.6370849609375,
        "probe_delta_L55": 376.703369140625
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5475,
        "probe_delta_L31": 198.49468994140625,
        "probe_delta_L43": 668.7053833007812,
        "probe_delta_L55": 1974.2041015625
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.03249999999999997,
        "probe_delta_L31": 103.036376953125,
        "probe_delta_L43": 412.337646484375,
        "probe_delta_L55": 491.07666015625
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 55.480712890625,
        "probe_delta_L43": 214.3463134765625,
        "probe_delta_L55": 346.156005859375
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.06818181818181818,
        "probe_delta_L31": -4.04229736328125,
        "probe_delta_L43": 85.2589111328125,
        "probe_delta_L55": 171.22802734375
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.7150000000000001,
        "probe_delta_L31": 138.6109619140625,
        "probe_delta_L43": 457.85791015625,
        "probe_delta_L55": 424.74951171875
      },
      {
        "prompt_id": "rainy_weather_pos_experiential",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5201005025125628,
        "probe_delta_L31": 56.16259765625,
        "probe_delta_L43": 851.510986328125,
        "probe_delta_L55": 582.99365234375
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.25,
        "probe_delta_L31": -7.8232421875,
        "probe_delta_L43": 519.1697998046875,
        "probe_delta_L55": 511.8673095703125
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.29250000000000004,
        "probe_delta_L31": 32.51605224609375,
        "probe_delta_L43": 521.5405883789062,
        "probe_delta_L55": 926.3523559570312
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9425,
        "probe_delta_L31": 102.01513671875,
        "probe_delta_L43": 924.5777587890625,
        "probe_delta_L55": 1258.401123046875
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.975,
        "probe_delta_L31": 18.3284912109375,
        "probe_delta_L43": 13.1856689453125,
        "probe_delta_L55": 271.4365234375
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 117.8079833984375,
        "probe_delta_L43": 255.1795654296875,
        "probe_delta_L55": 683.6485595703125
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.25999999999999995,
        "probe_delta_L31": 73.7181396484375,
        "probe_delta_L43": 560.0216064453125,
        "probe_delta_L55": 387.543212890625
      },
      {
        "prompt_id": "rainy_weather_neg_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4548994974874372,
        "probe_delta_L31": -176.33349609375,
        "probe_delta_L43": 204.9879150390625,
        "probe_delta_L55": -95.57861328125
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.25,
        "probe_delta_L31": 139.7672119140625,
        "probe_delta_L43": 585.2974853515625,
        "probe_delta_L55": 438.2674560546875
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5875,
        "probe_delta_L31": 130.3839111328125,
        "probe_delta_L43": 314.41766357421875,
        "probe_delta_L55": 1085.665771484375
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.03249999999999997,
        "probe_delta_L31": 60.8568115234375,
        "probe_delta_L43": 250.2259521484375,
        "probe_delta_L55": 230.2427978515625
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 6.841552734375,
        "probe_delta_L43": 187.3824462890625,
        "probe_delta_L55": 329.724609375
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 40.04345703125,
        "probe_delta_L43": 337.6600341796875,
        "probe_delta_L55": 581.1798095703125
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.665,
        "probe_delta_L31": 144.7823486328125,
        "probe_delta_L43": 204.805419921875,
        "probe_delta_L55": 29.250244140625
      },
      {
        "prompt_id": "rainy_weather_pos_value",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.4776005025125628,
        "probe_delta_L31": 75.4349365234375,
        "probe_delta_L43": 720.8564453125,
        "probe_delta_L55": 483.796630859375
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.8524999999999999,
        "probe_delta_L31": -99.689453125,
        "probe_delta_L43": 351.3707275390625,
        "probe_delta_L55": 485.10302734375
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.445,
        "probe_delta_L31": -16.53125,
        "probe_delta_L43": 308.68048095703125,
        "probe_delta_L55": 888.3375244140625
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -129.83636474609375,
        "probe_delta_L43": 383.7578125,
        "probe_delta_L55": 1093.78857421875
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -381.402587890625,
        "probe_delta_L43": -87.9287109375,
        "probe_delta_L55": 509.645751953125
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -28.75341796875,
        "probe_delta_L43": 343.7127685546875,
        "probe_delta_L55": 1113.8045654296875
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.855,
        "probe_delta_L31": -279.40362548828125,
        "probe_delta_L43": 380.22314453125,
        "probe_delta_L55": 1076.04150390625
      },
      {
        "prompt_id": "cats_neg_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8825,
        "probe_delta_L31": -359.7999267578125,
        "probe_delta_L43": -47.1112060546875,
        "probe_delta_L55": 350.0963134765625
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.09750000000000003,
        "probe_delta_L31": 279.8900146484375,
        "probe_delta_L43": 866.9556884765625,
        "probe_delta_L55": 674.037353515625
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.4071303258145363,
        "probe_delta_L31": 268.38818359375,
        "probe_delta_L43": 1006.8869018554688,
        "probe_delta_L55": 1779.3609619140625
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 129.3319091796875,
        "probe_delta_L43": 825.75537109375,
        "probe_delta_L55": 1344.565185546875
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 41.458740234375,
        "probe_delta_L43": 380.106689453125,
        "probe_delta_L55": 805.60400390625
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0012467191601049851,
        "probe_delta_L31": 41.449951171875,
        "probe_delta_L43": 202.19775390625,
        "probe_delta_L55": -63.4415283203125
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.14500000000000002,
        "probe_delta_L31": 165.834228515625,
        "probe_delta_L43": 971.3497314453125,
        "probe_delta_L55": 1260.474365234375
      },
      {
        "prompt_id": "cats_pos_persona",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.11750000000000005,
        "probe_delta_L31": 135.666259765625,
        "probe_delta_L43": 889.32275390625,
        "probe_delta_L55": 930.9493408203125
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.05249999999999999,
        "probe_delta_L31": -47.4559326171875,
        "probe_delta_L43": 422.4716796875,
        "probe_delta_L55": -21.8570556640625
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.08000000000000002,
        "probe_delta_L31": -7.6251220703125,
        "probe_delta_L43": 421.65484619140625,
        "probe_delta_L55": 422.3341064453125
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.10499999999999998,
        "probe_delta_L31": 6.8614501953125,
        "probe_delta_L43": 457.8939208984375,
        "probe_delta_L55": 500.5811767578125
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.19499999999999995,
        "probe_delta_L31": -257.97314453125,
        "probe_delta_L43": -123.948974609375,
        "probe_delta_L55": 249.50537109375
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 50.927001953125,
        "probe_delta_L43": 175.0115966796875,
        "probe_delta_L55": -135.690673828125
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.83,
        "probe_delta_L31": -206.823486328125,
        "probe_delta_L43": 275.8997802734375,
        "probe_delta_L55": 733.956298828125
      },
      {
        "prompt_id": "cats_neg_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.7075,
        "probe_delta_L31": -248.42974853515625,
        "probe_delta_L43": -95.0743408203125,
        "probe_delta_L55": -80.1143798828125
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.09500000000000008,
        "probe_delta_L31": 269.7794189453125,
        "probe_delta_L43": 929.135009765625,
        "probe_delta_L55": 1001.35400390625
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.21749999999999997,
        "probe_delta_L31": 220.8226318359375,
        "probe_delta_L43": 395.05804443359375,
        "probe_delta_L55": 1273.2625732421875
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 111.6485595703125,
        "probe_delta_L43": 412.88818359375,
        "probe_delta_L55": 422.877685546875
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 38.272705078125,
        "probe_delta_L43": 251.65185546875,
        "probe_delta_L55": 258.89697265625
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 74.7535400390625,
        "probe_delta_L43": 340.247802734375,
        "probe_delta_L55": 44.7987060546875
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.14500000000000002,
        "probe_delta_L31": 190.3665771484375,
        "probe_delta_L43": 846.3782958984375,
        "probe_delta_L55": 1614.585205078125
      },
      {
        "prompt_id": "cats_pos_experiential",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.11750000000000005,
        "probe_delta_L31": 209.0830078125,
        "probe_delta_L43": 990.359619140625,
        "probe_delta_L55": 1037.7750244140625
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.5774999999999999,
        "probe_delta_L31": 115.0880126953125,
        "probe_delta_L43": 461.2484130859375,
        "probe_delta_L55": 478.4892578125
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.32,
        "probe_delta_L31": 11.025634765625,
        "probe_delta_L43": 313.57916259765625,
        "probe_delta_L55": 602.1416015625
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.87,
        "probe_delta_L31": 3.5069580078125,
        "probe_delta_L43": 449.9930419921875,
        "probe_delta_L55": 766.738525390625
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9249999999999999,
        "probe_delta_L31": -272.43255615234375,
        "probe_delta_L43": -104.0302734375,
        "probe_delta_L55": 59.7847900390625
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.0024436090225563936,
        "probe_delta_L31": 103.9664306640625,
        "probe_delta_L43": 456.380126953125,
        "probe_delta_L55": 344.338623046875
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.7999999999999999,
        "probe_delta_L31": -219.03753662109375,
        "probe_delta_L43": 259.8443603515625,
        "probe_delta_L55": 970.84423828125
      },
      {
        "prompt_id": "cats_neg_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8524999999999999,
        "probe_delta_L31": -236.7647705078125,
        "probe_delta_L43": -73.86962890625,
        "probe_delta_L55": 234.4010009765625
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.039999999999999925,
        "probe_delta_L31": 197.31005859375,
        "probe_delta_L43": 633.337158203125,
        "probe_delta_L55": 1085.572998046875
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.2475,
        "probe_delta_L31": 147.39794921875,
        "probe_delta_L43": 317.40118408203125,
        "probe_delta_L55": 621.0159912109375
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 75.7623291015625,
        "probe_delta_L43": 305.386474609375,
        "probe_delta_L55": 304.963134765625
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 36.8251953125,
        "probe_delta_L43": 149.03759765625,
        "probe_delta_L55": 292.31103515625
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.004899497487437187,
        "probe_delta_L31": 108.1158447265625,
        "probe_delta_L43": 560.0517578125,
        "probe_delta_L55": 313.547119140625
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.14500000000000002,
        "probe_delta_L31": 131.068359375,
        "probe_delta_L43": 409.5625,
        "probe_delta_L55": 746.02587890625
      },
      {
        "prompt_id": "cats_pos_value",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.11750000000000005,
        "probe_delta_L31": 181.6453857421875,
        "probe_delta_L43": 914.491943359375,
        "probe_delta_L55": 1181.8817138671875
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.855,
        "probe_delta_L31": -124.50531005859375,
        "probe_delta_L43": 162.048583984375,
        "probe_delta_L55": 336.4417724609375
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.225,
        "probe_delta_L31": 23.5360107421875,
        "probe_delta_L43": 445.567626953125,
        "probe_delta_L55": 1941.5640869140625
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": 16.9945068359375,
        "probe_delta_L43": 625.210693359375,
        "probe_delta_L55": 1104.802001953125
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.78,
        "probe_delta_L31": -128.9432373046875,
        "probe_delta_L43": -26.541015625,
        "probe_delta_L55": 58.4033203125
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -40.44903564453125,
        "probe_delta_L43": 164.1614990234375,
        "probe_delta_L55": -215.32568359375
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.45,
        "probe_delta_L31": -43.1070556640625,
        "probe_delta_L43": 82.4637451171875,
        "probe_delta_L55": 431.808837890625
      },
      {
        "prompt_id": "classical_music_neg_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.1625,
        "probe_delta_L31": 16.40771484375,
        "probe_delta_L43": 618.9281005859375,
        "probe_delta_L55": 1468.297607421875
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.3125,
        "probe_delta_L31": -53.6148681640625,
        "probe_delta_L43": 364.6983642578125,
        "probe_delta_L55": 264.5213623046875
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.51,
        "probe_delta_L31": 235.89892578125,
        "probe_delta_L43": 956.3665771484375,
        "probe_delta_L55": 1117.7291259765625
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 60.08447265625,
        "probe_delta_L43": 699.621337890625,
        "probe_delta_L55": 401.869140625
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.21999999999999997,
        "probe_delta_L31": 106.5433349609375,
        "probe_delta_L43": 211.5965576171875,
        "probe_delta_L55": -229.087890625
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 7.54241943359375,
        "probe_delta_L43": -97.7718505859375,
        "probe_delta_L55": -872.5588989257812
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 150.8887939453125,
        "probe_delta_L43": 277.7703857421875,
        "probe_delta_L55": 138.21435546875
      },
      {
        "prompt_id": "classical_music_pos_persona",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8125,
        "probe_delta_L31": 342.35595703125,
        "probe_delta_L43": 1176.8841552734375,
        "probe_delta_L55": 963.1085815429688
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.09499999999999997,
        "probe_delta_L31": 25.74920654296875,
        "probe_delta_L43": 253.471435546875,
        "probe_delta_L55": -189.72607421875
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.06,
        "probe_delta_L31": 82.57122802734375,
        "probe_delta_L43": 185.0147705078125,
        "probe_delta_L55": 642.3685302734375
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": 50.52685546875,
        "probe_delta_L43": 306.607666015625,
        "probe_delta_L55": 665.300537109375
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.78,
        "probe_delta_L31": -46.57373046875,
        "probe_delta_L43": -293.057861328125,
        "probe_delta_L55": -473.5889892578125
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0175,
        "probe_delta_L31": -17.4107666015625,
        "probe_delta_L43": -133.634765625,
        "probe_delta_L55": -383.363525390625
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.425,
        "probe_delta_L31": -30.13134765625,
        "probe_delta_L43": -236.1591796875,
        "probe_delta_L55": -290.1229248046875
      },
      {
        "prompt_id": "classical_music_neg_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.1625,
        "probe_delta_L31": 97.0816650390625,
        "probe_delta_L43": 323.3726806640625,
        "probe_delta_L55": 468.03680419921875
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.6725,
        "probe_delta_L31": 90.50433349609375,
        "probe_delta_L43": 828.1510009765625,
        "probe_delta_L55": 666.403564453125
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.2175,
        "probe_delta_L31": 147.5203857421875,
        "probe_delta_L43": 228.76513671875,
        "probe_delta_L55": 869.8663330078125
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 126.78515625,
        "probe_delta_L43": 133.189208984375,
        "probe_delta_L55": 440.548583984375
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.21999999999999997,
        "probe_delta_L31": 97.144287109375,
        "probe_delta_L43": 129.236083984375,
        "probe_delta_L55": 294.833251953125
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -49.2230224609375,
        "probe_delta_L43": -41.322998046875,
        "probe_delta_L55": -608.387939453125
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 157.2083740234375,
        "probe_delta_L43": 174.46728515625,
        "probe_delta_L55": 664.983154296875
      },
      {
        "prompt_id": "classical_music_pos_experiential",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.7875,
        "probe_delta_L31": 316.9495849609375,
        "probe_delta_L43": 1151.9237060546875,
        "probe_delta_L55": 1541.80712890625
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.26750000000000007,
        "probe_delta_L31": 51.28216552734375,
        "probe_delta_L43": 94.83203125,
        "probe_delta_L55": -299.94775390625
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2,
        "probe_delta_L31": 66.67169189453125,
        "probe_delta_L43": 172.9888916015625,
        "probe_delta_L55": 763.9664306640625
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.925,
        "probe_delta_L31": 60.1353759765625,
        "probe_delta_L43": 368.060302734375,
        "probe_delta_L55": 731.146240234375
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.755,
        "probe_delta_L31": -191.51116943359375,
        "probe_delta_L43": -236.0654296875,
        "probe_delta_L55": 38.538330078125
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.007518796992481203,
        "probe_delta_L31": -43.7041015625,
        "probe_delta_L43": -28.729736328125,
        "probe_delta_L55": -375.02734375
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.425,
        "probe_delta_L31": -97.7115478515625,
        "probe_delta_L43": -15.4305419921875,
        "probe_delta_L55": 441.799560546875
      },
      {
        "prompt_id": "classical_music_neg_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.1375,
        "probe_delta_L31": 67.93133544921875,
        "probe_delta_L43": 254.719970703125,
        "probe_delta_L55": 358.79571533203125
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.15749999999999997,
        "probe_delta_L31": 38.27655029296875,
        "probe_delta_L43": 598.51416015625,
        "probe_delta_L55": 659.2568359375
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.625,
        "probe_delta_L31": 71.5518798828125,
        "probe_delta_L43": 140.854736328125,
        "probe_delta_L55": 559.270751953125
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 55.14404296875,
        "probe_delta_L43": 97.149169921875,
        "probe_delta_L55": 147.24560546875
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.21999999999999997,
        "probe_delta_L31": 71.2354736328125,
        "probe_delta_L43": -122.3673095703125,
        "probe_delta_L55": -104.763671875
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -66.19549560546875,
        "probe_delta_L43": -70.631591796875,
        "probe_delta_L55": -584.8114013671875
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 101.8348388671875,
        "probe_delta_L43": -4.1317138671875,
        "probe_delta_L55": 285.697021484375
      },
      {
        "prompt_id": "classical_music_pos_value",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.785,
        "probe_delta_L31": 155.5755615234375,
        "probe_delta_L43": 705.9205322265625,
        "probe_delta_L55": 813.2490844726562
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.955,
        "probe_delta_L31": -204.11016845703125,
        "probe_delta_L43": -64.15771484375,
        "probe_delta_L55": -111.95556640625
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2075,
        "probe_delta_L31": 66.1373291015625,
        "probe_delta_L43": 1027.795654296875,
        "probe_delta_L55": 2036.405517578125
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -160.786376953125,
        "probe_delta_L43": 461.6922607421875,
        "probe_delta_L55": 1116.5048828125
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -507.562255859375,
        "probe_delta_L43": -259.197265625,
        "probe_delta_L55": -118.010498046875
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 10.0386962890625,
        "probe_delta_L43": 271.14794921875,
        "probe_delta_L55": -431.5731201171875
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5,
        "probe_delta_L31": -188.92010498046875,
        "probe_delta_L43": 665.1093139648438,
        "probe_delta_L55": 1105.89892578125
      },
      {
        "prompt_id": "gardening_neg_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6473551637279596,
        "probe_delta_L31": -291.60589599609375,
        "probe_delta_L43": 126.7342529296875,
        "probe_delta_L55": 147.64404296875
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 197.8641357421875,
        "probe_delta_L43": 470.121826171875,
        "probe_delta_L55": 214.070068359375
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.7424999999999999,
        "probe_delta_L31": 303.054931640625,
        "probe_delta_L43": 1435.4827880859375,
        "probe_delta_L55": 2246.968505859375
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 77.982666015625,
        "probe_delta_L43": 904.8714599609375,
        "probe_delta_L55": 1031.54443359375
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 18.0169677734375,
        "probe_delta_L43": 341.609619140625,
        "probe_delta_L55": 470.190673828125
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0006410256410256387,
        "probe_delta_L31": 65.2734375,
        "probe_delta_L43": 182.93408203125,
        "probe_delta_L55": -489.3690185546875
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5,
        "probe_delta_L31": 271.621337890625,
        "probe_delta_L43": 1326.078857421875,
        "probe_delta_L55": 1173.1376953125
      },
      {
        "prompt_id": "gardening_pos_persona",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.35264483627204035,
        "probe_delta_L31": 185.6910400390625,
        "probe_delta_L43": 839.113525390625,
        "probe_delta_L55": 639.946533203125
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.07750000000000001,
        "probe_delta_L31": 66.6961669921875,
        "probe_delta_L43": 21.644287109375,
        "probe_delta_L55": -578.208740234375
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.0925,
        "probe_delta_L31": 157.0169677734375,
        "probe_delta_L43": 705.565185546875,
        "probe_delta_L55": 901.6526489257812
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": -48.4488525390625,
        "probe_delta_L43": 207.9732666015625,
        "probe_delta_L55": 808.520263671875
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": -191.736328125,
        "probe_delta_L43": -414.8941650390625,
        "probe_delta_L55": -545.3355712890625
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0002525252525252507,
        "probe_delta_L31": 18.3323974609375,
        "probe_delta_L43": -43.232177734375,
        "probe_delta_L55": 27.165283203125
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09499999999999997,
        "probe_delta_L31": 68.4622802734375,
        "probe_delta_L43": 401.00653076171875,
        "probe_delta_L55": 244.2137451171875
      },
      {
        "prompt_id": "gardening_neg_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.32764483627204033,
        "probe_delta_L31": 44.18359375,
        "probe_delta_L43": 76.061279296875,
        "probe_delta_L55": -403.1212158203125
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 302.5673828125,
        "probe_delta_L43": 862.23974609375,
        "probe_delta_L55": 916.15966796875
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.6625,
        "probe_delta_L31": 214.9049072265625,
        "probe_delta_L43": 431.4394836425781,
        "probe_delta_L55": 1017.3838500976562
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 77.0291748046875,
        "probe_delta_L43": 267.1318359375,
        "probe_delta_L55": 503.332763671875
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 40.39013671875,
        "probe_delta_L43": 200.128173828125,
        "probe_delta_L55": 357.809814453125
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.004899497487437187,
        "probe_delta_L31": 62.260009765625,
        "probe_delta_L43": 100.752685546875,
        "probe_delta_L55": 123.0550537109375
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5,
        "probe_delta_L31": 303.83642578125,
        "probe_delta_L43": 1135.09130859375,
        "probe_delta_L55": 1622.2236328125
      },
      {
        "prompt_id": "gardening_pos_experiential",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.35264483627204035,
        "probe_delta_L31": 237.716064453125,
        "probe_delta_L43": 1122.202392578125,
        "probe_delta_L55": 1007.990234375
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.19999999999999996,
        "probe_delta_L31": 111.526611328125,
        "probe_delta_L43": -161.8836669921875,
        "probe_delta_L55": -669.5738525390625
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.15749999999999997,
        "probe_delta_L31": 60.71502685546875,
        "probe_delta_L43": 526.34912109375,
        "probe_delta_L55": 948.0009155273438
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.725,
        "probe_delta_L31": -6.9700927734375,
        "probe_delta_L43": 404.12451171875,
        "probe_delta_L55": 790.23779296875
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.85,
        "probe_delta_L31": -260.308837890625,
        "probe_delta_L43": -212.290283203125,
        "probe_delta_L55": -292.170654296875
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.004274611398963731,
        "probe_delta_L31": 48.4925537109375,
        "probe_delta_L43": 154.048828125,
        "probe_delta_L55": 48.6036376953125
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.475,
        "probe_delta_L31": -164.717529296875,
        "probe_delta_L43": 502.43084716796875,
        "probe_delta_L55": 484.953125
      },
      {
        "prompt_id": "gardening_neg_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5723551637279597,
        "probe_delta_L31": -175.7652587890625,
        "probe_delta_L43": -135.45068359375,
        "probe_delta_L55": -795.9979248046875
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 224.3431396484375,
        "probe_delta_L43": 579.950439453125,
        "probe_delta_L55": 927.58447265625
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.535,
        "probe_delta_L31": 137.836669921875,
        "probe_delta_L43": 440.7931823730469,
        "probe_delta_L55": 917.4296264648438
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 38.866455078125,
        "probe_delta_L43": 292.42431640625,
        "probe_delta_L55": 498.42138671875
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 36.21240234375,
        "probe_delta_L43": 146.59521484375,
        "probe_delta_L55": 334.15673828125
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 33.897705078125,
        "probe_delta_L43": 163.9066162109375,
        "probe_delta_L55": -48.912109375
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5,
        "probe_delta_L31": 193.7093505859375,
        "probe_delta_L43": 805.8513793945312,
        "probe_delta_L55": 1245.1005859375
      },
      {
        "prompt_id": "gardening_pos_value",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.34264483627204034,
        "probe_delta_L31": 192.1693115234375,
        "probe_delta_L43": 725.647216796875,
        "probe_delta_L55": 358.437255859375
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.9725,
        "probe_delta_L31": -353.76092529296875,
        "probe_delta_L43": -236.0787353515625,
        "probe_delta_L55": 357.008544921875
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.15,
        "probe_delta_L31": 75.00152587890625,
        "probe_delta_L43": 822.8236694335938,
        "probe_delta_L55": 2282.765380859375
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -117.5712890625,
        "probe_delta_L43": 592.376708984375,
        "probe_delta_L55": 1492.7142333984375
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -410.1480712890625,
        "probe_delta_L43": -152.1898193359375,
        "probe_delta_L55": 589.107177734375
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.05,
        "probe_delta_L31": 84.7879638671875,
        "probe_delta_L43": 64.5037841796875,
        "probe_delta_L55": 758.268798828125
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8266331658291457,
        "probe_delta_L31": -264.247314453125,
        "probe_delta_L43": 240.5587158203125,
        "probe_delta_L55": 513.451904296875
      },
      {
        "prompt_id": "astronomy_neg_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6075,
        "probe_delta_L31": -175.5181884765625,
        "probe_delta_L43": 434.9459228515625,
        "probe_delta_L55": 1034.387451171875
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.022500000000000075,
        "probe_delta_L31": -27.1470947265625,
        "probe_delta_L43": 157.223876953125,
        "probe_delta_L55": 279.073974609375
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.42499999999999993,
        "probe_delta_L31": 190.42181396484375,
        "probe_delta_L43": 1158.34228515625,
        "probe_delta_L55": 2030.9573974609375
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 58.886474609375,
        "probe_delta_L43": 758.10009765625,
        "probe_delta_L55": 1488.8236083984375
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": -67.3338623046875,
        "probe_delta_L43": 309.85546875,
        "probe_delta_L55": 133.784423828125
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -10.791748046875,
        "probe_delta_L43": 224.2491455078125,
        "probe_delta_L55": 79.276123046875
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.14836683417085428,
        "probe_delta_L31": 104.6036376953125,
        "probe_delta_L43": 681.4124755859375,
        "probe_delta_L55": 532.69580078125
      },
      {
        "prompt_id": "astronomy_pos_persona",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.39249999999999996,
        "probe_delta_L31": 142.69140625,
        "probe_delta_L43": 932.1473388671875,
        "probe_delta_L55": 1261.7421875
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.22000000000000008,
        "probe_delta_L31": -267.392333984375,
        "probe_delta_L43": -322.998779296875,
        "probe_delta_L55": -437.5850830078125
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.075,
        "probe_delta_L31": 39.41973876953125,
        "probe_delta_L43": 698.0455932617188,
        "probe_delta_L55": 1260.500732421875
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.5275000000000001,
        "probe_delta_L31": -118.534912109375,
        "probe_delta_L43": 294.3248291015625,
        "probe_delta_L55": 569.718505859375
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.4225,
        "probe_delta_L31": -265.7769775390625,
        "probe_delta_L43": -81.625244140625,
        "probe_delta_L55": 238.391845703125
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.017543859649122806,
        "probe_delta_L31": -24.1292724609375,
        "probe_delta_L43": -3.865234375,
        "probe_delta_L55": -140.009521484375
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6766331658291457,
        "probe_delta_L31": -168.2862548828125,
        "probe_delta_L43": 10.6968994140625,
        "probe_delta_L55": 332.514892578125
      },
      {
        "prompt_id": "astronomy_neg_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5075000000000001,
        "probe_delta_L31": -107.43017578125,
        "probe_delta_L43": 68.9837646484375,
        "probe_delta_L55": 192.2568359375
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.0024999999999999467,
        "probe_delta_L31": 104.9566650390625,
        "probe_delta_L43": 711.711181640625,
        "probe_delta_L55": 1353.212158203125
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.395,
        "probe_delta_L31": 143.9066162109375,
        "probe_delta_L43": 274.89654541015625,
        "probe_delta_L55": 1029.917236328125
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 40.245361328125,
        "probe_delta_L43": 193.082275390625,
        "probe_delta_L55": 336.6375732421875
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -52.301025390625,
        "probe_delta_L43": 253.734375,
        "probe_delta_L55": 326.048828125
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -21.7342529296875,
        "probe_delta_L43": 60.43017578125,
        "probe_delta_L55": 3.061279296875
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.1733668341708543,
        "probe_delta_L31": 115.7239990234375,
        "probe_delta_L43": 376.6153564453125,
        "probe_delta_L55": 723.484375
      },
      {
        "prompt_id": "astronomy_pos_experiential",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.39249999999999996,
        "probe_delta_L31": 232.38525390625,
        "probe_delta_L43": 1202.8826904296875,
        "probe_delta_L55": 1558.46923828125
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.8725,
        "probe_delta_L31": -129.9232177734375,
        "probe_delta_L43": -424.841796875,
        "probe_delta_L55": -464.5859375
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.125,
        "probe_delta_L31": 54.7847900390625,
        "probe_delta_L43": 202.60009765625,
        "probe_delta_L55": 520.70166015625
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -5.785888671875,
        "probe_delta_L43": 362.6217041015625,
        "probe_delta_L55": 750.7528076171875
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -168.369140625,
        "probe_delta_L43": -274.65185546875,
        "probe_delta_L55": 66.23876953125
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 97.3319091796875,
        "probe_delta_L43": 284.445068359375,
        "probe_delta_L55": 641.081787109375
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8016331658291457,
        "probe_delta_L31": -85.4376220703125,
        "probe_delta_L43": -38.832763671875,
        "probe_delta_L55": 632.30810546875
      },
      {
        "prompt_id": "astronomy_neg_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5575,
        "probe_delta_L31": -82.1151123046875,
        "probe_delta_L43": -33.3404541015625,
        "probe_delta_L55": -165.3358154296875
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.0475000000000001,
        "probe_delta_L31": 12.05615234375,
        "probe_delta_L43": 242.28662109375,
        "probe_delta_L55": 565.245361328125
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5225,
        "probe_delta_L31": 66.3338623046875,
        "probe_delta_L43": 219.4266357421875,
        "probe_delta_L55": 822.0357666015625
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 50.5341796875,
        "probe_delta_L43": 235.5904541015625,
        "probe_delta_L55": 311.572509765625
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -33.6024169921875,
        "probe_delta_L43": 23.494873046875,
        "probe_delta_L55": 336.660400390625
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 36.8277587890625,
        "probe_delta_L43": 170.101318359375,
        "probe_delta_L55": 325.2672119140625
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.12336683417085426,
        "probe_delta_L31": 90.234130859375,
        "probe_delta_L43": 133.7816162109375,
        "probe_delta_L55": 295.987548828125
      },
      {
        "prompt_id": "astronomy_pos_value",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3175,
        "probe_delta_L31": 146.8524169921875,
        "probe_delta_L43": 994.0555419921875,
        "probe_delta_L55": 1131.109619140625
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.905,
        "probe_delta_L31": -251.70025634765625,
        "probe_delta_L43": -192.0164794921875,
        "probe_delta_L55": 229.53564453125
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.20750000000000002,
        "probe_delta_L31": 71.41314697265625,
        "probe_delta_L43": 565.6387329101562,
        "probe_delta_L55": 1327.825439453125
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -189.7894287109375,
        "probe_delta_L43": 466.450439453125,
        "probe_delta_L55": 1221.4754638671875
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -292.09698486328125,
        "probe_delta_L43": 81.986328125,
        "probe_delta_L55": -232.6328125
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 15.6990966796875,
        "probe_delta_L43": 175.796875,
        "probe_delta_L55": -363.6683349609375
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.0775,
        "probe_delta_L31": -131.32391357421875,
        "probe_delta_L43": 351.3895263671875,
        "probe_delta_L55": 594.9305419921875
      },
      {
        "prompt_id": "cooking_neg_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6926952141057935,
        "probe_delta_L31": -358.50836181640625,
        "probe_delta_L43": -36.6083984375,
        "probe_delta_L55": 381.312255859375
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.0050000000000000044,
        "probe_delta_L31": 172.9979248046875,
        "probe_delta_L43": 212.314453125,
        "probe_delta_L55": 427.126220703125
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.645,
        "probe_delta_L31": 191.65777587890625,
        "probe_delta_L43": 697.8908081054688,
        "probe_delta_L55": 970.6787109375
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 16.8758544921875,
        "probe_delta_L43": 569.2200927734375,
        "probe_delta_L55": 5.276123046875
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -7.828125,
        "probe_delta_L43": 339.855224609375,
        "probe_delta_L55": 297.664306640625
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.04050632911392405,
        "probe_delta_L31": 76.98675537109375,
        "probe_delta_L43": 79.9124755859375,
        "probe_delta_L55": -391.80078125
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.9225,
        "probe_delta_L31": 248.422119140625,
        "probe_delta_L43": 1067.351318359375,
        "probe_delta_L55": 1114.469970703125
      },
      {
        "prompt_id": "cooking_pos_persona",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3073047858942065,
        "probe_delta_L31": 95.7388916015625,
        "probe_delta_L43": 667.7786865234375,
        "probe_delta_L55": 719.844482421875
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.020000000000000018,
        "probe_delta_L31": -31.5284423828125,
        "probe_delta_L43": -348.045654296875,
        "probe_delta_L55": -731.08154296875
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.39249999999999996,
        "probe_delta_L31": 25.3001708984375,
        "probe_delta_L43": -0.95989990234375,
        "probe_delta_L55": -364.794921875
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": -98.3447265625,
        "probe_delta_L43": 423.9578857421875,
        "probe_delta_L55": 524.14453125
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.3375,
        "probe_delta_L31": -182.9447021484375,
        "probe_delta_L43": -107.3990478515625,
        "probe_delta_L55": -119.540283203125
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 36.8258056640625,
        "probe_delta_L43": -44.5009765625,
        "probe_delta_L55": -570.6264038085938
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.0525,
        "probe_delta_L31": 4.35589599609375,
        "probe_delta_L43": 183.993408203125,
        "probe_delta_L55": -331.2193603515625
      },
      {
        "prompt_id": "cooking_neg_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.23230478589420656,
        "probe_delta_L31": -163.477294921875,
        "probe_delta_L43": -200.7401123046875,
        "probe_delta_L55": -731.1514892578125
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.020000000000000018,
        "probe_delta_L31": 266.99560546875,
        "probe_delta_L43": 677.49462890625,
        "probe_delta_L55": 1269.78955078125
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.26,
        "probe_delta_L31": 169.25274658203125,
        "probe_delta_L43": 250.8240966796875,
        "probe_delta_L55": 486.0223388671875
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.09999999999999998,
        "probe_delta_L31": 33.88525390625,
        "probe_delta_L43": 409.0400390625,
        "probe_delta_L55": 770.5560302734375
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.0024999999999999467,
        "probe_delta_L31": 40.57421875,
        "probe_delta_L43": 247.589599609375,
        "probe_delta_L55": 410.0087890625
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 26.57891845703125,
        "probe_delta_L43": 89.668212890625,
        "probe_delta_L55": -279.6146240234375
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8975,
        "probe_delta_L31": 188.3304443359375,
        "probe_delta_L43": 1004.7952880859375,
        "probe_delta_L55": 1272.885986328125
      },
      {
        "prompt_id": "cooking_pos_experiential",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3073047858942065,
        "probe_delta_L31": 118.1007080078125,
        "probe_delta_L43": 1001.2054443359375,
        "probe_delta_L55": 1191.244140625
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.03749999999999998,
        "probe_delta_L31": 77.5125732421875,
        "probe_delta_L43": 230.1571044921875,
        "probe_delta_L55": 398.600830078125
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.10500000000000001,
        "probe_delta_L31": 89.74713134765625,
        "probe_delta_L43": 354.58941650390625,
        "probe_delta_L55": 869.443359375
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9375,
        "probe_delta_L31": -27.5592041015625,
        "probe_delta_L43": 593.841796875,
        "probe_delta_L55": 760.7530517578125
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -72.280517578125,
        "probe_delta_L43": 206.6573486328125,
        "probe_delta_L55": 370.965087890625
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 40.22760009765625,
        "probe_delta_L43": 97.1925048828125,
        "probe_delta_L55": -455.8983154296875
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.0525,
        "probe_delta_L31": 26.8267822265625,
        "probe_delta_L43": 593.0567626953125,
        "probe_delta_L55": 554.420654296875
      },
      {
        "prompt_id": "cooking_neg_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6676952141057935,
        "probe_delta_L31": -116.244140625,
        "probe_delta_L43": 88.6092529296875,
        "probe_delta_L55": -145.00634765625
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.0050000000000000044,
        "probe_delta_L31": 171.7646484375,
        "probe_delta_L43": 454.7552490234375,
        "probe_delta_L55": 611.195556640625
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.53,
        "probe_delta_L31": 81.98712158203125,
        "probe_delta_L43": 226.638427734375,
        "probe_delta_L55": 447.802490234375
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": 58.7110595703125,
        "probe_delta_L43": 451.955322265625,
        "probe_delta_L55": 572.1104736328125
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 5.6634521484375,
        "probe_delta_L43": 90.98046875,
        "probe_delta_L55": 220.28271484375
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.007518796992481203,
        "probe_delta_L31": 68.735107421875,
        "probe_delta_L43": 52.0289306640625,
        "probe_delta_L55": -426.378173828125
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.7474999999999999,
        "probe_delta_L31": 132.4112548828125,
        "probe_delta_L43": 804.6942138671875,
        "probe_delta_L55": 1079.4326171875
      },
      {
        "prompt_id": "cooking_pos_value",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.2823047858942065,
        "probe_delta_L31": 118.073486328125,
        "probe_delta_L43": 776.6517333984375,
        "probe_delta_L55": 663.06005859375
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.67,
        "probe_delta_L31": -332.51226806640625,
        "probe_delta_L43": 232.422119140625,
        "probe_delta_L55": 1184.28857421875
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.17293233082706766,
        "probe_delta_L31": 10.48822021484375,
        "probe_delta_L43": 598.43798828125,
        "probe_delta_L55": 1632.653076171875
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -94.73931884765625,
        "probe_delta_L43": 791.5401611328125,
        "probe_delta_L55": 903.079833984375
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -302.84783935546875,
        "probe_delta_L43": 186.9150390625,
        "probe_delta_L55": 450.0107421875
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 72.5028076171875,
        "probe_delta_L43": 275.5087890625,
        "probe_delta_L55": 653.5118408203125
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -298.84381103515625,
        "probe_delta_L43": -39.8380126953125,
        "probe_delta_L55": 790.712646484375
      },
      {
        "prompt_id": "ancient_history_neg_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4375,
        "probe_delta_L31": -218.18109130859375,
        "probe_delta_L43": 530.2740478515625,
        "probe_delta_L55": 1636.4686279296875
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.30499999999999994,
        "probe_delta_L31": 34.156982421875,
        "probe_delta_L43": 426.705810546875,
        "probe_delta_L55": 865.675048828125
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.35206766917293236,
        "probe_delta_L31": 137.98919677734375,
        "probe_delta_L43": 859.1893310546875,
        "probe_delta_L55": 1552.827880859375
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 90.5635986328125,
        "probe_delta_L43": 864.4398193359375,
        "probe_delta_L55": 256.158447265625
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 9.6336669921875,
        "probe_delta_L43": 378.197509765625,
        "probe_delta_L55": 748.448974609375
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.29815521628498726,
        "probe_delta_L31": 99.43017578125,
        "probe_delta_L43": 385.0528564453125,
        "probe_delta_L55": 824.8004150390625
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 122.141357421875,
        "probe_delta_L43": 495.8712158203125,
        "probe_delta_L55": 451.304931640625
      },
      {
        "prompt_id": "ancient_history_pos_persona",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5625,
        "probe_delta_L31": 197.8516845703125,
        "probe_delta_L43": 1384.8614501953125,
        "probe_delta_L55": 2140.74365234375
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.22999999999999998,
        "probe_delta_L31": -87.7408447265625,
        "probe_delta_L43": 123.3919677734375,
        "probe_delta_L55": 23.490966796875
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.02706766917293235,
        "probe_delta_L31": 85.86865234375,
        "probe_delta_L43": 334.4378662109375,
        "probe_delta_L55": 751.6747436523438
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -28.0894775390625,
        "probe_delta_L43": 457.783447265625,
        "probe_delta_L55": 649.977783203125
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.47750000000000004,
        "probe_delta_L31": -109.4691162109375,
        "probe_delta_L43": 115.0159912109375,
        "probe_delta_L55": 704.7939453125
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 120.2083740234375,
        "probe_delta_L43": 396.7493896484375,
        "probe_delta_L55": 924.0318603515625
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.875,
        "probe_delta_L31": -221.61773681640625,
        "probe_delta_L43": 5.699462890625,
        "probe_delta_L55": 204.8082275390625
      },
      {
        "prompt_id": "ancient_history_neg_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.08750000000000002,
        "probe_delta_L31": -53.08868408203125,
        "probe_delta_L43": 609.7938232421875,
        "probe_delta_L55": 1256.77734375
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.17999999999999994,
        "probe_delta_L31": 123.245361328125,
        "probe_delta_L43": 767.683837890625,
        "probe_delta_L55": 1308.4599609375
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.10206766917293236,
        "probe_delta_L31": 127.5374755859375,
        "probe_delta_L43": 339.5806884765625,
        "probe_delta_L55": 978.5202026367188
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 99.33251953125,
        "probe_delta_L43": 403.9632568359375,
        "probe_delta_L55": 637.082275390625
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 34.8648681640625,
        "probe_delta_L43": 415.16650390625,
        "probe_delta_L55": 850.594482421875
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.047864321608040196,
        "probe_delta_L31": 118.211669921875,
        "probe_delta_L43": 364.0125732421875,
        "probe_delta_L55": 735.1783447265625
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 87.3199462890625,
        "probe_delta_L43": 255.4951171875,
        "probe_delta_L55": 559.84423828125
      },
      {
        "prompt_id": "ancient_history_pos_experiential",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "experiential",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5375,
        "probe_delta_L31": 209.293701171875,
        "probe_delta_L43": 1491.0950927734375,
        "probe_delta_L55": 2208.228759765625
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.5700000000000001,
        "probe_delta_L31": -198.3931884765625,
        "probe_delta_L43": -34.943359375,
        "probe_delta_L55": -239.0413818359375
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.13793233082706766,
        "probe_delta_L31": -23.047607421875,
        "probe_delta_L43": 351.5782470703125,
        "probe_delta_L55": 347.34942626953125
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9025,
        "probe_delta_L31": -90.35382080078125,
        "probe_delta_L43": 659.0614013671875,
        "probe_delta_L55": 621.985107421875
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -318.46514892578125,
        "probe_delta_L43": -97.0706787109375,
        "probe_delta_L55": 120.831298828125
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 134.00537109375,
        "probe_delta_L43": 396.4617919921875,
        "probe_delta_L55": 623.9161376953125
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -203.491455078125,
        "probe_delta_L43": 177.6866455078125,
        "probe_delta_L55": 139.5869140625
      },
      {
        "prompt_id": "ancient_history_neg_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4375,
        "probe_delta_L31": -172.8587646484375,
        "probe_delta_L43": 384.3641357421875,
        "probe_delta_L55": 725.397216796875
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.25749999999999995,
        "probe_delta_L31": 58.9447021484375,
        "probe_delta_L43": 697.96142578125,
        "probe_delta_L55": 977.312744140625
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.11706766917293232,
        "probe_delta_L31": 65.93048095703125,
        "probe_delta_L43": 238.05877685546875,
        "probe_delta_L55": 467.15850830078125
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": 40.37744140625,
        "probe_delta_L43": 378.600830078125,
        "probe_delta_L55": 509.372802734375
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 75.33740234375,
        "probe_delta_L43": 131.9698486328125,
        "probe_delta_L55": 618.28515625
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.07274436090225564,
        "probe_delta_L31": 87.539306640625,
        "probe_delta_L43": 273.1451416015625,
        "probe_delta_L55": 684.7293701171875
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.07499999999999996,
        "probe_delta_L31": 42.7982177734375,
        "probe_delta_L43": 223.527587890625,
        "probe_delta_L55": 218.3095703125
      },
      {
        "prompt_id": "ancient_history_pos_value",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "value_laden",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5375,
        "probe_delta_L31": 188.40478515625,
        "probe_delta_L43": 1436.9539794921875,
        "probe_delta_L55": 2109.037353515625
      }
    ]
  },
  "holdout": {
    "behavioral": [
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.825,
        "manipulation_rate": 0.0,
        "delta": -0.825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.4925,
        "manipulation_rate": 0.0,
        "delta": -0.4925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.0,
        "delta": -0.1,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06565656565656566,
        "manipulation_rate": 0.0,
        "delta": -0.06565656565656566,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.7167919799498746,
        "manipulation_rate": 0.0,
        "delta": -0.7167919799498746,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.825,
        "manipulation_rate": 1.0,
        "delta": 0.17500000000000004,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.4925,
        "manipulation_rate": 0.975,
        "delta": 0.4825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.63,
        "delta": 0.53,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06565656565656566,
        "manipulation_rate": 0.95,
        "delta": 0.8843434343434343,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.7167919799498746,
        "manipulation_rate": 1.0,
        "delta": 0.28320802005012535,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.825,
        "manipulation_rate": 0.065,
        "delta": -0.76,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.4925,
        "manipulation_rate": 0.025,
        "delta": -0.46749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.05,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.025,
        "delta": -0.875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.025,
        "delta": -0.07500000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.06565656565656566,
        "manipulation_rate": 0.0,
        "delta": -0.06565656565656566,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.7167919799498746,
        "manipulation_rate": 0.025,
        "delta": -0.6917919799498746,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 1.0,
        "delta": 0.25,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.335,
        "manipulation_rate": 1.0,
        "delta": 0.665,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.97,
        "manipulation_rate": 1.0,
        "delta": 0.030000000000000027,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 1.0,
        "delta": 1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.3025,
        "manipulation_rate": 1.0,
        "delta": 0.6975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.47619047619047616,
        "manipulation_rate": 1.0,
        "delta": 0.5238095238095238,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 0.055,
        "delta": -0.695,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.335,
        "manipulation_rate": 0.025,
        "delta": -0.31,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.97,
        "manipulation_rate": 0.05,
        "delta": -0.9199999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.045,
        "delta": 0.045,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.3025,
        "manipulation_rate": 0.025,
        "delta": -0.27749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.47619047619047616,
        "manipulation_rate": 0.025,
        "delta": -0.45119047619047614,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.75,
        "manipulation_rate": 0.55,
        "delta": -0.19999999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.335,
        "manipulation_rate": 0.325,
        "delta": -0.010000000000000009,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.97,
        "manipulation_rate": 1.0,
        "delta": 0.030000000000000027,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.3025,
        "manipulation_rate": 0.8075,
        "delta": 0.505,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.47619047619047616,
        "manipulation_rate": 0.4,
        "delta": -0.07619047619047614,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.86,
        "manipulation_rate": 0.025,
        "delta": -0.835,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.0,
        "delta": -0.445,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.0,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.0,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.020100502512562814,
        "manipulation_rate": 0.025,
        "delta": 0.004899497487437187,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8621553884711779,
        "manipulation_rate": 0.0,
        "delta": -0.8621553884711779,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.88,
        "manipulation_rate": 0.0,
        "delta": -0.88,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.86,
        "manipulation_rate": 0.975,
        "delta": 0.11499999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.975,
        "delta": 0.53,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.020100502512562814,
        "manipulation_rate": 0.034210526315789476,
        "delta": 0.014110023803226662,
        "baseline_n": 398,
        "manipulation_n": 380,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8621553884711779,
        "manipulation_rate": 1.0,
        "delta": 0.1378446115288221,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.88,
        "manipulation_rate": 1.0,
        "delta": 0.12,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.86,
        "manipulation_rate": 0.125,
        "delta": -0.735,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.445,
        "manipulation_rate": 0.025,
        "delta": -0.42,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.05,
        "delta": -0.9249999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.025,
        "delta": -0.9249999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.020100502512562814,
        "manipulation_rate": 0.025,
        "delta": 0.004899497487437187,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8621553884711779,
        "manipulation_rate": 0.025,
        "delta": -0.8371553884711779,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.88,
        "manipulation_rate": 0.025,
        "delta": -0.855,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 1.0,
        "delta": 0.09499999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 1.0,
        "delta": 0.775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.775,
        "manipulation_rate": 1.0,
        "delta": 0.22499999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.6015228426395939,
        "delta": 0.6015228426395939,
        "baseline_n": 399,
        "manipulation_n": 394,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1525,
        "manipulation_rate": 1.0,
        "delta": 0.8475,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 1.0,
        "delta": 0.09499999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.025,
        "delta": -0.2,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0475,
        "delta": -0.9525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.775,
        "manipulation_rate": 0.025,
        "delta": -0.75,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.020050125313283207,
        "delta": 0.020050125313283207,
        "baseline_n": 399,
        "manipulation_n": 399,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.025,
        "delta": -0.425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1525,
        "manipulation_rate": 0.0,
        "delta": -0.1525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.905,
        "manipulation_rate": 0.8625,
        "delta": -0.04249999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.7475,
        "delta": 0.5225000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.775,
        "manipulation_rate": 1.0,
        "delta": 0.22499999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1525,
        "manipulation_rate": 0.75,
        "delta": 0.5975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9675,
        "manipulation_rate": 0.0,
        "delta": -0.9675,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.0,
        "delta": -0.2075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 0.0,
        "delta": -0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6455696202531646,
        "manipulation_rate": 0.0,
        "delta": -0.6455696202531646,
        "baseline_n": 395,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9675,
        "manipulation_rate": 1.0,
        "delta": 0.03249999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.95,
        "delta": 0.7424999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 1.0,
        "delta": 0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6455696202531646,
        "manipulation_rate": 1.0,
        "delta": 0.35443037974683544,
        "baseline_n": 395,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9675,
        "manipulation_rate": 0.1,
        "delta": -0.8675,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.025,
        "delta": -0.1825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.655,
        "delta": -0.345,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.175,
        "delta": -0.825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.04,
        "delta": 0.015,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.5,
        "manipulation_rate": 0.0,
        "delta": -0.5,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6455696202531646,
        "manipulation_rate": 0.025,
        "delta": -0.6205696202531645,
        "baseline_n": 395,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 1.0,
        "delta": 0.0024999999999999467,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 1.0,
        "delta": 0.85,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.4575,
        "delta": 0.4575,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8295739348370927,
        "manipulation_rate": 1.0,
        "delta": 0.17042606516290726,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.605,
        "manipulation_rate": 1.0,
        "delta": 0.395,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 0.0,
        "delta": -0.9975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.0,
        "delta": -0.15,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0225,
        "delta": -0.9775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.45,
        "delta": 0.45,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8295739348370927,
        "manipulation_rate": 0.0,
        "delta": -0.8295739348370927,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.605,
        "manipulation_rate": 0.0,
        "delta": -0.605,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9975,
        "manipulation_rate": 1.0,
        "delta": 0.0024999999999999467,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.475,
        "delta": 0.32499999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8295739348370927,
        "manipulation_rate": 0.925,
        "delta": 0.0954260651629073,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.605,
        "manipulation_rate": 0.955,
        "delta": 0.35,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9775,
        "manipulation_rate": 0.025,
        "delta": -0.9525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2575,
        "manipulation_rate": 0.025,
        "delta": -0.2325,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.025,
        "delta": -0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0925,
        "manipulation_rate": 0.0,
        "delta": -0.0925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6934673366834171,
        "manipulation_rate": 0.0,
        "delta": -0.6934673366834171,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9775,
        "manipulation_rate": 1.0,
        "delta": 0.022499999999999964,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2575,
        "manipulation_rate": 0.83,
        "delta": 0.5725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.975,
        "delta": -0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0925,
        "manipulation_rate": 1.0,
        "delta": 0.9075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6934673366834171,
        "manipulation_rate": 1.0,
        "delta": 0.3065326633165829,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9775,
        "manipulation_rate": 0.635,
        "delta": -0.3425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2575,
        "manipulation_rate": 0.9675,
        "delta": 0.71,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.96,
        "delta": -0.040000000000000036,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.0925,
        "manipulation_rate": 0.025,
        "delta": -0.0675,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6934673366834171,
        "manipulation_rate": 0.25,
        "delta": -0.4434673366834171,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.6675,
        "manipulation_rate": 1.0,
        "delta": 0.3325,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.175,
        "manipulation_rate": 1.0,
        "delta": 0.825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 1.0,
        "delta": 0.975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4425,
        "manipulation_rate": 1.0,
        "delta": 0.5575,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.6675,
        "manipulation_rate": 0.0,
        "delta": -0.6675,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.175,
        "manipulation_rate": 0.0,
        "delta": -0.175,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.075,
        "delta": -0.925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0,
        "delta": -0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 0.0,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4425,
        "manipulation_rate": 0.0,
        "delta": -0.4425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.6675,
        "manipulation_rate": 0.975,
        "delta": 0.3075,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.175,
        "manipulation_rate": 0.3725,
        "delta": 0.1975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0625,
        "delta": 0.0375,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4425,
        "manipulation_rate": 0.9275,
        "delta": 0.485,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      }
    ],
    "probe_behavioral": [
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.825,
        "probe_delta_L31": -308.82037353515625,
        "probe_delta_L43": -430.54193115234375,
        "probe_delta_L55": -702.2392578125
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.4925,
        "probe_delta_L31": -252.78997802734375,
        "probe_delta_L43": -388.044677734375,
        "probe_delta_L55": -1813.1630859375
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -167.28851318359375,
        "probe_delta_L43": -99.35009765625,
        "probe_delta_L55": -44.0616455078125
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -537.094482421875,
        "probe_delta_L43": -948.8903198242188,
        "probe_delta_L55": -1252.435546875
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.1,
        "probe_delta_L31": -299.5184326171875,
        "probe_delta_L43": -730.2793579101562,
        "probe_delta_L55": -1236.380859375
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.06565656565656566,
        "probe_delta_L31": -249.09405517578125,
        "probe_delta_L43": -106.24066162109375,
        "probe_delta_L55": 11.760421752929688
      },
      {
        "prompt_id": "holdout_cheese_neg_instruction",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.7167919799498746,
        "probe_delta_L31": -506.2784423828125,
        "probe_delta_L43": -1024.217529296875,
        "probe_delta_L55": -1318.9794921875
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.17500000000000004,
        "probe_delta_L31": 219.7276611328125,
        "probe_delta_L43": 546.490966796875,
        "probe_delta_L55": -126.04052734375
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.4825,
        "probe_delta_L31": 139.60235595703125,
        "probe_delta_L43": 555.0385131835938,
        "probe_delta_L55": 1166.607177734375
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 92.3038330078125,
        "probe_delta_L43": 545.8006591796875,
        "probe_delta_L55": 793.3927001953125
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 41.3153076171875,
        "probe_delta_L43": 74.9222412109375,
        "probe_delta_L55": 188.01904296875
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.53,
        "probe_delta_L31": 84.2020263671875,
        "probe_delta_L43": 212.6435546875,
        "probe_delta_L55": 681.512451171875
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8843434343434343,
        "probe_delta_L31": 262.2974853515625,
        "probe_delta_L43": 895.4695434570312,
        "probe_delta_L55": 1752.5921630859375
      },
      {
        "prompt_id": "holdout_cheese_pos_identity",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.28320802005012535,
        "probe_delta_L31": 142.00048828125,
        "probe_delta_L43": 557.90185546875,
        "probe_delta_L55": 216.8935546875
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.76,
        "probe_delta_L31": 80.2935791015625,
        "probe_delta_L43": 175.4488525390625,
        "probe_delta_L55": -136.1533203125
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.46749999999999997,
        "probe_delta_L31": -120.2725830078125,
        "probe_delta_L43": -16.94769287109375,
        "probe_delta_L55": 462.114013671875
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -107.697998046875,
        "probe_delta_L43": 169.393310546875,
        "probe_delta_L55": 147.9444580078125
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.875,
        "probe_delta_L31": -258.35546875,
        "probe_delta_L43": -429.057373046875,
        "probe_delta_L55": -459.9715576171875
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.07500000000000001,
        "probe_delta_L31": -3.0308837890625,
        "probe_delta_L43": 147.811279296875,
        "probe_delta_L55": 528.384033203125
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.06565656565656566,
        "probe_delta_L31": -210.5745849609375,
        "probe_delta_L43": 319.46868896484375,
        "probe_delta_L55": 578.35986328125
      },
      {
        "prompt_id": "holdout_cheese_neg_casual",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6917919799498746,
        "probe_delta_L31": -30.322998046875,
        "probe_delta_L43": -214.2325439453125,
        "probe_delta_L55": -445.1070556640625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.25,
        "probe_delta_L31": 52.5633544921875,
        "probe_delta_L43": 480.10986328125,
        "probe_delta_L55": 596.603515625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.665,
        "probe_delta_L31": 178.0198974609375,
        "probe_delta_L43": 683.0914916992188,
        "probe_delta_L55": 2036.209228515625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.030000000000000027,
        "probe_delta_L31": 144.2061767578125,
        "probe_delta_L43": 465.2674560546875,
        "probe_delta_L55": 882.14453125
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 21.989501953125,
        "probe_delta_L43": 64.9638671875,
        "probe_delta_L55": 597.461669921875
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 1.0,
        "probe_delta_L31": 43.056640625,
        "probe_delta_L43": -21.180419921875,
        "probe_delta_L55": 265.05615234375
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.6975,
        "probe_delta_L31": 138.320556640625,
        "probe_delta_L43": 605.4974365234375,
        "probe_delta_L55": 744.31494140625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_instruction",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5238095238095238,
        "probe_delta_L31": -1.0006103515625,
        "probe_delta_L43": 703.33447265625,
        "probe_delta_L55": 742.076416015625
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.695,
        "probe_delta_L31": 124.5181884765625,
        "probe_delta_L43": 557.0999755859375,
        "probe_delta_L55": 673.6748046875
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.31,
        "probe_delta_L31": 229.73272705078125,
        "probe_delta_L43": 754.4940795898438,
        "probe_delta_L55": 1951.9423828125
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9199999999999999,
        "probe_delta_L31": 225.1197509765625,
        "probe_delta_L43": 997.3199462890625,
        "probe_delta_L55": 1455.901611328125
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.975,
        "probe_delta_L31": 102.51953125,
        "probe_delta_L43": -14.2816162109375,
        "probe_delta_L55": 441.837646484375
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.045,
        "probe_delta_L31": 234.755615234375,
        "probe_delta_L43": 588.845947265625,
        "probe_delta_L55": 1525.6680908203125
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.27749999999999997,
        "probe_delta_L31": 186.8831787109375,
        "probe_delta_L43": 689.177978515625,
        "probe_delta_L55": 480.1417236328125
      },
      {
        "prompt_id": "holdout_rainy_weather_neg_identity",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.45119047619047614,
        "probe_delta_L31": -44.282470703125,
        "probe_delta_L43": 376.3470458984375,
        "probe_delta_L55": 497.043212890625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.19999999999999996,
        "probe_delta_L31": 44.751220703125,
        "probe_delta_L43": 442.287353515625,
        "probe_delta_L55": -267.575927734375
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.010000000000000009,
        "probe_delta_L31": 102.41400146484375,
        "probe_delta_L43": 212.298583984375,
        "probe_delta_L55": 919.9895629882812
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.030000000000000027,
        "probe_delta_L31": 40.6322021484375,
        "probe_delta_L43": 162.9232177734375,
        "probe_delta_L55": 421.8577880859375
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 17.93408203125,
        "probe_delta_L43": 175.258056640625,
        "probe_delta_L55": 365.496337890625
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 114.9085693359375,
        "probe_delta_L43": 298.2366943359375,
        "probe_delta_L55": 600.1781005859375
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.505,
        "probe_delta_L31": 13.3878173828125,
        "probe_delta_L43": 72.5006103515625,
        "probe_delta_L55": -254.1611328125
      },
      {
        "prompt_id": "holdout_rainy_weather_pos_casual",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.07619047619047614,
        "probe_delta_L31": 119.6812744140625,
        "probe_delta_L43": 420.498291015625,
        "probe_delta_L55": -8.4798583984375
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.835,
        "probe_delta_L31": -119.0069580078125,
        "probe_delta_L43": 310.952392578125,
        "probe_delta_L55": 131.3734130859375
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.445,
        "probe_delta_L31": -106.7596435546875,
        "probe_delta_L43": -279.64263916015625,
        "probe_delta_L55": -1279.4210205078125
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -154.527587890625,
        "probe_delta_L43": -458.568603515625,
        "probe_delta_L55": -642.101318359375
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -529.1343994140625,
        "probe_delta_L43": -873.942138671875,
        "probe_delta_L55": -1194.36572265625
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.004899497487437187,
        "probe_delta_L31": -178.87432861328125,
        "probe_delta_L43": -234.0521240234375,
        "probe_delta_L55": -903.7584228515625
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8621553884711779,
        "probe_delta_L31": -323.4490966796875,
        "probe_delta_L43": -129.46484375,
        "probe_delta_L55": -715.5371704101562
      },
      {
        "prompt_id": "holdout_cats_neg_instruction",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.88,
        "probe_delta_L31": -528.717041015625,
        "probe_delta_L43": -703.793212890625,
        "probe_delta_L55": -1607.13232421875
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.11499999999999999,
        "probe_delta_L31": 341.028076171875,
        "probe_delta_L43": 1145.5665283203125,
        "probe_delta_L55": 1020.596435546875
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.53,
        "probe_delta_L31": 277.0897216796875,
        "probe_delta_L43": 766.6962280273438,
        "probe_delta_L55": 1795.7508544921875
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 239.1156005859375,
        "probe_delta_L43": 980.39404296875,
        "probe_delta_L55": 1374.27099609375
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 103.6485595703125,
        "probe_delta_L43": 278.883056640625,
        "probe_delta_L55": 611.820068359375
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.014110023803226662,
        "probe_delta_L31": 166.8363037109375,
        "probe_delta_L43": 467.8311767578125,
        "probe_delta_L55": 656.0701904296875
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.1378446115288221,
        "probe_delta_L31": 165.5205078125,
        "probe_delta_L43": 919.6529541015625,
        "probe_delta_L55": 1498.392822265625
      },
      {
        "prompt_id": "holdout_cats_pos_identity",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.12,
        "probe_delta_L31": 175.0677490234375,
        "probe_delta_L43": 946.60888671875,
        "probe_delta_L55": 1058.8402099609375
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.735,
        "probe_delta_L31": 70.8199462890625,
        "probe_delta_L43": 336.2232666015625,
        "probe_delta_L55": 3.5316162109375
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.42,
        "probe_delta_L31": -57.68817138671875,
        "probe_delta_L43": -47.38690185546875,
        "probe_delta_L55": 374.6717529296875
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9249999999999999,
        "probe_delta_L31": -5.2269287109375,
        "probe_delta_L43": 58.2484130859375,
        "probe_delta_L55": 179.1009521484375
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9249999999999999,
        "probe_delta_L31": -199.436279296875,
        "probe_delta_L43": -299.96484375,
        "probe_delta_L55": -194.254638671875
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.004899497487437187,
        "probe_delta_L31": 95.34912109375,
        "probe_delta_L43": 410.2294921875,
        "probe_delta_L55": 334.294189453125
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8371553884711779,
        "probe_delta_L31": -253.04266357421875,
        "probe_delta_L43": 238.5450439453125,
        "probe_delta_L55": 387.542724609375
      },
      {
        "prompt_id": "holdout_cats_neg_casual",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.855,
        "probe_delta_L31": -221.39617919921875,
        "probe_delta_L43": -317.966552734375,
        "probe_delta_L55": -1023.3479614257812
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.09499999999999997,
        "probe_delta_L31": -194.39605712890625,
        "probe_delta_L43": -384.0546875,
        "probe_delta_L55": -1583.5001220703125
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.775,
        "probe_delta_L31": 162.82220458984375,
        "probe_delta_L43": 183.0321044921875,
        "probe_delta_L55": 524.130615234375
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 40.58740234375,
        "probe_delta_L43": 236.7108154296875,
        "probe_delta_L55": 444.21875
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.22499999999999998,
        "probe_delta_L31": 29.7034912109375,
        "probe_delta_L43": -57.0299072265625,
        "probe_delta_L55": -280.132568359375
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.6015228426395939,
        "probe_delta_L31": 38.19305419921875,
        "probe_delta_L43": -304.68731689453125,
        "probe_delta_L55": -1420.7640380859375
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 61.5419921875,
        "probe_delta_L43": 38.61865234375,
        "probe_delta_L55": 81.2353515625
      },
      {
        "prompt_id": "holdout_classical_music_pos_instruction",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8475,
        "probe_delta_L31": 137.31671142578125,
        "probe_delta_L43": 483.6416015625,
        "probe_delta_L55": -41.54925537109375
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.09499999999999997,
        "probe_delta_L31": 61.08087158203125,
        "probe_delta_L43": 245.8223876953125,
        "probe_delta_L55": 604.252685546875
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2,
        "probe_delta_L31": 137.57080078125,
        "probe_delta_L43": 467.438232421875,
        "probe_delta_L55": 1646.1168212890625
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9525,
        "probe_delta_L31": 159.9471435546875,
        "probe_delta_L43": 812.72802734375,
        "probe_delta_L55": 1098.03271484375
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.75,
        "probe_delta_L31": 37.79443359375,
        "probe_delta_L43": 3.8739013671875,
        "probe_delta_L55": 335.93115234375
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.020050125313283207,
        "probe_delta_L31": -34.15045166015625,
        "probe_delta_L43": 142.5244140625,
        "probe_delta_L55": -485.2177734375
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.425,
        "probe_delta_L31": 68.837890625,
        "probe_delta_L43": 81.49560546875,
        "probe_delta_L55": 586.659423828125
      },
      {
        "prompt_id": "holdout_classical_music_neg_identity",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.1525,
        "probe_delta_L31": 152.916259765625,
        "probe_delta_L43": 720.4105224609375,
        "probe_delta_L55": 1430.376953125
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.04249999999999998,
        "probe_delta_L31": 22.95648193359375,
        "probe_delta_L43": 416.8751220703125,
        "probe_delta_L55": -130.5162353515625
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5225000000000001,
        "probe_delta_L31": 101.1378173828125,
        "probe_delta_L43": 140.027099609375,
        "probe_delta_L55": 404.6888427734375
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 30.6676025390625,
        "probe_delta_L43": 91.649169921875,
        "probe_delta_L55": 347.1531982421875
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.22499999999999998,
        "probe_delta_L31": 54.2572021484375,
        "probe_delta_L43": 8.351806640625,
        "probe_delta_L55": 228.2978515625
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -34.720947265625,
        "probe_delta_L43": -22.9501953125,
        "probe_delta_L55": -480.110595703125
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 70.84228515625,
        "probe_delta_L43": 75.445068359375,
        "probe_delta_L55": 471.21826171875
      },
      {
        "prompt_id": "holdout_classical_music_pos_casual",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5975,
        "probe_delta_L31": 162.7996826171875,
        "probe_delta_L43": 661.7435302734375,
        "probe_delta_L55": 481.69940185546875
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.9675,
        "probe_delta_L31": -296.45574951171875,
        "probe_delta_L43": -423.89794921875,
        "probe_delta_L55": -990.9510498046875
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2075,
        "probe_delta_L31": -82.348876953125,
        "probe_delta_L43": 259.8852844238281,
        "probe_delta_L55": -613.7592163085938
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -200.1168212890625,
        "probe_delta_L43": -4.060302734375,
        "probe_delta_L55": 172.7802734375
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -254.3199462890625,
        "probe_delta_L43": -124.7850341796875,
        "probe_delta_L55": -185.546630859375
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": -71.5279541015625,
        "probe_delta_L43": -390.91619873046875,
        "probe_delta_L55": -521.85693359375
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5,
        "probe_delta_L31": -283.45703125,
        "probe_delta_L43": -159.46197509765625,
        "probe_delta_L55": -1333.2315673828125
      },
      {
        "prompt_id": "holdout_gardening_neg_instruction",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6455696202531646,
        "probe_delta_L31": -252.2135009765625,
        "probe_delta_L43": -217.733154296875,
        "probe_delta_L55": -1240.02783203125
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.03249999999999997,
        "probe_delta_L31": 356.1751708984375,
        "probe_delta_L43": 705.75439453125,
        "probe_delta_L55": 246.53125
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.7424999999999999,
        "probe_delta_L31": 304.606201171875,
        "probe_delta_L43": 1125.620849609375,
        "probe_delta_L55": 2126.314453125
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 172.513916015625,
        "probe_delta_L43": 719.5335693359375,
        "probe_delta_L55": 1050.421875
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 99.610107421875,
        "probe_delta_L43": 255.063720703125,
        "probe_delta_L55": 480.7177734375
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 91.83154296875,
        "probe_delta_L43": 294.0457763671875,
        "probe_delta_L55": -345.2462158203125
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5,
        "probe_delta_L31": 330.1419677734375,
        "probe_delta_L43": 1184.848388671875,
        "probe_delta_L55": 978.287109375
      },
      {
        "prompt_id": "holdout_gardening_pos_identity",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.35443037974683544,
        "probe_delta_L31": 285.451904296875,
        "probe_delta_L43": 1109.395263671875,
        "probe_delta_L55": 645.649169921875
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.8675,
        "probe_delta_L31": 60.3267822265625,
        "probe_delta_L43": -397.73779296875,
        "probe_delta_L55": -1190.804443359375
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.1825,
        "probe_delta_L31": 94.2247314453125,
        "probe_delta_L43": 444.0237731933594,
        "probe_delta_L55": 931.3667602539062
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.345,
        "probe_delta_L31": -45.37255859375,
        "probe_delta_L43": 247.768310546875,
        "probe_delta_L55": 692.14208984375
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.825,
        "probe_delta_L31": -194.69775390625,
        "probe_delta_L43": -428.766357421875,
        "probe_delta_L55": -346.46630859375
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.015,
        "probe_delta_L31": 28.5330810546875,
        "probe_delta_L43": 24.629638671875,
        "probe_delta_L55": -166.6689453125
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5,
        "probe_delta_L31": -43.76080322265625,
        "probe_delta_L43": 259.69635009765625,
        "probe_delta_L55": -256.5963134765625
      },
      {
        "prompt_id": "holdout_gardening_neg_casual",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6205696202531645,
        "probe_delta_L31": -124.2147216796875,
        "probe_delta_L43": -253.3897705078125,
        "probe_delta_L55": -1145.5284423828125
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.0024999999999999467,
        "probe_delta_L31": -37.3056640625,
        "probe_delta_L43": 214.38916015625,
        "probe_delta_L55": 727.843505859375
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.85,
        "probe_delta_L31": 146.28167724609375,
        "probe_delta_L43": 862.8605346679688,
        "probe_delta_L55": 1746.9183349609375
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 111.15576171875,
        "probe_delta_L43": 439.6119384765625,
        "probe_delta_L55": 599.2803955078125
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -67.739990234375,
        "probe_delta_L43": 255.502685546875,
        "probe_delta_L55": -80.484130859375
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.4575,
        "probe_delta_L31": 10.0169677734375,
        "probe_delta_L43": -97.53076171875,
        "probe_delta_L55": -292.8563232421875
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.17042606516290726,
        "probe_delta_L31": 70.0128173828125,
        "probe_delta_L43": 569.4417724609375,
        "probe_delta_L55": 548.04638671875
      },
      {
        "prompt_id": "holdout_astronomy_pos_instruction",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.395,
        "probe_delta_L31": 92.5003662109375,
        "probe_delta_L43": 797.7904052734375,
        "probe_delta_L55": 860.584228515625
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.9975,
        "probe_delta_L31": -235.54248046875,
        "probe_delta_L43": -318.28076171875,
        "probe_delta_L55": 170.36083984375
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.15,
        "probe_delta_L31": 59.06414794921875,
        "probe_delta_L43": 772.1504516601562,
        "probe_delta_L55": 1796.4720458984375
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -50.156005859375,
        "probe_delta_L43": 480.41552734375,
        "probe_delta_L55": 1229.3670654296875
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.9775,
        "probe_delta_L31": -262.9044189453125,
        "probe_delta_L43": -214.752685546875,
        "probe_delta_L55": 378.24169921875
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.45,
        "probe_delta_L31": 142.310546875,
        "probe_delta_L43": 115.57421875,
        "probe_delta_L55": 792.701171875
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.8295739348370927,
        "probe_delta_L31": -273.6300048828125,
        "probe_delta_L43": 77.6585693359375,
        "probe_delta_L55": 344.0556640625
      },
      {
        "prompt_id": "holdout_astronomy_neg_identity",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.605,
        "probe_delta_L31": -179.124755859375,
        "probe_delta_L43": 294.9112548828125,
        "probe_delta_L55": 669.619140625
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.0024999999999999467,
        "probe_delta_L31": -34.8885498046875,
        "probe_delta_L43": -133.525390625,
        "probe_delta_L55": -431.6982421875
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.32499999999999996,
        "probe_delta_L31": 54.03900146484375,
        "probe_delta_L43": 171.8680419921875,
        "probe_delta_L55": 620.1116943359375
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 13.61767578125,
        "probe_delta_L43": 179.3250732421875,
        "probe_delta_L55": 369.5028076171875
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -11.5760498046875,
        "probe_delta_L43": 11.977783203125,
        "probe_delta_L55": 395.7177734375
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 3.8951416015625,
        "probe_delta_L43": 41.4991455078125,
        "probe_delta_L55": -39.06982421875
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.0954260651629073,
        "probe_delta_L31": 72.0714111328125,
        "probe_delta_L43": 54.7503662109375,
        "probe_delta_L55": 175.6943359375
      },
      {
        "prompt_id": "holdout_astronomy_pos_casual",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.35,
        "probe_delta_L31": 208.34765625,
        "probe_delta_L43": 695.3192138671875,
        "probe_delta_L55": 398.93359375
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.9525,
        "probe_delta_L31": -344.7047119140625,
        "probe_delta_L43": -441.340087890625,
        "probe_delta_L55": -365.80615234375
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2325,
        "probe_delta_L31": 3.8701171875,
        "probe_delta_L43": 34.46490478515625,
        "probe_delta_L55": -98.47198486328125
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.975,
        "probe_delta_L31": -127.212646484375,
        "probe_delta_L43": 96.32421875,
        "probe_delta_L55": 363.5791015625
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -206.091552734375,
        "probe_delta_L43": -513.5325927734375,
        "probe_delta_L55": -1012.805908203125
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": -113.66314697265625,
        "probe_delta_L43": -169.654541015625,
        "probe_delta_L55": -681.5025634765625
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.0925,
        "probe_delta_L31": -162.791748046875,
        "probe_delta_L43": 239.078369140625,
        "probe_delta_L55": -71.6015625
      },
      {
        "prompt_id": "holdout_cooking_neg_instruction",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6934673366834171,
        "probe_delta_L31": -427.19158935546875,
        "probe_delta_L43": -340.2379150390625,
        "probe_delta_L55": -855.3092041015625
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.022499999999999964,
        "probe_delta_L31": 211.044921875,
        "probe_delta_L43": 576.650634765625,
        "probe_delta_L55": 693.911376953125
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.5725,
        "probe_delta_L31": 191.1094970703125,
        "probe_delta_L43": 590.7383422851562,
        "probe_delta_L55": 1005.3614501953125
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.025000000000000022,
        "probe_delta_L31": 132.2523193359375,
        "probe_delta_L43": 771.78369140625,
        "probe_delta_L55": 540.8453369140625
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 61.0645751953125,
        "probe_delta_L43": 289.78515625,
        "probe_delta_L55": 479.859130859375
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 70.14215087890625,
        "probe_delta_L43": 152.603759765625,
        "probe_delta_L55": -551.0410766601562
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.9075,
        "probe_delta_L31": 216.7110595703125,
        "probe_delta_L43": 1125.7579345703125,
        "probe_delta_L55": 1572.270263671875
      },
      {
        "prompt_id": "holdout_cooking_pos_identity",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3065326633165829,
        "probe_delta_L31": 152.8450927734375,
        "probe_delta_L43": 1014.0208740234375,
        "probe_delta_L55": 968.96923828125
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.3425,
        "probe_delta_L31": 79.498291015625,
        "probe_delta_L43": -256.599365234375,
        "probe_delta_L55": -755.9813232421875
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.71,
        "probe_delta_L31": 91.84857177734375,
        "probe_delta_L43": 23.00396728515625,
        "probe_delta_L55": 249.537353515625
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.040000000000000036,
        "probe_delta_L31": -62.187255859375,
        "probe_delta_L43": 377.9796142578125,
        "probe_delta_L55": 473.8851318359375
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -29.303955078125,
        "probe_delta_L43": 95.46923828125,
        "probe_delta_L55": 176.640625
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 15.6295166015625,
        "probe_delta_L43": -62.3494873046875,
        "probe_delta_L55": -403.8310546875
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.0675,
        "probe_delta_L31": 34.61248779296875,
        "probe_delta_L43": -119.0213623046875,
        "probe_delta_L55": -838.7947998046875
      },
      {
        "prompt_id": "holdout_cooking_neg_casual",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4434673366834171,
        "probe_delta_L31": -49.4046630859375,
        "probe_delta_L43": -96.696533203125,
        "probe_delta_L55": -809.31591796875
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.3325,
        "probe_delta_L31": -26.4364013671875,
        "probe_delta_L43": 310.3204345703125,
        "probe_delta_L55": 772.51318359375
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.825,
        "probe_delta_L31": 136.85723876953125,
        "probe_delta_L43": 475.2547607421875,
        "probe_delta_L55": 710.1416625976562
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 71.1756591796875,
        "probe_delta_L43": 491.3265380859375,
        "probe_delta_L55": 131.7666015625
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -29.2646484375,
        "probe_delta_L43": 206.8929443359375,
        "probe_delta_L55": 839.540283203125
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.975,
        "probe_delta_L31": 44.9447021484375,
        "probe_delta_L43": 187.8427734375,
        "probe_delta_L55": 146.62109375
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 21.634765625,
        "probe_delta_L43": 214.047119140625,
        "probe_delta_L55": -429.16259765625
      },
      {
        "prompt_id": "holdout_ancient_history_pos_instruction",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "instruction",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5575,
        "probe_delta_L31": 70.3446044921875,
        "probe_delta_L43": 1094.3873291015625,
        "probe_delta_L55": 1586.76220703125
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.6675,
        "probe_delta_L31": -75.05810546875,
        "probe_delta_L43": 370.616455078125,
        "probe_delta_L55": 1378.087158203125
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.175,
        "probe_delta_L31": 50.85882568359375,
        "probe_delta_L43": 604.732666015625,
        "probe_delta_L55": 1232.4619140625
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.925,
        "probe_delta_L31": 91.668212890625,
        "probe_delta_L43": 1015.2100830078125,
        "probe_delta_L55": 887.976318359375
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -71.293212890625,
        "probe_delta_L43": 263.761962890625,
        "probe_delta_L55": 764.317138671875
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.025,
        "probe_delta_L31": 94.546142578125,
        "probe_delta_L43": 504.7408447265625,
        "probe_delta_L55": 604.8760986328125
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -145.525146484375,
        "probe_delta_L43": 104.9578857421875,
        "probe_delta_L55": 447.30615234375
      },
      {
        "prompt_id": "holdout_ancient_history_neg_identity",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "identity",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4425,
        "probe_delta_L31": -120.0362548828125,
        "probe_delta_L43": 505.4346923828125,
        "probe_delta_L55": 1388.9559326171875
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.3075,
        "probe_delta_L31": -19.0003662109375,
        "probe_delta_L43": 212.5751953125,
        "probe_delta_L55": -88.62060546875
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.1975,
        "probe_delta_L31": 56.08624267578125,
        "probe_delta_L43": 88.5513916015625,
        "probe_delta_L55": 335.51544189453125
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 18.73583984375,
        "probe_delta_L43": 230.0072021484375,
        "probe_delta_L55": 554.8759765625
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 36.53564453125,
        "probe_delta_L43": 166.918701171875,
        "probe_delta_L55": 457.340576171875
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0375,
        "probe_delta_L31": 87.5140380859375,
        "probe_delta_L43": 236.19482421875,
        "probe_delta_L55": 412.8214111328125
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 23.214599609375,
        "probe_delta_L43": 135.6072998046875,
        "probe_delta_L55": 226.1083984375
      },
      {
        "prompt_id": "holdout_ancient_history_pos_casual",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "casual",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.485,
        "probe_delta_L31": 150.7608642578125,
        "probe_delta_L43": 1075.8106689453125,
        "probe_delta_L55": 987.6561279296875
      }
    ]
  },
  "subtle": {
    "behavioral": [
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.825,
        "manipulation_rate": 1.0,
        "delta": 0.17500000000000004,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.495,
        "manipulation_rate": 1.0,
        "delta": 0.505,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9025,
        "manipulation_rate": 1.0,
        "delta": 0.09750000000000003,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.1,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.07035175879396985,
        "manipulation_rate": 1.0,
        "delta": 0.9296482412060302,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.7075,
        "manipulation_rate": 1.0,
        "delta": 0.2925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.825,
        "manipulation_rate": 0.05,
        "delta": -0.7749999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.495,
        "manipulation_rate": 0.025,
        "delta": -0.47,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.3,
        "delta": -0.7,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.9025,
        "manipulation_rate": 0.025,
        "delta": -0.8775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.1,
        "manipulation_rate": 0.025,
        "delta": -0.07500000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.07035175879396985,
        "manipulation_rate": 0.0,
        "delta": -0.07035175879396985,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.7075,
        "manipulation_rate": 0.025,
        "delta": -0.6825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.7575,
        "manipulation_rate": 1.0,
        "delta": 0.24250000000000005,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.335,
        "manipulation_rate": 1.0,
        "delta": 0.665,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9725,
        "manipulation_rate": 1.0,
        "delta": 0.02749999999999997,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.4275,
        "delta": 0.4275,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.29,
        "manipulation_rate": 1.0,
        "delta": 0.71,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 1.0,
        "delta": 0.5201005025125628,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.7575,
        "manipulation_rate": 0.0,
        "delta": -0.7575,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.335,
        "manipulation_rate": 0.025,
        "delta": -0.31,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.9725,
        "manipulation_rate": 0.0025,
        "delta": -0.9700000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.29,
        "manipulation_rate": 0.0,
        "delta": -0.29,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4798994974874372,
        "manipulation_rate": 0.0,
        "delta": -0.4798994974874372,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.87,
        "manipulation_rate": 1.0,
        "delta": 0.13,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 1.0,
        "delta": 0.025000000000000022,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.022556390977443608,
        "manipulation_rate": 0.05,
        "delta": 0.027443609022556395,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8671679197994987,
        "manipulation_rate": 1.0,
        "delta": 0.1328320802005013,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8847117794486216,
        "manipulation_rate": 1.0,
        "delta": 0.11528822055137844,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.87,
        "manipulation_rate": 0.5775,
        "delta": -0.2925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.95,
        "delta": 0.49999999999999994,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 0.975,
        "manipulation_rate": 0.975,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.925,
        "delta": -0.02499999999999991,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.022556390977443608,
        "manipulation_rate": 0.025,
        "delta": 0.0024436090225563936,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8671679197994987,
        "manipulation_rate": 1.0,
        "delta": 0.1328320802005013,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8847117794486216,
        "manipulation_rate": 0.875,
        "delta": -0.00971177944862156,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9025,
        "manipulation_rate": 0.825,
        "delta": -0.07750000000000001,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.975,
        "delta": 0.75,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.7775,
        "manipulation_rate": 1.0,
        "delta": 0.22250000000000003,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.04639175257731959,
        "delta": 0.04639175257731959,
        "baseline_n": 400,
        "manipulation_n": 388,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 1.0,
        "delta": 0.55,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1525,
        "manipulation_rate": 1.0,
        "delta": 0.8475,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9025,
        "manipulation_rate": 0.275,
        "delta": -0.6275,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.225,
        "manipulation_rate": 0.025,
        "delta": -0.2,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.195,
        "delta": -0.8049999999999999,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 0.7775,
        "manipulation_rate": 0.025,
        "delta": -0.7525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.45,
        "manipulation_rate": 0.025,
        "delta": -0.425,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.1525,
        "manipulation_rate": 0.025,
        "delta": -0.1275,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.97,
        "manipulation_rate": 1.0,
        "delta": 0.030000000000000027,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 1.0,
        "delta": 0.7925,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4949748743718593,
        "manipulation_rate": 1.0,
        "delta": 0.5050251256281407,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6464646464646465,
        "manipulation_rate": 1.0,
        "delta": 0.3535353535353535,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.97,
        "manipulation_rate": 0.3975,
        "delta": -0.5725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.2075,
        "manipulation_rate": 0.025,
        "delta": -0.1825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.9,
        "delta": -0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.25,
        "delta": -0.75,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.0275,
        "delta": 0.0024999999999999988,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4949748743718593,
        "manipulation_rate": 0.1,
        "delta": -0.3949748743718593,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6464646464646465,
        "manipulation_rate": 0.1125,
        "delta": -0.5339646464646465,
        "baseline_n": 396,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.995,
        "manipulation_rate": 1.0,
        "delta": 0.0050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.73,
        "delta": 0.58,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8275,
        "manipulation_rate": 1.0,
        "delta": 0.1725,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6090225563909775,
        "manipulation_rate": 1.0,
        "delta": 0.39097744360902253,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.995,
        "manipulation_rate": 0.0,
        "delta": -0.995,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.0,
        "delta": -0.15,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.0,
        "delta": -1.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.315,
        "delta": -0.685,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.8275,
        "manipulation_rate": 0.05,
        "delta": -0.7775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6090225563909775,
        "manipulation_rate": 0.0,
        "delta": -0.6090225563909775,
        "baseline_n": 399,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9775,
        "manipulation_rate": 1.0,
        "delta": 0.022499999999999964,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 1.0,
        "delta": 0.745,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.025,
        "delta": 0.025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.085,
        "manipulation_rate": 1.0,
        "delta": 0.915,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6909547738693468,
        "manipulation_rate": 1.0,
        "delta": 0.3090452261306532,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.9775,
        "manipulation_rate": 0.025,
        "delta": -0.9525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.255,
        "manipulation_rate": 0.075,
        "delta": -0.18,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.1,
        "delta": -0.9,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 0.05,
        "delta": -0.95,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.0,
        "manipulation_rate": 0.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.085,
        "manipulation_rate": 0.0,
        "delta": -0.085,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.6909547738693468,
        "manipulation_rate": 0.0,
        "delta": -0.6909547738693468,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.6725,
        "manipulation_rate": 1.0,
        "delta": 0.3275,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.1775,
        "manipulation_rate": 1.0,
        "delta": 0.8225,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4475,
        "manipulation_rate": 1.0,
        "delta": 0.5525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "baseline_rate": 0.6725,
        "manipulation_rate": 0.975,
        "delta": 0.3025,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "baseline_rate": 0.1775,
        "manipulation_rate": 0.375,
        "delta": 0.1975,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "baseline_rate": 1.0,
        "manipulation_rate": 1.0,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "baseline_rate": 0.025,
        "manipulation_rate": 0.025,
        "delta": 0.0,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.9,
        "manipulation_rate": 1.0,
        "delta": 0.09999999999999998,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "baseline_rate": 0.4475,
        "manipulation_rate": 1.0,
        "delta": 0.5525,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_spreadsheets_pos",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.975,
        "delta": 0.825,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_spreadsheets_pos",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.542713567839196,
        "manipulation_rate": 1.0,
        "delta": 0.457286432160804,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_spreadsheets_neg",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.15,
        "manipulation_rate": 0.0,
        "delta": -0.15,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_spreadsheets_neg",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.542713567839196,
        "manipulation_rate": 0.0,
        "delta": -0.542713567839196,
        "baseline_n": 398,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_puns_pos",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.7325,
        "manipulation_rate": 0.975,
        "delta": 0.24249999999999994,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_puns_pos",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.9125,
        "manipulation_rate": 0.975,
        "delta": 0.0625,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_puns_neg",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.7325,
        "manipulation_rate": 0.0,
        "delta": -0.7325,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_puns_neg",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.9125,
        "manipulation_rate": 0.025,
        "delta": -0.8875,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_public_speaking_pos",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.85,
        "manipulation_rate": 1.0,
        "delta": 0.15000000000000002,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_public_speaking_pos",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.7775,
        "manipulation_rate": 1.0,
        "delta": 0.22250000000000003,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_public_speaking_neg",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.85,
        "manipulation_rate": 0.005,
        "delta": -0.845,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_public_speaking_neg",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.7775,
        "manipulation_rate": 0.0,
        "delta": -0.7775,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_board_games_pos",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.95,
        "manipulation_rate": 1.0,
        "delta": 0.050000000000000044,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_board_games_pos",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.645,
        "manipulation_rate": 1.0,
        "delta": 0.355,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_board_games_neg",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.95,
        "manipulation_rate": 0.5,
        "delta": -0.44999999999999996,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      },
      {
        "prompt_id": "subtle_board_games_neg",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "baseline_rate": 0.645,
        "manipulation_rate": 0.1,
        "delta": -0.545,
        "baseline_n": 400,
        "manipulation_n": 400,
        "n_comparisons": 40
      }
    ],
    "probe_behavioral": [
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.17500000000000004,
        "probe_delta_L31": 19.130859375,
        "probe_delta_L43": 481.26708984375,
        "probe_delta_L55": 729.419677734375
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.505,
        "probe_delta_L31": -4.34326171875,
        "probe_delta_L43": 665.5181274414062,
        "probe_delta_L55": 637.8419189453125
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -38.1273193359375,
        "probe_delta_L43": 532.171142578125,
        "probe_delta_L55": 738.4697265625
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.09750000000000003,
        "probe_delta_L31": -110.06884765625,
        "probe_delta_L43": 174.2603759765625,
        "probe_delta_L55": 529.298828125
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -18.906494140625,
        "probe_delta_L43": 273.944580078125,
        "probe_delta_L55": 656.586669921875
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.9296482412060302,
        "probe_delta_L31": 162.271728515625,
        "probe_delta_L43": 1133.84375,
        "probe_delta_L55": 2113.08544921875
      },
      {
        "prompt_id": "subtle_cheese_pos_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.2925,
        "probe_delta_L31": -50.943115234375,
        "probe_delta_L43": 460.162353515625,
        "probe_delta_L55": 882.05517578125
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_math",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.7749999999999999,
        "probe_delta_L31": -265.1585693359375,
        "probe_delta_L43": -148.9921875,
        "probe_delta_L55": 232.7197265625
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_coding",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.47,
        "probe_delta_L31": -169.11541748046875,
        "probe_delta_L43": 204.66607666015625,
        "probe_delta_L55": 434.8116455078125
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_fiction",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.7,
        "probe_delta_L31": -235.3685302734375,
        "probe_delta_L43": 377.3553466796875,
        "probe_delta_L55": 450.775634765625
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_content",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.8775,
        "probe_delta_L31": -456.7232666015625,
        "probe_delta_L43": -396.9476318359375,
        "probe_delta_L55": 297.88134765625
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "crossed_cheese_harmful",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": -0.07500000000000001,
        "probe_delta_L31": -106.59033203125,
        "probe_delta_L43": 127.4691162109375,
        "probe_delta_L55": 495.8525390625
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_1",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.07035175879396985,
        "probe_delta_L31": -338.12408447265625,
        "probe_delta_L43": 81.38714599609375,
        "probe_delta_L55": 1136.136962890625
      },
      {
        "prompt_id": "subtle_cheese_neg_implicit",
        "target_topic": "cheese",
        "target_task_id": "hidden_cheese_2",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6825,
        "probe_delta_L31": -349.6114501953125,
        "probe_delta_L43": -582.462890625,
        "probe_delta_L55": -281.0771484375
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.24250000000000005,
        "probe_delta_L31": 121.9237060546875,
        "probe_delta_L43": 642.74658203125,
        "probe_delta_L55": 531.467041015625
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.665,
        "probe_delta_L31": 123.920166015625,
        "probe_delta_L43": 452.68316650390625,
        "probe_delta_L55": 1080.41015625
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.02749999999999997,
        "probe_delta_L31": 94.17333984375,
        "probe_delta_L43": 280.6871337890625,
        "probe_delta_L55": 497.59619140625
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 29.7928466796875,
        "probe_delta_L43": 213.1500244140625,
        "probe_delta_L55": 412.71728515625
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.4275,
        "probe_delta_L31": 46.83203125,
        "probe_delta_L43": 241.0867919921875,
        "probe_delta_L55": 675.4229736328125
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.71,
        "probe_delta_L31": 108.5179443359375,
        "probe_delta_L43": 301.5472412109375,
        "probe_delta_L55": 153.4697265625
      },
      {
        "prompt_id": "subtle_rainy_weather_pos_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5201005025125628,
        "probe_delta_L31": 94.5587158203125,
        "probe_delta_L43": 882.03857421875,
        "probe_delta_L55": 781.606689453125
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_math",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.7575,
        "probe_delta_L31": -26.1051025390625,
        "probe_delta_L43": 128.1220703125,
        "probe_delta_L55": -147.208740234375
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_coding",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.31,
        "probe_delta_L31": 9.64971923828125,
        "probe_delta_L43": 227.02978515625,
        "probe_delta_L55": 685.2595825195312
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_fiction",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9700000000000001,
        "probe_delta_L31": 55.27001953125,
        "probe_delta_L43": 293.7183837890625,
        "probe_delta_L55": 308.022705078125
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_content",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -1.0,
        "probe_delta_L31": 24.4285888671875,
        "probe_delta_L43": 150.3375244140625,
        "probe_delta_L55": 344.062255859375
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "crossed_rainy_weather_harmful",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -20.175537109375,
        "probe_delta_L43": 52.4068603515625,
        "probe_delta_L55": 336.1104736328125
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_1",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.29,
        "probe_delta_L31": 43.1549072265625,
        "probe_delta_L43": 285.4560546875,
        "probe_delta_L55": 136.3660888671875
      },
      {
        "prompt_id": "subtle_rainy_weather_neg_conditional",
        "target_topic": "rainy_weather",
        "target_task_id": "hidden_rainy_weather_2",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.4798994974874372,
        "probe_delta_L31": -153.5482177734375,
        "probe_delta_L43": 232.462890625,
        "probe_delta_L55": -272.059326171875
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.13,
        "probe_delta_L31": 195.2357177734375,
        "probe_delta_L43": 653.2779541015625,
        "probe_delta_L55": 430.25634765625
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 166.6207275390625,
        "probe_delta_L43": 535.6520385742188,
        "probe_delta_L55": 1051.0743408203125
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.025000000000000022,
        "probe_delta_L31": 112.4976806640625,
        "probe_delta_L43": 201.348876953125,
        "probe_delta_L55": 439.306396484375
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 45.574951171875,
        "probe_delta_L43": 198.68701171875,
        "probe_delta_L55": 163.074951171875
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.027443609022556395,
        "probe_delta_L31": 159.6767578125,
        "probe_delta_L43": 639.254150390625,
        "probe_delta_L55": 678.3922119140625
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.1328320802005013,
        "probe_delta_L31": 144.810302734375,
        "probe_delta_L43": 857.8087158203125,
        "probe_delta_L55": 1223.676025390625
      },
      {
        "prompt_id": "subtle_cats_pos_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.11528822055137844,
        "probe_delta_L31": 162.268310546875,
        "probe_delta_L43": 157.4205322265625,
        "probe_delta_L55": 438.2989501953125
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_math",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.2925,
        "probe_delta_L31": 175.3184814453125,
        "probe_delta_L43": 913.3736572265625,
        "probe_delta_L55": 1164.123779296875
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_coding",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.49999999999999994,
        "probe_delta_L31": 66.19732666015625,
        "probe_delta_L43": 569.1189575195312,
        "probe_delta_L55": 696.9261474609375
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_fiction",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -6.2884521484375,
        "probe_delta_L43": 96.3973388671875,
        "probe_delta_L55": 138.253173828125
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_content",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.02499999999999991,
        "probe_delta_L31": -82.6038818359375,
        "probe_delta_L43": 107.6103515625,
        "probe_delta_L55": 331.041259765625
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "crossed_cats_harmful",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0024436090225563936,
        "probe_delta_L31": 3.09765625,
        "probe_delta_L43": 73.6832275390625,
        "probe_delta_L55": 14.6942138671875
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_1",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.1328320802005013,
        "probe_delta_L31": 5.851318359375,
        "probe_delta_L43": 806.3370361328125,
        "probe_delta_L55": 968.23046875
      },
      {
        "prompt_id": "subtle_cats_neg_backstory",
        "target_topic": "cats",
        "target_task_id": "hidden_cats_2",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.00971177944862156,
        "probe_delta_L31": -13.6717529296875,
        "probe_delta_L43": 305.0946044921875,
        "probe_delta_L55": -76.6192626953125
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.07750000000000001,
        "probe_delta_L31": 100.29730224609375,
        "probe_delta_L43": 625.884521484375,
        "probe_delta_L55": 225.55419921875
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.75,
        "probe_delta_L31": 197.7900390625,
        "probe_delta_L43": 413.526611328125,
        "probe_delta_L55": 1056.41796875
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 165.4049072265625,
        "probe_delta_L43": 293.0072021484375,
        "probe_delta_L55": 668.959228515625
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.22250000000000003,
        "probe_delta_L31": 137.16259765625,
        "probe_delta_L43": 224.4398193359375,
        "probe_delta_L55": 427.9150390625
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.04639175257731959,
        "probe_delta_L31": -18.64410400390625,
        "probe_delta_L43": 56.9774169921875,
        "probe_delta_L55": -695.7525024414062
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.55,
        "probe_delta_L31": 176.4512939453125,
        "probe_delta_L43": 325.3001708984375,
        "probe_delta_L55": 831.981689453125
      },
      {
        "prompt_id": "subtle_classical_music_pos_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "positive",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.8475,
        "probe_delta_L31": 338.13916015625,
        "probe_delta_L43": 1099.1234130859375,
        "probe_delta_L55": 1400.719482421875
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_math",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.6275,
        "probe_delta_L31": -48.2166748046875,
        "probe_delta_L43": 351.9583740234375,
        "probe_delta_L55": 405.305419921875
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_coding",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.2,
        "probe_delta_L31": 143.35430908203125,
        "probe_delta_L43": 221.816650390625,
        "probe_delta_L55": 860.27734375
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_fiction",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.8049999999999999,
        "probe_delta_L31": 102.2413330078125,
        "probe_delta_L43": 335.5592041015625,
        "probe_delta_L55": 675.89892578125
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_content",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.7525,
        "probe_delta_L31": 44.48046875,
        "probe_delta_L43": 22.35693359375,
        "probe_delta_L55": 306.196044921875
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "crossed_classical_music_harmful",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": -36.90313720703125,
        "probe_delta_L43": 123.734619140625,
        "probe_delta_L55": -703.545166015625
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_1",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.425,
        "probe_delta_L31": 23.2666015625,
        "probe_delta_L43": -8.6588134765625,
        "probe_delta_L55": 408.52587890625
      },
      {
        "prompt_id": "subtle_classical_music_neg_understatement",
        "target_topic": "classical_music",
        "target_task_id": "hidden_classical_music_2",
        "direction": "negative",
        "prompt_type": "understatement",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.1275,
        "probe_delta_L31": 178.59228515625,
        "probe_delta_L43": 512.8121337890625,
        "probe_delta_L55": 824.6754760742188
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.030000000000000027,
        "probe_delta_L31": 241.5989990234375,
        "probe_delta_L43": 413.155517578125,
        "probe_delta_L55": 733.67822265625
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.7925,
        "probe_delta_L31": 256.443359375,
        "probe_delta_L43": 578.1866455078125,
        "probe_delta_L55": 1302.107666015625
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 46.91455078125,
        "probe_delta_L43": 257.14892578125,
        "probe_delta_L55": 559.074951171875
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 9.0025634765625,
        "probe_delta_L43": 207.260498046875,
        "probe_delta_L55": 520.80029296875
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 29.243408203125,
        "probe_delta_L43": -90.0887451171875,
        "probe_delta_L55": -478.749755859375
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5050251256281407,
        "probe_delta_L31": 200.5496826171875,
        "probe_delta_L43": 742.7869262695312,
        "probe_delta_L55": 1258.612060546875
      },
      {
        "prompt_id": "subtle_gardening_pos_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "positive",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3535353535353535,
        "probe_delta_L31": 200.9935302734375,
        "probe_delta_L43": 561.008056640625,
        "probe_delta_L55": 244.353271484375
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_math",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.5725,
        "probe_delta_L31": 4.8734130859375,
        "probe_delta_L43": 106.271728515625,
        "probe_delta_L55": -46.7275390625
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_coding",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.1825,
        "probe_delta_L31": 128.876708984375,
        "probe_delta_L43": 433.6608581542969,
        "probe_delta_L55": 885.1567993164062
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_fiction",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.09999999999999998,
        "probe_delta_L31": -42.5723876953125,
        "probe_delta_L43": 175.4449462890625,
        "probe_delta_L55": 582.4306640625
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_content",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.75,
        "probe_delta_L31": -69.9405517578125,
        "probe_delta_L43": 120.912841796875,
        "probe_delta_L55": 524.6337890625
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "crossed_gardening_harmful",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0024999999999999988,
        "probe_delta_L31": -3.0811767578125,
        "probe_delta_L43": -210.5689697265625,
        "probe_delta_L55": -568.578857421875
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_1",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.3949748743718593,
        "probe_delta_L31": 1.33203125,
        "probe_delta_L43": 425.56072998046875,
        "probe_delta_L55": 530.2646484375
      },
      {
        "prompt_id": "subtle_gardening_neg_third_person",
        "target_topic": "gardening",
        "target_task_id": "hidden_gardening_2",
        "direction": "negative",
        "prompt_type": "third_person",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.5339646464646465,
        "probe_delta_L31": -60.01416015625,
        "probe_delta_L43": -55.9638671875,
        "probe_delta_L55": -706.2509765625
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.0050000000000000044,
        "probe_delta_L31": 82.09716796875,
        "probe_delta_L43": 192.90576171875,
        "probe_delta_L55": 122.760498046875
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.58,
        "probe_delta_L31": 194.3927001953125,
        "probe_delta_L43": 416.045654296875,
        "probe_delta_L55": 946.5584716796875
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 57.560546875,
        "probe_delta_L43": 185.412109375,
        "probe_delta_L55": 518.6817626953125
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -20.583251953125,
        "probe_delta_L43": 225.842529296875,
        "probe_delta_L55": 389.958251953125
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 16.2947998046875,
        "probe_delta_L43": 125.833740234375,
        "probe_delta_L55": 236.8310546875
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.1725,
        "probe_delta_L31": 131.61669921875,
        "probe_delta_L43": 535.3721923828125,
        "probe_delta_L55": 756.999267578125
      },
      {
        "prompt_id": "subtle_astronomy_pos_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "positive",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.39097744360902253,
        "probe_delta_L31": 202.47021484375,
        "probe_delta_L43": 624.1898193359375,
        "probe_delta_L55": 841.91748046875
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_math",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.995,
        "probe_delta_L31": -365.95904541015625,
        "probe_delta_L43": -447.0137939453125,
        "probe_delta_L55": -342.052734375
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_coding",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.15,
        "probe_delta_L31": 104.60174560546875,
        "probe_delta_L43": 700.6016235351562,
        "probe_delta_L55": 1652.4571533203125
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_fiction",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -1.0,
        "probe_delta_L31": -57.223876953125,
        "probe_delta_L43": 760.53564453125,
        "probe_delta_L55": 1494.5806884765625
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_content",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.685,
        "probe_delta_L31": -239.21484375,
        "probe_delta_L43": -176.2144775390625,
        "probe_delta_L55": 391.316650390625
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "crossed_astronomy_harmful",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 83.805419921875,
        "probe_delta_L43": 152.23583984375,
        "probe_delta_L55": 804.083251953125
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_1",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.7775,
        "probe_delta_L31": -136.18017578125,
        "probe_delta_L43": 417.5364990234375,
        "probe_delta_L55": 836.470458984375
      },
      {
        "prompt_id": "subtle_astronomy_neg_implicit",
        "target_topic": "astronomy",
        "target_task_id": "hidden_astronomy_2",
        "direction": "negative",
        "prompt_type": "implicit",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6090225563909775,
        "probe_delta_L31": -204.724609375,
        "probe_delta_L43": 351.984619140625,
        "probe_delta_L55": 515.432861328125
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.022499999999999964,
        "probe_delta_L31": 104.51416015625,
        "probe_delta_L43": 201.9110107421875,
        "probe_delta_L55": -65.598876953125
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.745,
        "probe_delta_L31": 142.7164306640625,
        "probe_delta_L43": 275.56451416015625,
        "probe_delta_L55": 455.654296875
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 19.5313720703125,
        "probe_delta_L43": 462.1103515625,
        "probe_delta_L55": 846.6405029296875
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -22.6766357421875,
        "probe_delta_L43": 17.8531494140625,
        "probe_delta_L55": 49.634765625
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.025,
        "probe_delta_L31": 33.8223876953125,
        "probe_delta_L43": -20.0721435546875,
        "probe_delta_L55": -554.7015991210938
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.915,
        "probe_delta_L31": 119.4017333984375,
        "probe_delta_L43": 777.5750732421875,
        "probe_delta_L55": 959.931884765625
      },
      {
        "prompt_id": "subtle_cooking_pos_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "positive",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.3090452261306532,
        "probe_delta_L31": 43.9185791015625,
        "probe_delta_L43": 591.5355224609375,
        "probe_delta_L55": 330.66650390625
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_math",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": -0.9525,
        "probe_delta_L31": -36.990478515625,
        "probe_delta_L43": -163.623046875,
        "probe_delta_L55": -533.3546142578125
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_coding",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": -0.18,
        "probe_delta_L31": 80.54510498046875,
        "probe_delta_L43": 126.62078857421875,
        "probe_delta_L55": 68.2091064453125
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_fiction",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": -0.9,
        "probe_delta_L31": -28.3182373046875,
        "probe_delta_L43": 406.335205078125,
        "probe_delta_L55": 695.2860107421875
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_content",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": -0.95,
        "probe_delta_L31": -72.281494140625,
        "probe_delta_L43": -48.4580078125,
        "probe_delta_L55": -24.855224609375
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "crossed_cooking_harmful",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 10.6087646484375,
        "probe_delta_L43": -110.64013671875,
        "probe_delta_L55": -603.4677734375
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_1",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.085,
        "probe_delta_L31": -2.42376708984375,
        "probe_delta_L43": 399.5540771484375,
        "probe_delta_L55": -117.5003662109375
      },
      {
        "prompt_id": "subtle_cooking_neg_conditional",
        "target_topic": "cooking",
        "target_task_id": "hidden_cooking_2",
        "direction": "negative",
        "prompt_type": "conditional",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": -0.6909547738693468,
        "probe_delta_L31": -104.9725341796875,
        "probe_delta_L43": 55.856689453125,
        "probe_delta_L55": -407.71826171875
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.3275,
        "probe_delta_L31": 12.355712890625,
        "probe_delta_L43": 338.00146484375,
        "probe_delta_L55": 645.584228515625
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.8225,
        "probe_delta_L31": 174.57275390625,
        "probe_delta_L43": 407.433349609375,
        "probe_delta_L55": 688.7538452148438
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 61.0625,
        "probe_delta_L43": 322.0028076171875,
        "probe_delta_L55": 668.365966796875
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 59.407958984375,
        "probe_delta_L43": 233.3087158203125,
        "probe_delta_L55": 776.021484375
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 92.4583740234375,
        "probe_delta_L43": 383.0882568359375,
        "probe_delta_L55": 624.3326416015625
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": 37.582763671875,
        "probe_delta_L43": 214.0892333984375,
        "probe_delta_L55": 156.99365234375
      },
      {
        "prompt_id": "subtle_ancient_history_pos_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "positive",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5525,
        "probe_delta_L31": 18.722900390625,
        "probe_delta_L43": 955.7061767578125,
        "probe_delta_L55": 1926.2044677734375
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_math",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "math",
        "behavioral_delta": 0.3025,
        "probe_delta_L31": -101.9259033203125,
        "probe_delta_L43": 399.583740234375,
        "probe_delta_L55": 388.8543701171875
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_coding",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "coding",
        "behavioral_delta": 0.1975,
        "probe_delta_L31": 127.3868408203125,
        "probe_delta_L43": 252.6495361328125,
        "probe_delta_L55": 326.6929931640625
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_fiction",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "fiction",
        "behavioral_delta": 0.0,
        "probe_delta_L31": -40.745361328125,
        "probe_delta_L43": 200.4342041015625,
        "probe_delta_L55": 445.078125
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_content",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "content_generation",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 38.84033203125,
        "probe_delta_L43": 182.236328125,
        "probe_delta_L55": 436.4296875
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "crossed_ancient_history_harmful",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "crossed",
        "category_shell": "harmful",
        "behavioral_delta": 0.0,
        "probe_delta_L31": 1.1126708984375,
        "probe_delta_L43": 241.3350830078125,
        "probe_delta_L55": 70.8978271484375
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_1",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.09999999999999998,
        "probe_delta_L31": -26.3934326171875,
        "probe_delta_L43": 74.6990966796875,
        "probe_delta_L55": -129.325439453125
      },
      {
        "prompt_id": "subtle_ancient_history_neg_backstory",
        "target_topic": "ancient_history",
        "target_task_id": "hidden_ancient_history_2",
        "direction": "negative",
        "prompt_type": "backstory",
        "task_set": "pure",
        "category_shell": "pure",
        "behavioral_delta": 0.5525,
        "probe_delta_L31": -58.87384033203125,
        "probe_delta_L43": 808.057861328125,
        "probe_delta_L55": 1017.1259765625
      },
      {
        "prompt_id": "subtle_spreadsheets_pos",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.825,
        "probe_delta_L31": 73.3883056640625,
        "probe_delta_L43": 140.07080078125,
        "probe_delta_L55": 501.2578125
      },
      {
        "prompt_id": "subtle_spreadsheets_pos",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.457286432160804,
        "probe_delta_L31": 107.392578125,
        "probe_delta_L43": 481.8251953125,
        "probe_delta_L55": 819.6539306640625
      },
      {
        "prompt_id": "subtle_spreadsheets_neg",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.15,
        "probe_delta_L31": 33.66656494140625,
        "probe_delta_L43": 58.1851806640625,
        "probe_delta_L55": 414.4788818359375
      },
      {
        "prompt_id": "subtle_spreadsheets_neg",
        "target_topic": "spreadsheets",
        "target_task_id": "hidden_spreadsheets_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.542713567839196,
        "probe_delta_L31": -118.3760986328125,
        "probe_delta_L43": 43.4080810546875,
        "probe_delta_L55": 373.0697021484375
      },
      {
        "prompt_id": "subtle_puns_pos",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.24249999999999994,
        "probe_delta_L31": 152.7645263671875,
        "probe_delta_L43": 500.2911376953125,
        "probe_delta_L55": 245.34228515625
      },
      {
        "prompt_id": "subtle_puns_pos",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.0625,
        "probe_delta_L31": 117.7410888671875,
        "probe_delta_L43": 125.5330810546875,
        "probe_delta_L55": 472.08984375
      },
      {
        "prompt_id": "subtle_puns_neg",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.7325,
        "probe_delta_L31": -257.60699462890625,
        "probe_delta_L43": -285.318115234375,
        "probe_delta_L55": -1493.2786865234375
      },
      {
        "prompt_id": "subtle_puns_neg",
        "target_topic": "puns",
        "target_task_id": "hidden_puns_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.8875,
        "probe_delta_L31": -67.745361328125,
        "probe_delta_L43": 98.5760498046875,
        "probe_delta_L55": 144.332763671875
      },
      {
        "prompt_id": "subtle_public_speaking_pos",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.15000000000000002,
        "probe_delta_L31": 198.94580078125,
        "probe_delta_L43": 517.4837646484375,
        "probe_delta_L55": 724.8397216796875
      },
      {
        "prompt_id": "subtle_public_speaking_pos",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.22250000000000003,
        "probe_delta_L31": 44.477294921875,
        "probe_delta_L43": -91.3165283203125,
        "probe_delta_L55": -128.855712890625
      },
      {
        "prompt_id": "subtle_public_speaking_neg",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.845,
        "probe_delta_L31": 81.51513671875,
        "probe_delta_L43": 358.22265625,
        "probe_delta_L55": 534.5704345703125
      },
      {
        "prompt_id": "subtle_public_speaking_neg",
        "target_topic": "public_speaking",
        "target_task_id": "hidden_public_speaking_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.7775,
        "probe_delta_L31": -70.4354248046875,
        "probe_delta_L43": -291.499755859375,
        "probe_delta_L55": -576.532958984375
      },
      {
        "prompt_id": "subtle_board_games_pos",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_1",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.050000000000000044,
        "probe_delta_L31": 184.814697265625,
        "probe_delta_L43": 543.23046875,
        "probe_delta_L55": 676.93408203125
      },
      {
        "prompt_id": "subtle_board_games_pos",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_2",
        "direction": "positive",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": 0.355,
        "probe_delta_L31": 262.8424072265625,
        "probe_delta_L43": 1023.240478515625,
        "probe_delta_L55": 1608.88916015625
      },
      {
        "prompt_id": "subtle_board_games_neg",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_1",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.44999999999999996,
        "probe_delta_L31": 75.454833984375,
        "probe_delta_L43": 145.8548583984375,
        "probe_delta_L55": 220.41064453125
      },
      {
        "prompt_id": "subtle_board_games_neg",
        "target_topic": "board_games",
        "target_task_id": "hidden_board_games_2",
        "direction": "negative",
        "prompt_type": "persona",
        "task_set": "subtle",
        "category_shell": "pure",
        "behavioral_delta": -0.545,
        "probe_delta_L31": -28.605224609375,
        "probe_delta_L43": -26.75341796875,
        "probe_delta_L55": 484.9205322265625
      }
    ]
  }
}