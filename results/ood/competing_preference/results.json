{
  "experiment": "competing_preference",
  "model": "gemma-3-27b-it",
  "probe": "ridge_L31_3k",
  "source_commit": "5487348",
  "behavioral": [
    {
      "pair_id": "cheese_math",
      "target_topic": "cheese",
      "category_shell": "math",
      "target_task_id": "crossed_cheese_math",
      "prompt_id": "compete_cheese_math_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.825,
      "manipulation_rate": 0.2475,
      "delta": -0.5774999999999999,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cheese_math",
      "target_topic": "cheese",
      "category_shell": "math",
      "target_task_id": "crossed_cheese_math",
      "prompt_id": "compete_cheese_math_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.825,
      "manipulation_rate": 0.1475,
      "delta": -0.6775,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cats_coding",
      "target_topic": "cats",
      "category_shell": "coding",
      "target_task_id": "crossed_cats_coding",
      "prompt_id": "compete_cats_coding_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.45,
      "manipulation_rate": 0.005012531328320802,
      "delta": -0.4449874686716792,
      "baseline_n": 400,
      "manipulation_n": 399,
      "n_comparisons": 40
    },
    {
      "pair_id": "cats_coding",
      "target_topic": "cats",
      "category_shell": "coding",
      "target_task_id": "crossed_cats_coding",
      "prompt_id": "compete_cats_coding_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.45,
      "manipulation_rate": 0.0875,
      "delta": -0.36250000000000004,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "gardening_fiction",
      "target_topic": "gardening",
      "category_shell": "fiction",
      "target_task_id": "crossed_gardening_fiction",
      "prompt_id": "compete_gardening_fiction_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.1075,
      "delta": -0.8925,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "gardening_fiction",
      "target_topic": "gardening",
      "category_shell": "fiction",
      "target_task_id": "crossed_gardening_fiction",
      "prompt_id": "compete_gardening_fiction_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.52,
      "delta": -0.48,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "astronomy_math",
      "target_topic": "astronomy",
      "category_shell": "math",
      "target_task_id": "crossed_astronomy_math",
      "prompt_id": "compete_astronomy_math_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.9975,
      "manipulation_rate": 0.165,
      "delta": -0.8325,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "astronomy_math",
      "target_topic": "astronomy",
      "category_shell": "math",
      "target_task_id": "crossed_astronomy_math",
      "prompt_id": "compete_astronomy_math_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.9975,
      "manipulation_rate": 0.5175,
      "delta": -0.4800000000000001,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "classical_music_coding",
      "target_topic": "classical_music",
      "category_shell": "coding",
      "target_task_id": "crossed_classical_music_coding",
      "prompt_id": "compete_classical_music_coding_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.225,
      "manipulation_rate": 0.08521303258145363,
      "delta": -0.13978696741854638,
      "baseline_n": 400,
      "manipulation_n": 399,
      "n_comparisons": 40
    },
    {
      "pair_id": "classical_music_coding",
      "target_topic": "classical_music",
      "category_shell": "coding",
      "target_task_id": "crossed_classical_music_coding",
      "prompt_id": "compete_classical_music_coding_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.225,
      "manipulation_rate": 0.0475,
      "delta": -0.1775,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cooking_fiction",
      "target_topic": "cooking",
      "category_shell": "fiction",
      "target_task_id": "crossed_cooking_fiction",
      "prompt_id": "compete_cooking_fiction_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.095,
      "delta": -0.905,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cooking_fiction",
      "target_topic": "cooking",
      "category_shell": "fiction",
      "target_task_id": "crossed_cooking_fiction",
      "prompt_id": "compete_cooking_fiction_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.75,
      "delta": -0.25,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "rainy_weather_math",
      "target_topic": "rainy_weather",
      "category_shell": "math",
      "target_task_id": "crossed_rainy_weather_math",
      "prompt_id": "compete_rainy_weather_math_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.7475,
      "manipulation_rate": 0.105,
      "delta": -0.6425000000000001,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "rainy_weather_math",
      "target_topic": "rainy_weather",
      "category_shell": "math",
      "target_task_id": "crossed_rainy_weather_math",
      "prompt_id": "compete_rainy_weather_math_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.7475,
      "manipulation_rate": 0.365,
      "delta": -0.38250000000000006,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "ancient_history_coding",
      "target_topic": "ancient_history",
      "category_shell": "coding",
      "target_task_id": "crossed_ancient_history_coding",
      "prompt_id": "compete_ancient_history_coding_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.17293233082706766,
      "manipulation_rate": 0.0,
      "delta": -0.17293233082706766,
      "baseline_n": 399,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "ancient_history_coding",
      "target_topic": "ancient_history",
      "category_shell": "coding",
      "target_task_id": "crossed_ancient_history_coding",
      "prompt_id": "compete_ancient_history_coding_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.17293233082706766,
      "manipulation_rate": 0.1075,
      "delta": -0.06543233082706766,
      "baseline_n": 399,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cheese_fiction",
      "target_topic": "cheese",
      "category_shell": "fiction",
      "target_task_id": "crossed_cheese_fiction",
      "prompt_id": "compete_cheese_fiction_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.1825,
      "delta": -0.8175,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cheese_fiction",
      "target_topic": "cheese",
      "category_shell": "fiction",
      "target_task_id": "crossed_cheese_fiction",
      "prompt_id": "compete_cheese_fiction_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 1.0,
      "manipulation_rate": 0.13,
      "delta": -0.87,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cats_math",
      "target_topic": "cats",
      "category_shell": "math",
      "target_task_id": "crossed_cats_math",
      "prompt_id": "compete_cats_math_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.87,
      "manipulation_rate": 0.2506265664160401,
      "delta": -0.6193734335839599,
      "baseline_n": 400,
      "manipulation_n": 399,
      "n_comparisons": 40
    },
    {
      "pair_id": "cats_math",
      "target_topic": "cats",
      "category_shell": "math",
      "target_task_id": "crossed_cats_math",
      "prompt_id": "compete_cats_math_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.87,
      "manipulation_rate": 0.41,
      "delta": -0.46,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cooking_coding",
      "target_topic": "cooking",
      "category_shell": "coding",
      "target_task_id": "crossed_cooking_coding",
      "prompt_id": "compete_cooking_coding_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.2575,
      "manipulation_rate": 0.025,
      "delta": -0.2325,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "cooking_coding",
      "target_topic": "cooking",
      "category_shell": "coding",
      "target_task_id": "crossed_cooking_coding",
      "prompt_id": "compete_cooking_coding_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.2575,
      "manipulation_rate": 0.58,
      "delta": 0.32249999999999995,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "gardening_math",
      "target_topic": "gardening",
      "category_shell": "math",
      "target_task_id": "crossed_gardening_math",
      "prompt_id": "compete_gardening_math_topicpos",
      "favored_dim": "topic",
      "baseline_rate": 0.97,
      "manipulation_rate": 0.3475,
      "delta": -0.6225,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    },
    {
      "pair_id": "gardening_math",
      "target_topic": "gardening",
      "category_shell": "math",
      "target_task_id": "crossed_gardening_math",
      "prompt_id": "compete_gardening_math_shellpos",
      "favored_dim": "shell",
      "baseline_rate": 0.97,
      "manipulation_rate": 0.55,
      "delta": -0.41999999999999993,
      "baseline_n": 400,
      "manipulation_n": 400,
      "n_comparisons": 40
    }
  ],
  "probe_behavioral": [
    {
      "pair_id": "cheese_math",
      "target_topic": "cheese",
      "category_shell": "math",
      "target_task_id": "crossed_cheese_math",
      "prompt_id": "compete_cheese_math_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.5774999999999999,
      "probe_delta_L31": -123.33331298828125,
      "probe_delta_L43": -99.125244140625,
      "probe_delta_L55": -190.039794921875
    },
    {
      "pair_id": "cheese_math",
      "target_topic": "cheese",
      "category_shell": "math",
      "target_task_id": "crossed_cheese_math",
      "prompt_id": "compete_cheese_math_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.6775,
      "probe_delta_L31": -142.911865234375,
      "probe_delta_L43": -64.1591796875,
      "probe_delta_L55": -183.443603515625
    },
    {
      "pair_id": "cats_coding",
      "target_topic": "cats",
      "category_shell": "coding",
      "target_task_id": "crossed_cats_coding",
      "prompt_id": "compete_cats_coding_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.4449874686716792,
      "probe_delta_L31": -30.70477294921875,
      "probe_delta_L43": 210.93829345703125,
      "probe_delta_L55": 489.921875
    },
    {
      "pair_id": "cats_coding",
      "target_topic": "cats",
      "category_shell": "coding",
      "target_task_id": "crossed_cats_coding",
      "prompt_id": "compete_cats_coding_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.36250000000000004,
      "probe_delta_L31": -14.6092529296875,
      "probe_delta_L43": 209.60296630859375,
      "probe_delta_L55": 452.770751953125
    },
    {
      "pair_id": "gardening_fiction",
      "target_topic": "gardening",
      "category_shell": "fiction",
      "target_task_id": "crossed_gardening_fiction",
      "prompt_id": "compete_gardening_fiction_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.8925,
      "probe_delta_L31": -226.83355712890625,
      "probe_delta_L43": 17.563232421875,
      "probe_delta_L55": 266.8675537109375
    },
    {
      "pair_id": "gardening_fiction",
      "target_topic": "gardening",
      "category_shell": "fiction",
      "target_task_id": "crossed_gardening_fiction",
      "prompt_id": "compete_gardening_fiction_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.48,
      "probe_delta_L31": -65.76904296875,
      "probe_delta_L43": 398.737060546875,
      "probe_delta_L55": 827.343017578125
    },
    {
      "pair_id": "astronomy_math",
      "target_topic": "astronomy",
      "category_shell": "math",
      "target_task_id": "crossed_astronomy_math",
      "prompt_id": "compete_astronomy_math_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.8325,
      "probe_delta_L31": -275.1026611328125,
      "probe_delta_L43": -400.466552734375,
      "probe_delta_L55": -157.557861328125
    },
    {
      "pair_id": "astronomy_math",
      "target_topic": "astronomy",
      "category_shell": "math",
      "target_task_id": "crossed_astronomy_math",
      "prompt_id": "compete_astronomy_math_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.4800000000000001,
      "probe_delta_L31": -446.6707763671875,
      "probe_delta_L43": -783.4859619140625,
      "probe_delta_L55": -773.694091796875
    },
    {
      "pair_id": "classical_music_coding",
      "target_topic": "classical_music",
      "category_shell": "coding",
      "target_task_id": "crossed_classical_music_coding",
      "prompt_id": "compete_classical_music_coding_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.13978696741854638,
      "probe_delta_L31": 65.10784912109375,
      "probe_delta_L43": 292.781005859375,
      "probe_delta_L55": 787.268310546875
    },
    {
      "pair_id": "classical_music_coding",
      "target_topic": "classical_music",
      "category_shell": "coding",
      "target_task_id": "crossed_classical_music_coding",
      "prompt_id": "compete_classical_music_coding_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.1775,
      "probe_delta_L31": 158.1658935546875,
      "probe_delta_L43": 437.2806396484375,
      "probe_delta_L55": 1318.0079345703125
    },
    {
      "pair_id": "cooking_fiction",
      "target_topic": "cooking",
      "category_shell": "fiction",
      "target_task_id": "crossed_cooking_fiction",
      "prompt_id": "compete_cooking_fiction_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.905,
      "probe_delta_L31": -239.04901123046875,
      "probe_delta_L43": 102.4873046875,
      "probe_delta_L55": 117.0181884765625
    },
    {
      "pair_id": "cooking_fiction",
      "target_topic": "cooking",
      "category_shell": "fiction",
      "target_task_id": "crossed_cooking_fiction",
      "prompt_id": "compete_cooking_fiction_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.25,
      "probe_delta_L31": -80.4482421875,
      "probe_delta_L43": 444.122802734375,
      "probe_delta_L55": 751.5491943359375
    },
    {
      "pair_id": "rainy_weather_math",
      "target_topic": "rainy_weather",
      "category_shell": "math",
      "target_task_id": "crossed_rainy_weather_math",
      "prompt_id": "compete_rainy_weather_math_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.6425000000000001,
      "probe_delta_L31": -106.72344970703125,
      "probe_delta_L43": 84.215576171875,
      "probe_delta_L55": 421.3702392578125
    },
    {
      "pair_id": "rainy_weather_math",
      "target_topic": "rainy_weather",
      "category_shell": "math",
      "target_task_id": "crossed_rainy_weather_math",
      "prompt_id": "compete_rainy_weather_math_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.38250000000000006,
      "probe_delta_L31": -170.6287841796875,
      "probe_delta_L43": -41.4700927734375,
      "probe_delta_L55": -14.58349609375
    },
    {
      "pair_id": "ancient_history_coding",
      "target_topic": "ancient_history",
      "category_shell": "coding",
      "target_task_id": "crossed_ancient_history_coding",
      "prompt_id": "compete_ancient_history_coding_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.17293233082706766,
      "probe_delta_L31": 63.2230224609375,
      "probe_delta_L43": 493.1219482421875,
      "probe_delta_L55": 811.6616821289062
    },
    {
      "pair_id": "ancient_history_coding",
      "target_topic": "ancient_history",
      "category_shell": "coding",
      "target_task_id": "crossed_ancient_history_coding",
      "prompt_id": "compete_ancient_history_coding_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.06543233082706766,
      "probe_delta_L31": 37.35858154296875,
      "probe_delta_L43": 395.3509521484375,
      "probe_delta_L55": 528.6090698242188
    },
    {
      "pair_id": "cheese_fiction",
      "target_topic": "cheese",
      "category_shell": "fiction",
      "target_task_id": "crossed_cheese_fiction",
      "prompt_id": "compete_cheese_fiction_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.8175,
      "probe_delta_L31": -212.153076171875,
      "probe_delta_L43": 169.0213623046875,
      "probe_delta_L55": 519.3311767578125
    },
    {
      "pair_id": "cheese_fiction",
      "target_topic": "cheese",
      "category_shell": "fiction",
      "target_task_id": "crossed_cheese_fiction",
      "prompt_id": "compete_cheese_fiction_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.87,
      "probe_delta_L31": -195.8577880859375,
      "probe_delta_L43": 385.2850341796875,
      "probe_delta_L55": 920.1326904296875
    },
    {
      "pair_id": "cats_math",
      "target_topic": "cats",
      "category_shell": "math",
      "target_task_id": "crossed_cats_math",
      "prompt_id": "compete_cats_math_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.6193734335839599,
      "probe_delta_L31": -2.89495849609375,
      "probe_delta_L43": 519.8031005859375,
      "probe_delta_L55": 468.1517333984375
    },
    {
      "pair_id": "cats_math",
      "target_topic": "cats",
      "category_shell": "math",
      "target_task_id": "crossed_cats_math",
      "prompt_id": "compete_cats_math_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.46,
      "probe_delta_L31": -13.2960205078125,
      "probe_delta_L43": 524.740234375,
      "probe_delta_L55": 702.245361328125
    },
    {
      "pair_id": "cooking_coding",
      "target_topic": "cooking",
      "category_shell": "coding",
      "target_task_id": "crossed_cooking_coding",
      "prompt_id": "compete_cooking_coding_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.2325,
      "probe_delta_L31": 24.787109375,
      "probe_delta_L43": 232.26666259765625,
      "probe_delta_L55": 176.855224609375
    },
    {
      "pair_id": "cooking_coding",
      "target_topic": "cooking",
      "category_shell": "coding",
      "target_task_id": "crossed_cooking_coding",
      "prompt_id": "compete_cooking_coding_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": 0.32249999999999995,
      "probe_delta_L31": 114.04290771484375,
      "probe_delta_L43": 501.61260986328125,
      "probe_delta_L55": 820.820556640625
    },
    {
      "pair_id": "gardening_math",
      "target_topic": "gardening",
      "category_shell": "math",
      "target_task_id": "crossed_gardening_math",
      "prompt_id": "compete_gardening_math_topicpos",
      "favored_dim": "topic",
      "behavioral_delta": -0.6225,
      "probe_delta_L31": -121.00311279296875,
      "probe_delta_L43": -239.85498046875,
      "probe_delta_L55": -482.096923828125
    },
    {
      "pair_id": "gardening_math",
      "target_topic": "gardening",
      "category_shell": "math",
      "target_task_id": "crossed_gardening_math",
      "prompt_id": "compete_gardening_math_shellpos",
      "favored_dim": "shell",
      "behavioral_delta": -0.41999999999999993,
      "probe_delta_L31": -19.897705078125,
      "probe_delta_L43": 236.85400390625,
      "probe_delta_L55": 419.275146484375
    }
  ],
  "cross_task_analysis": {
    "topic_flips": [
      {
        "pair": "ancient_history_coding",
        "dim": "topic",
        "topic_pos_mean": 1.641876220703125,
        "shell_pos_mean": -184.87673950195312,
        "diff": 186.51861572265625,
        "expected_sign": "positive"
      },
      {
        "pair": "astronomy_math",
        "dim": "topic",
        "topic_pos_mean": 8.884490966796875,
        "shell_pos_mean": -225.51707458496094,
        "diff": 234.4015655517578,
        "expected_sign": "positive"
      },
      {
        "pair": "cats_coding",
        "dim": "topic",
        "topic_pos_mean": 48.728363037109375,
        "shell_pos_mean": -203.12582397460938,
        "diff": 251.85418701171875,
        "expected_sign": "positive"
      },
      {
        "pair": "cats_math",
        "dim": "topic",
        "topic_pos_mean": 81.23233032226562,
        "shell_pos_mean": -257.6205139160156,
        "diff": 338.85284423828125,
        "expected_sign": "positive"
      },
      {
        "pair": "cheese_fiction",
        "dim": "topic",
        "topic_pos_mean": -15.425735473632812,
        "shell_pos_mean": -284.2962951660156,
        "diff": 268.8705596923828,
        "expected_sign": "positive"
      },
      {
        "pair": "cheese_math",
        "dim": "topic",
        "topic_pos_mean": -2.40087890625,
        "shell_pos_mean": -286.9352111816406,
        "diff": 284.5343322753906,
        "expected_sign": "positive"
      },
      {
        "pair": "classical_music_coding",
        "dim": "topic",
        "topic_pos_mean": 0.1935882568359375,
        "shell_pos_mean": 17.536911010742188,
        "diff": -17.34332275390625,
        "expected_sign": "positive"
      },
      {
        "pair": "cooking_coding",
        "dim": "topic",
        "topic_pos_mean": 20.217941284179688,
        "shell_pos_mean": -189.62026977539062,
        "diff": 209.8382110595703,
        "expected_sign": "positive"
      },
      {
        "pair": "cooking_fiction",
        "dim": "topic",
        "topic_pos_mean": 24.28582763671875,
        "shell_pos_mean": -111.7232666015625,
        "diff": 136.00909423828125,
        "expected_sign": "positive"
      },
      {
        "pair": "gardening_fiction",
        "dim": "topic",
        "topic_pos_mean": 50.93650817871094,
        "shell_pos_mean": -167.3747100830078,
        "diff": 218.31121826171875,
        "expected_sign": "positive"
      },
      {
        "pair": "gardening_math",
        "dim": "topic",
        "topic_pos_mean": 9.9176025390625,
        "shell_pos_mean": -204.7498321533203,
        "diff": 214.6674346923828,
        "expected_sign": "positive"
      },
      {
        "pair": "rainy_weather_math",
        "dim": "topic",
        "topic_pos_mean": 95.05624389648438,
        "shell_pos_mean": -95.17984008789062,
        "diff": 190.236083984375,
        "expected_sign": "positive"
      }
    ],
    "shell_flips": [
      {
        "pair": "ancient_history_coding",
        "dim": "shell",
        "topic_pos_mean": 2.711146763392857,
        "shell_pos_mean": 178.50641741071428,
        "diff": -175.79527064732142,
        "expected_sign": "negative"
      },
      {
        "pair": "astronomy_math",
        "dim": "shell",
        "topic_pos_mean": -169.20996965680803,
        "shell_pos_mean": -23.123299734933035,
        "diff": -146.086669921875,
        "expected_sign": "negative"
      },
      {
        "pair": "cats_coding",
        "dim": "shell",
        "topic_pos_mean": -9.498482840401786,
        "shell_pos_mean": 140.81691196986608,
        "diff": -150.31539481026786,
        "expected_sign": "negative"
      },
      {
        "pair": "cats_math",
        "dim": "shell",
        "topic_pos_mean": -212.79822649274553,
        "shell_pos_mean": -68.04855782645089,
        "diff": -144.74966866629464,
        "expected_sign": "negative"
      },
      {
        "pair": "cheese_fiction",
        "dim": "shell",
        "topic_pos_mean": -271.63743373325894,
        "shell_pos_mean": 47.501865931919646,
        "diff": -319.1392996651786,
        "expected_sign": "negative"
      },
      {
        "pair": "cheese_math",
        "dim": "shell",
        "topic_pos_mean": -207.21538434709822,
        "shell_pos_mean": -56.251072474888396,
        "diff": -150.96431187220983,
        "expected_sign": "negative"
      },
      {
        "pair": "classical_music_coding",
        "dim": "shell",
        "topic_pos_mean": -10.996695382254464,
        "shell_pos_mean": 290.02044677734375,
        "diff": -301.0171421595982,
        "expected_sign": "negative"
      },
      {
        "pair": "cooking_coding",
        "dim": "shell",
        "topic_pos_mean": -35.02622767857143,
        "shell_pos_mean": 150.69698660714286,
        "diff": -185.72321428571428,
        "expected_sign": "negative"
      },
      {
        "pair": "cooking_fiction",
        "dim": "shell",
        "topic_pos_mean": -268.001961844308,
        "shell_pos_mean": 93.18730817522321,
        "diff": -361.18927001953125,
        "expected_sign": "negative"
      },
      {
        "pair": "gardening_fiction",
        "dim": "shell",
        "topic_pos_mean": -269.24930245535717,
        "shell_pos_mean": 63.227085658482146,
        "diff": -332.47638811383933,
        "expected_sign": "negative"
      },
      {
        "pair": "gardening_math",
        "dim": "shell",
        "topic_pos_mean": -250.4103742327009,
        "shell_pos_mean": -36.848336356026785,
        "diff": -213.5620378766741,
        "expected_sign": "negative"
      },
      {
        "pair": "rainy_weather_math",
        "dim": "shell",
        "topic_pos_mean": -188.91275460379464,
        "shell_pos_mean": -64.52562604631696,
        "diff": -124.38712855747768,
        "expected_sign": "negative"
      }
    ]
  },
  "pair_comparisons": [
    {
      "pair_id": "ancient_history_coding",
      "topic": "ancient_history",
      "shell": "coding",
      "beh_topic_delta": -0.17293233082706766,
      "beh_shell_delta": -0.06543233082706766,
      "beh_competing_delta": -0.1075,
      "probe_topic_delta_L31": 63.2230224609375,
      "probe_shell_delta_L31": 37.35858154296875,
      "probe_competing_delta_L31": 25.86444091796875,
      "probe_sign_flips": false
    },
    {
      "pair_id": "astronomy_math",
      "topic": "astronomy",
      "shell": "math",
      "beh_topic_delta": -0.8325,
      "beh_shell_delta": -0.4800000000000001,
      "beh_competing_delta": -0.3524999999999999,
      "probe_topic_delta_L31": -275.1026611328125,
      "probe_shell_delta_L31": -446.6707763671875,
      "probe_competing_delta_L31": 171.568115234375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cats_coding",
      "topic": "cats",
      "shell": "coding",
      "beh_topic_delta": -0.4449874686716792,
      "beh_shell_delta": -0.36250000000000004,
      "beh_competing_delta": -0.08248746867167916,
      "probe_topic_delta_L31": -30.70477294921875,
      "probe_shell_delta_L31": -14.6092529296875,
      "probe_competing_delta_L31": -16.09552001953125,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cats_math",
      "topic": "cats",
      "shell": "math",
      "beh_topic_delta": -0.6193734335839599,
      "beh_shell_delta": -0.46,
      "beh_competing_delta": -0.1593734335839599,
      "probe_topic_delta_L31": -2.89495849609375,
      "probe_shell_delta_L31": -13.2960205078125,
      "probe_competing_delta_L31": 10.40106201171875,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cheese_fiction",
      "topic": "cheese",
      "shell": "fiction",
      "beh_topic_delta": -0.8175,
      "beh_shell_delta": -0.87,
      "beh_competing_delta": 0.05249999999999999,
      "probe_topic_delta_L31": -212.153076171875,
      "probe_shell_delta_L31": -195.8577880859375,
      "probe_competing_delta_L31": -16.2952880859375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cheese_math",
      "topic": "cheese",
      "shell": "math",
      "beh_topic_delta": -0.5774999999999999,
      "beh_shell_delta": -0.6775,
      "beh_competing_delta": 0.10000000000000009,
      "probe_topic_delta_L31": -123.33331298828125,
      "probe_shell_delta_L31": -142.911865234375,
      "probe_competing_delta_L31": 19.57855224609375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "classical_music_coding",
      "topic": "classical_music",
      "shell": "coding",
      "beh_topic_delta": -0.13978696741854638,
      "beh_shell_delta": -0.1775,
      "beh_competing_delta": 0.037713032581453615,
      "probe_topic_delta_L31": 65.10784912109375,
      "probe_shell_delta_L31": 158.1658935546875,
      "probe_competing_delta_L31": -93.05804443359375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cooking_coding",
      "topic": "cooking",
      "shell": "coding",
      "beh_topic_delta": -0.2325,
      "beh_shell_delta": 0.32249999999999995,
      "beh_competing_delta": -0.5549999999999999,
      "probe_topic_delta_L31": 24.787109375,
      "probe_shell_delta_L31": 114.04290771484375,
      "probe_competing_delta_L31": -89.25579833984375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "cooking_fiction",
      "topic": "cooking",
      "shell": "fiction",
      "beh_topic_delta": -0.905,
      "beh_shell_delta": -0.25,
      "beh_competing_delta": -0.655,
      "probe_topic_delta_L31": -239.04901123046875,
      "probe_shell_delta_L31": -80.4482421875,
      "probe_competing_delta_L31": -158.60076904296875,
      "probe_sign_flips": false
    },
    {
      "pair_id": "gardening_fiction",
      "topic": "gardening",
      "shell": "fiction",
      "beh_topic_delta": -0.8925,
      "beh_shell_delta": -0.48,
      "beh_competing_delta": -0.4125,
      "probe_topic_delta_L31": -226.83355712890625,
      "probe_shell_delta_L31": -65.76904296875,
      "probe_competing_delta_L31": -161.06451416015625,
      "probe_sign_flips": false
    },
    {
      "pair_id": "gardening_math",
      "topic": "gardening",
      "shell": "math",
      "beh_topic_delta": -0.6225,
      "beh_shell_delta": -0.41999999999999993,
      "beh_competing_delta": -0.20250000000000012,
      "probe_topic_delta_L31": -121.00311279296875,
      "probe_shell_delta_L31": -19.897705078125,
      "probe_competing_delta_L31": -101.10540771484375,
      "probe_sign_flips": false
    },
    {
      "pair_id": "rainy_weather_math",
      "topic": "rainy_weather",
      "shell": "math",
      "beh_topic_delta": -0.6425000000000001,
      "beh_shell_delta": -0.38250000000000006,
      "beh_competing_delta": -0.26,
      "probe_topic_delta_L31": -106.72344970703125,
      "probe_shell_delta_L31": -170.6287841796875,
      "probe_competing_delta_L31": 63.90533447265625,
      "probe_sign_flips": false
    }
  ]
}