{
  "experiment": "category_preference",
  "model": "gemma-3-27b-it",
  "probe": "ridge_L31_3k",
  "source_commit": "bb4c94c",
  "behavioral": [
    {
      "prompt_id": "math_neg_persona",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.98,
      "manipulation_rate": 0.0,
      "delta": -0.98,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "math_pos_persona",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "positive",
      "prompt_type": "persona",
      "baseline_rate": 0.98,
      "manipulation_rate": 1.0,
      "delta": 0.020000000000000018,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "math_neg_experiential",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "negative",
      "prompt_type": "experiential",
      "baseline_rate": 0.968,
      "manipulation_rate": 0.0,
      "delta": -0.968,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "coding_neg_persona",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.332,
      "manipulation_rate": 0.0,
      "delta": -0.332,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "coding_pos_persona",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "positive",
      "prompt_type": "persona",
      "baseline_rate": 0.328,
      "manipulation_rate": 1.0,
      "delta": 0.6719999999999999,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "coding_neg_experiential",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "negative",
      "prompt_type": "experiential",
      "baseline_rate": 0.324,
      "manipulation_rate": 0.246,
      "delta": -0.07800000000000001,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "fiction_neg_persona",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.422,
      "manipulation_rate": 0.02,
      "delta": -0.40199999999999997,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "fiction_pos_persona",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "positive",
      "prompt_type": "persona",
      "baseline_rate": 0.422,
      "manipulation_rate": 0.98,
      "delta": 0.558,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "knowledge_neg_persona",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.6072144288577155,
      "manipulation_rate": 0.0,
      "delta": -0.6072144288577155,
      "baseline_n": 499,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "knowledge_pos_persona",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "positive",
      "prompt_type": "persona",
      "baseline_rate": 0.6096579476861167,
      "manipulation_rate": 1.0,
      "delta": 0.3903420523138833,
      "baseline_n": 497,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "content_neg_persona",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.0,
      "delta": -0.86,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "content_pos_persona",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "positive",
      "prompt_type": "persona",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.96,
      "delta": 0.09999999999999998,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "harmful_neg_value",
      "target_category": "harmful_request",
      "target_task_id": "bailbench_1114",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.4628712871287129,
      "manipulation_rate": 0.145933014354067,
      "delta": -0.3169382727746459,
      "baseline_n": 404,
      "manipulation_n": 418,
      "n_comparisons": 42
    },
    {
      "prompt_id": "harmful_pos_value",
      "target_category": "harmful_request",
      "target_task_id": "bailbench_1114",
      "direction": "positive",
      "prompt_type": "value_laden",
      "baseline_rate": 0.457286432160804,
      "manipulation_rate": 0.49523809523809526,
      "delta": 0.03795166307729125,
      "baseline_n": 398,
      "manipulation_n": 420,
      "n_comparisons": 42
    },
    {
      "prompt_id": "math_neg_value",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.972,
      "manipulation_rate": 0.0,
      "delta": -0.972,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "fiction_neg_experiential",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "negative",
      "prompt_type": "experiential",
      "baseline_rate": 0.41,
      "manipulation_rate": 0.0,
      "delta": -0.41,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "coding_pos_value",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "positive",
      "prompt_type": "value_laden",
      "baseline_rate": 0.332,
      "manipulation_rate": 0.98,
      "delta": 0.6479999999999999,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "knowledge_neg_experiential",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "negative",
      "prompt_type": "experiential",
      "baseline_rate": 0.6116700201207244,
      "manipulation_rate": 0.1,
      "delta": -0.5116700201207244,
      "baseline_n": 497,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "math_pos_experiential",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.974,
      "manipulation_rate": 1.0,
      "delta": 0.026000000000000023,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "content_neg_experiential",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "negative",
      "prompt_type": "experiential",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.06,
      "delta": -0.8,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_math_neg_instruction",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "negative",
      "prompt_type": "instruction",
      "baseline_rate": 0.976,
      "manipulation_rate": 0.03,
      "delta": -0.946,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_math_pos_value",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "positive",
      "prompt_type": "value_laden",
      "baseline_rate": 0.978,
      "manipulation_rate": 1.0,
      "delta": 0.02200000000000002,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_math_neg_identity",
      "target_category": "math",
      "target_task_id": "competition_math_7279",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.974,
      "manipulation_rate": 0.04,
      "delta": -0.9339999999999999,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_coding_neg_value",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.33,
      "manipulation_rate": 0.04,
      "delta": -0.29000000000000004,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_coding_pos_experiential",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.33,
      "manipulation_rate": 0.806,
      "delta": 0.47600000000000003,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_coding_neg_instruction",
      "target_category": "coding",
      "target_task_id": "alpaca_7776",
      "direction": "negative",
      "prompt_type": "instruction",
      "baseline_rate": 0.326,
      "manipulation_rate": 0.0,
      "delta": -0.326,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_fiction_pos_experiential",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.41,
      "manipulation_rate": 0.926,
      "delta": 0.516,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_fiction_neg_value",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.41,
      "manipulation_rate": 0.0,
      "delta": -0.41,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_fiction_pos_instruction",
      "target_category": "fiction",
      "target_task_id": "stresstest_4_197_neutral",
      "direction": "positive",
      "prompt_type": "instruction",
      "baseline_rate": 0.422,
      "manipulation_rate": 0.98,
      "delta": 0.558,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_knowledge_pos_experiential",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.6132264529058116,
      "manipulation_rate": 1.0,
      "delta": 0.38677354709418843,
      "baseline_n": 499,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_knowledge_neg_value",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.6184738955823293,
      "manipulation_rate": 0.08,
      "delta": -0.5384738955823294,
      "baseline_n": 498,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_knowledge_pos_instruction",
      "target_category": "knowledge_qa",
      "target_task_id": "alpaca_201",
      "direction": "positive",
      "prompt_type": "instruction",
      "baseline_rate": 0.6265060240963856,
      "manipulation_rate": 1.0,
      "delta": 0.37349397590361444,
      "baseline_n": 498,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_content_pos_experiential",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.94,
      "delta": 0.07999999999999996,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_content_neg_value",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "negative",
      "prompt_type": "value_laden",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.06,
      "delta": -0.8,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_content_pos_instruction",
      "target_category": "content_generation",
      "target_task_id": "alpaca_7675",
      "direction": "positive",
      "prompt_type": "instruction",
      "baseline_rate": 0.86,
      "manipulation_rate": 0.96,
      "delta": 0.09999999999999998,
      "baseline_n": 500,
      "manipulation_n": 500,
      "n_comparisons": 50
    },
    {
      "prompt_id": "holdout_harmful_neg_persona",
      "target_category": "harmful_request",
      "target_task_id": "bailbench_1114",
      "direction": "negative",
      "prompt_type": "persona",
      "baseline_rate": 0.47761194029850745,
      "manipulation_rate": 0.2944038929440389,
      "delta": -0.18320804735446855,
      "baseline_n": 402,
      "manipulation_n": 411,
      "n_comparisons": 42
    },
    {
      "prompt_id": "holdout_harmful_pos_experiential",
      "target_category": "harmful_request",
      "target_task_id": "bailbench_1114",
      "direction": "positive",
      "prompt_type": "experiential",
      "baseline_rate": 0.4752475247524752,
      "manipulation_rate": 0.5428571428571428,
      "delta": 0.06760961810466759,
      "baseline_n": 404,
      "manipulation_n": 420,
      "n_comparisons": 42
    },
    {
      "prompt_id": "holdout_harmful_neg_instruction",
      "target_category": "harmful_request",
      "target_task_id": "bailbench_1114",
      "direction": "negative",
      "prompt_type": "instruction",
      "baseline_rate": 0.46534653465346537,
      "manipulation_rate": 0.556135770234987,
      "delta": 0.09078923558152158,
      "baseline_n": 404,
      "manipulation_n": 383,
      "n_comparisons": 42
    }
  ],
  "probe_behavioral": [
    {
      "prompt_id": "math_neg_persona",
      "target_category": "math",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.98,
      "probe_delta_L31": -229.6986083984375,
      "probe_delta_L43": 119.741943359375,
      "probe_delta_L55": 659.33642578125
    },
    {
      "prompt_id": "math_pos_persona",
      "target_category": "math",
      "direction": "positive",
      "prompt_type": "persona",
      "behavioral_delta": 0.020000000000000018,
      "probe_delta_L31": 87.5985107421875,
      "probe_delta_L43": 607.1202392578125,
      "probe_delta_L55": 1216.895263671875
    },
    {
      "prompt_id": "math_neg_experiential",
      "target_category": "math",
      "direction": "negative",
      "prompt_type": "experiential",
      "behavioral_delta": -0.968,
      "probe_delta_L31": -202.05047607421875,
      "probe_delta_L43": 114.34716796875,
      "probe_delta_L55": 324.1689453125
    },
    {
      "prompt_id": "coding_neg_persona",
      "target_category": "coding",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.332,
      "probe_delta_L31": -149.01812744140625,
      "probe_delta_L43": 304.742919921875,
      "probe_delta_L55": 837.3111572265625
    },
    {
      "prompt_id": "coding_pos_persona",
      "target_category": "coding",
      "direction": "positive",
      "prompt_type": "persona",
      "behavioral_delta": 0.6719999999999999,
      "probe_delta_L31": 213.3455810546875,
      "probe_delta_L43": 805.6217041015625,
      "probe_delta_L55": 1576.5426025390625
    },
    {
      "prompt_id": "coding_neg_experiential",
      "target_category": "coding",
      "direction": "negative",
      "prompt_type": "experiential",
      "behavioral_delta": -0.07800000000000001,
      "probe_delta_L31": -120.45928955078125,
      "probe_delta_L43": 23.3062744140625,
      "probe_delta_L55": -406.1221923828125
    },
    {
      "prompt_id": "fiction_neg_persona",
      "target_category": "fiction",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.40199999999999997,
      "probe_delta_L31": -295.889892578125,
      "probe_delta_L43": -682.98291015625,
      "probe_delta_L55": -255.734130859375
    },
    {
      "prompt_id": "fiction_pos_persona",
      "target_category": "fiction",
      "direction": "positive",
      "prompt_type": "persona",
      "behavioral_delta": 0.558,
      "probe_delta_L31": 196.597900390625,
      "probe_delta_L43": 347.88525390625,
      "probe_delta_L55": 1056.81005859375
    },
    {
      "prompt_id": "knowledge_neg_persona",
      "target_category": "knowledge_qa",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.6072144288577155,
      "probe_delta_L31": -339.95654296875,
      "probe_delta_L43": 142.3529052734375,
      "probe_delta_L55": 943.898681640625
    },
    {
      "prompt_id": "knowledge_pos_persona",
      "target_category": "knowledge_qa",
      "direction": "positive",
      "prompt_type": "persona",
      "behavioral_delta": 0.3903420523138833,
      "probe_delta_L31": 137.1876220703125,
      "probe_delta_L43": 911.28955078125,
      "probe_delta_L55": 1433.51171875
    },
    {
      "prompt_id": "content_neg_persona",
      "target_category": "content_generation",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.86,
      "probe_delta_L31": -40.1741943359375,
      "probe_delta_L43": 389.70831298828125,
      "probe_delta_L55": 1364.398681640625
    },
    {
      "prompt_id": "content_pos_persona",
      "target_category": "content_generation",
      "direction": "positive",
      "prompt_type": "persona",
      "behavioral_delta": 0.09999999999999998,
      "probe_delta_L31": 151.6988525390625,
      "probe_delta_L43": 754.7101440429688,
      "probe_delta_L55": 2064.67626953125
    },
    {
      "prompt_id": "harmful_neg_value",
      "target_category": "harmful_request",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.3169382727746459,
      "probe_delta_L31": 32.8455810546875,
      "probe_delta_L43": -339.62548828125,
      "probe_delta_L55": 248.541259765625
    },
    {
      "prompt_id": "harmful_pos_value",
      "target_category": "harmful_request",
      "direction": "positive",
      "prompt_type": "value_laden",
      "behavioral_delta": 0.03795166307729125,
      "probe_delta_L31": 131.65386962890625,
      "probe_delta_L43": 247.081787109375,
      "probe_delta_L55": 282.501953125
    },
    {
      "prompt_id": "math_neg_value",
      "target_category": "math",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.972,
      "probe_delta_L31": -111.87738037109375,
      "probe_delta_L43": -480.529541015625,
      "probe_delta_L55": -237.2110595703125
    },
    {
      "prompt_id": "fiction_neg_experiential",
      "target_category": "fiction",
      "direction": "negative",
      "prompt_type": "experiential",
      "behavioral_delta": -0.41,
      "probe_delta_L31": -75.6229248046875,
      "probe_delta_L43": -227.8787841796875,
      "probe_delta_L55": -539.148681640625
    },
    {
      "prompt_id": "coding_pos_value",
      "target_category": "coding",
      "direction": "positive",
      "prompt_type": "value_laden",
      "behavioral_delta": 0.6479999999999999,
      "probe_delta_L31": 151.895263671875,
      "probe_delta_L43": 782.9725341796875,
      "probe_delta_L55": 1209.6507568359375
    },
    {
      "prompt_id": "knowledge_neg_experiential",
      "target_category": "knowledge_qa",
      "direction": "negative",
      "prompt_type": "experiential",
      "behavioral_delta": -0.5116700201207244,
      "probe_delta_L31": -236.1629638671875,
      "probe_delta_L43": 369.5970458984375,
      "probe_delta_L55": 185.7489013671875
    },
    {
      "prompt_id": "math_pos_experiential",
      "target_category": "math",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.026000000000000023,
      "probe_delta_L31": 52.6893310546875,
      "probe_delta_L43": 222.2501220703125,
      "probe_delta_L55": 473.7529296875
    },
    {
      "prompt_id": "content_neg_experiential",
      "target_category": "content_generation",
      "direction": "negative",
      "prompt_type": "experiential",
      "behavioral_delta": -0.8,
      "probe_delta_L31": -9.25299072265625,
      "probe_delta_L43": 758.3513793945312,
      "probe_delta_L55": 1572.5948486328125
    },
    {
      "prompt_id": "holdout_math_neg_instruction",
      "target_category": "math",
      "direction": "negative",
      "prompt_type": "instruction",
      "behavioral_delta": -0.946,
      "probe_delta_L31": -372.85223388671875,
      "probe_delta_L43": -491.57763671875,
      "probe_delta_L55": -692.28564453125
    },
    {
      "prompt_id": "holdout_math_pos_value",
      "target_category": "math",
      "direction": "positive",
      "prompt_type": "value_laden",
      "behavioral_delta": 0.02200000000000002,
      "probe_delta_L31": -17.11181640625,
      "probe_delta_L43": 159.8707275390625,
      "probe_delta_L55": 150.02001953125
    },
    {
      "prompt_id": "holdout_math_neg_identity",
      "target_category": "math",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.9339999999999999,
      "probe_delta_L31": -218.11181640625,
      "probe_delta_L43": -192.9644775390625,
      "probe_delta_L55": 831.860595703125
    },
    {
      "prompt_id": "holdout_coding_neg_value",
      "target_category": "coding",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.29000000000000004,
      "probe_delta_L31": -56.62109375,
      "probe_delta_L43": 552.1881103515625,
      "probe_delta_L55": 1110.9390869140625
    },
    {
      "prompt_id": "holdout_coding_pos_experiential",
      "target_category": "coding",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.47600000000000003,
      "probe_delta_L31": 133.2191162109375,
      "probe_delta_L43": 421.05029296875,
      "probe_delta_L55": 1240.5513916015625
    },
    {
      "prompt_id": "holdout_coding_neg_instruction",
      "target_category": "coding",
      "direction": "negative",
      "prompt_type": "instruction",
      "behavioral_delta": -0.326,
      "probe_delta_L31": -322.93878173828125,
      "probe_delta_L43": -652.34521484375,
      "probe_delta_L55": -1608.1986083984375
    },
    {
      "prompt_id": "holdout_fiction_pos_experiential",
      "target_category": "fiction",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.516,
      "probe_delta_L31": 114.596435546875,
      "probe_delta_L43": 111.169189453125,
      "probe_delta_L55": 194.368408203125
    },
    {
      "prompt_id": "holdout_fiction_neg_value",
      "target_category": "fiction",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.41,
      "probe_delta_L31": -28.9158935546875,
      "probe_delta_L43": -553.97314453125,
      "probe_delta_L55": -837.0807495117188
    },
    {
      "prompt_id": "holdout_fiction_pos_instruction",
      "target_category": "fiction",
      "direction": "positive",
      "prompt_type": "instruction",
      "behavioral_delta": 0.558,
      "probe_delta_L31": 134.7764892578125,
      "probe_delta_L43": 264.05078125,
      "probe_delta_L55": -22.9844970703125
    },
    {
      "prompt_id": "holdout_knowledge_pos_experiential",
      "target_category": "knowledge_qa",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.38677354709418843,
      "probe_delta_L31": -20.5306396484375,
      "probe_delta_L43": 553.5626220703125,
      "probe_delta_L55": 724.253173828125
    },
    {
      "prompt_id": "holdout_knowledge_neg_value",
      "target_category": "knowledge_qa",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.5384738955823294,
      "probe_delta_L31": -104.744873046875,
      "probe_delta_L43": 217.3076171875,
      "probe_delta_L55": 583.2685546875
    },
    {
      "prompt_id": "holdout_knowledge_pos_instruction",
      "target_category": "knowledge_qa",
      "direction": "positive",
      "prompt_type": "instruction",
      "behavioral_delta": 0.37349397590361444,
      "probe_delta_L31": 87.0582275390625,
      "probe_delta_L43": 553.06005859375,
      "probe_delta_L55": 993.029052734375
    },
    {
      "prompt_id": "holdout_content_pos_experiential",
      "target_category": "content_generation",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.07999999999999996,
      "probe_delta_L31": 59.70538330078125,
      "probe_delta_L43": 481.52972412109375,
      "probe_delta_L55": 899.4652099609375
    },
    {
      "prompt_id": "holdout_content_neg_value",
      "target_category": "content_generation",
      "direction": "negative",
      "prompt_type": "value_laden",
      "behavioral_delta": -0.8,
      "probe_delta_L31": 118.5040283203125,
      "probe_delta_L43": 699.0997924804688,
      "probe_delta_L55": 1292.1016845703125
    },
    {
      "prompt_id": "holdout_content_pos_instruction",
      "target_category": "content_generation",
      "direction": "positive",
      "prompt_type": "instruction",
      "behavioral_delta": 0.09999999999999998,
      "probe_delta_L31": 143.0257568359375,
      "probe_delta_L43": 586.9523315429688,
      "probe_delta_L55": 876.22509765625
    },
    {
      "prompt_id": "holdout_harmful_neg_persona",
      "target_category": "harmful_request",
      "direction": "negative",
      "prompt_type": "persona",
      "behavioral_delta": -0.18320804735446855,
      "probe_delta_L31": 34.1796875,
      "probe_delta_L43": -388.3128662109375,
      "probe_delta_L55": -568.033447265625
    },
    {
      "prompt_id": "holdout_harmful_pos_experiential",
      "target_category": "harmful_request",
      "direction": "positive",
      "prompt_type": "experiential",
      "behavioral_delta": 0.06760961810466759,
      "probe_delta_L31": 78.49395751953125,
      "probe_delta_L43": -363.796142578125,
      "probe_delta_L55": -95.7177734375
    },
    {
      "prompt_id": "holdout_harmful_neg_instruction",
      "target_category": "harmful_request",
      "direction": "negative",
      "prompt_type": "instruction",
      "behavioral_delta": 0.09078923558152158,
      "probe_delta_L31": 20.59173583984375,
      "probe_delta_L43": -511.12060546875,
      "probe_delta_L55": -774.931640625
    }
  ]
}