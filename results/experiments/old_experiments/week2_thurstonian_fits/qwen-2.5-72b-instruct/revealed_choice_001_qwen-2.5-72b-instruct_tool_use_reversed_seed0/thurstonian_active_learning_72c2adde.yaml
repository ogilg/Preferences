fitting_method: active_learning
converged: true
neg_log_likelihood: 16642.10420880291
n_iterations: 79
n_function_evals: 83
termination_message: 'CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL'
gradient_norm: 9.36344592968735
history:
  loss:
  - 21843.71713852554
  - 19423.075560107634
  - 18840.957447528708
  - 17688.587196431472
  - 17427.416514940873
  - 17204.120909295583
  - 17109.79215850102
  - 17017.48702817847
  - 16961.51510070957
  - 16906.869113604822
  - 16886.75897601445
  - 16846.657042323703
  - 16833.903227225917
  - 16805.712733990284
  - 16794.638475663225
  - 16778.46365039534
  - 16761.968428515585
  - 16751.36884276987
  - 16743.750090799327
  - 16737.878171405904
  - 16731.508897938143
  - 16724.216793544772
  - 16717.404917559583
  - 16712.94399587952
  - 16708.89699444397
  - 16704.857788763922
  - 16701.101922625418
  - 16697.22260638654
  - 16694.54699695612
  - 16690.674658707096
  - 16688.24400632341
  - 16685.575131538913
  - 16683.281477998175
  - 16681.89114569027
  - 16679.807119553407
  - 16677.371341210903
  - 16675.996473645417
  - 16673.300880427625
  - 16672.095164800336
  - 16670.45296501375
  - 16667.757980548522
  - 16666.352783295133
  - 16664.35138581913
  - 16663.413862706107
  - 16662.13171325334
  - 16660.715648366062
  - 16660.094532621915
  - 16658.641498798326
  - 16658.105742511758
  - 16657.55385493673
  - 16656.855368702323
  - 16656.41475099121
  - 16655.298291381703
  - 16654.87999848144
  - 16654.42266752965
  - 16653.80118982863
  - 16653.361271706508
  - 16652.664042617795
  - 16651.968163673668
  - 16651.256065141035
  - 16650.474384876226
  - 16649.597539259343
  - 16648.71153610996
  - 16648.125106130727
  - 16647.52803367793
  - 16647.14563699191
  - 16646.633652858367
  - 16646.252669608468
  - 16645.839967724914
  - 16645.617950066604
  - 16645.171876963625
  - 16645.09216629797
  - 16644.638985503272
  - 16644.329986705634
  - 16643.9256270953
  - 16643.411478271417
  - 16643.109149868997
  - 16642.300858372328
  - 16642.10420880291
  sigma_max:
  - 1.0
  - 1.841498562065731
  - 2.160434359909895
  - 4.287943768270926
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
  - 7.38905609893065
config:
  n_tasks: 500
  seed: 42
  generation_seed: 0
